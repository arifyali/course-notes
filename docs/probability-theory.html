<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Master’s Companion</title>
  <meta name="description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program">
  <meta name="generator" content="bookdown 0.6 and GitBook 2.6.7">

  <meta property="og:title" content="A Master’s Companion" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Master’s Companion" />
  
  <meta name="twitter:description" content="Notes and supporting material for selected courses from Georgetown’s Master of Science in Mathematics and Statistics program" />
  

<meta name="author" content="Sean Wilson">


<meta name="date" content="2018-02-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\usepackage{amsthm}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
$$



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Master's Companion</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#upper-bounds-and-suprema"><i class="fa fa-check"></i><b>1.1</b> Upper bounds and suprema</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2</b> Probability theory</a><ul>
<li class="chapter" data-level="2.1" data-path="probability-theory.html"><a href="probability-theory.html#background-material"><i class="fa fa-check"></i><b>2.1</b> Background material</a></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html#transformations-and-expectations"><i class="fa fa-check"></i><b>2.2</b> Transformations and expectations</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#distributions-of-functions-of-a-random-variable"><i class="fa fa-check"></i><b>2.2.1</b> Distributions of functions of a random variable</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#expected-values"><i class="fa fa-check"></i><b>2.2.2</b> Expected values</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>2.2.3</b> Moments and moment generating functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probability-theory.html"><a href="probability-theory.html#multiple-random-variables"><i class="fa fa-check"></i><b>2.3</b> Multiple random variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="probability-theory.html"><a href="probability-theory.html#conditional-distributions-and-independence"><i class="fa fa-check"></i><b>2.3.1</b> Conditional distributions and independence</a></li>
<li class="chapter" data-level="2.3.2" data-path="probability-theory.html"><a href="probability-theory.html#covariance-and-correlation"><i class="fa fa-check"></i><b>2.3.2</b> Covariance and correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Master’s Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-theory" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Probability theory</h1>
<div id="background-material" class="section level2">
<h2><span class="header-section-number">2.1</span> Background material</h2>

<div class="theorem">
<p><span id="thm:prob-partitions" class="theorem"><strong>Theorem 2.1  </strong></span>If <span class="math inline">\(P\)</span> is a probability function, then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P\left(A\right)=\sum_{i=1}^{\infty}P\left(A\cap C_{i}\right)\)</span> for any partition <span class="math inline">\(C_{1},C_{2},\ldots\)</span>;</li>
<li><span class="math inline">\(P\left(\cup_{i=1}^{\infty}A_{i}\right)\leq\sum_{i=1}^{\infty}P\left(A_{i}\right)\)</span> for any sets <span class="math inline">\(A_{1},A_{2},\ldots\)</span>.
</div></li>
</ol>

<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 2.2  (Bayes’ Rule)  </strong></span>Let <span class="math inline">\(A_{1},A_{2},\ldots\)</span> be a partition of the sample space <span class="math inline">\(\Omega\)</span>, and let <span class="math inline">\(B\)</span> be any set. Then, for each <span class="math inline">\(i=1,2,\ldots\)</span>,
<span class="math display">\[
P\left(A_{i}|B\right)=\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.
\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From the definition of conditional probability, we have
<span class="math display">\[\begin{align*}
P\left(A_{i}|B\right) &amp; =\frac{P\left(A_{i}\cap B\right)}{P\left(B\right)} \\
 &amp; =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{P\left(B\right)}\tag{conditional probability} \\
 &amp; =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B\cap A_{j}\right)}\tag{the $A_{j}$ partition $\Omega$} \\
 &amp; =\frac{P\left(B|A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.\tag{conditional probability}
\end{align*}\]</span>
</div>


<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 2.1  </strong></span>The <em>cumulative distribution function</em> or <em>cdf</em> of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(F_{X}\left(x\right)\)</span>, is defined by <span class="math inline">\(F_{X}\left(x\right)=P_{X}\left(X\leq x\right)\)</span>, for all <span class="math inline">\(x\)</span>.
</div>


<div class="theorem">
<p><span id="thm:properties-of-cdf" class="theorem"><strong>Theorem 2.3  </strong></span>The function <span class="math inline">\(F\left(x\right)\)</span> is a cdf if and only if the following three conditions hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lim_{x\rightarrow-\infty}F\left(x\right)=0\)</span> and <span class="math inline">\(\lim_{x\rightarrow\infty}F\left(x\right)=1\)</span>.</li>
<li><span class="math inline">\(F\left(x\right)\)</span> is a nondecreasing function of <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(F\left(x\right)\)</span> is right-continuous; that is, for every number <span class="math inline">\(x_{0}\)</span>, <span class="math inline">\(\lim_{x\downarrow x_{0}}F\left(x\right)=F\left(x_{0}\right)\)</span>.</li>
</ol>
(This is Theorem 1.5.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="transformations-and-expectations" class="section level2">
<h2><span class="header-section-number">2.2</span> Transformations and expectations</h2>
<div id="distributions-of-functions-of-a-random-variable" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Distributions of functions of a random variable</h3>
<p>When transformations are made, it is important to keep track of the sample spaces of the random variables; otherwise, much confusion can arise. When the transformation is from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y=g\left(X\right)\)</span>, it is most convenient to use</p>
<span class="math display" id="eq:trans-rv-sample-space">\[\begin{equation}
\mathcal{X}=\left\{ x:f_{X}\left(x\right)&gt;0\right\} \quad\text{and}\quad\mathcal{Y}=\left\{ y:y=g\left(x\right)\text{ for some }x\in\mathcal{X}\right\}.
\tag{2.1}
\end{equation}\]</span>

<div class="theorem">
<p><span id="thm:cdf-of-function-of-rv" class="theorem"><strong>Theorem 2.4  </strong></span>Let <span class="math inline">\(X\)</span> have cdf <span class="math inline">\(F_{X}\left(x\right)\)</span>, let <span class="math inline">\(Y=g\left(X\right)\)</span>, and let <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> be defined as in Equation <a href="probability-theory.html#eq:trans-rv-sample-space">(2.1)</a>.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(g\)</span> is an increasing function on <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(F_{Y}\left(y\right)=F_{X}\left(g^{-1}\left(y\right)\right)\)</span> for <span class="math inline">\(y\in\mathcal{Y}\)</span>.</li>
<li>If <span class="math inline">\(g\)</span> is a decreasing function on <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(X\)</span> is a continuous random variable, <span class="math inline">\(F_{Y}\left(y\right)=1-F_{X}\left(g^{-1}\left(y\right)\right)\)</span> for <span class="math inline">\(y\in\mathcal{Y}\)</span>.</li>
</ol>
(This is Theorem 2.1.3 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> <span class="math inline">\(g\)</span> is a monotone function, i.e., it maps each <span class="math inline">\(x\)</span> to a single <span class="math inline">\(y\)</span>, and each <span class="math inline">\(y\)</span> comes from at most one <span class="math inline">\(x\)</span>. In this case that <span class="math inline">\(g\)</span> is increasing, we have</p>
<span class="math display">\[\begin{align*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} &amp; =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\leq g^{-1}\left(y\right)\right\} \\
    &amp; =\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\},
\end{align*}\]</span>
<p>and the cdf of <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\begin{align*}
F_{Y}\left(y\right) &amp; =P\left(\left\{ Y\leq y\right\} \right) \\
    &amp; =P\left(\left\{ g\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\} \right) \\
    &amp; =\int_{-\infty}^{g^{-1}\left(y\right)}f_{X}\left(x\right)\dif x \\
    &amp; =F_{X}\left(g^{-1}\left(y\right)\right).
\end{align*}\]</span>
<p>In the case that <span class="math inline">\(g\)</span> is decreasing, we have</p>
<span class="math display">\[\begin{align*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} &amp; =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\geq g^{-1}\left(y\right)\right\} \\
    &amp; =\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\},
\end{align*}\]</span>
<p>and the cdf of <span class="math inline">\(Y\)</span> is</p>
<span class="math display">\[\begin{align*}
F_{Y}\left(y\right) &amp; =P\left(\left\{ Y\leq y\right\} \right) \\
    &amp; =P\left(\left\{ g\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\} \right) \\
    &amp; =\int_{g^{-1}\left(y\right)}^{\infty}f_{X}\left(x\right)\dif x \\
    &amp; =1-F_{X}\left(g^{-1}\left(y\right)\right).\tag{continuity of $X$}
\end{align*}\]</span>
</div>


<div class="theorem">
<p><span id="thm:pdf-of-function-of-rv" class="theorem"><strong>Theorem 2.5  </strong></span>Let <span class="math inline">\(X\)</span> have pdf <span class="math inline">\(f_{X}\left(x\right)\)</span> and let <span class="math inline">\(Y=g\left(X\right)\)</span>, where <span class="math inline">\(g\)</span> is a monotone function. Let <span class="math inline">\(\mathcal{X}=\left\{ x:f_{X}\left(x\right)&gt;0\right\}\)</span> and let <span class="math inline">\(\mathcal{Y}=\left\{ y:y=g\left(x\right),x\in\mathcal{X}\right\}\)</span>. Suppose that <span class="math inline">\(f_{X}\left(x\right)\)</span> is continuous on <span class="math inline">\(\mathcal{X}\)</span> and that <span class="math inline">\(g^{-1}\left(y\right)\)</span> has a continuous derivative on <span class="math inline">\(\mathcal{Y}\)</span>. Then the pdf of <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \begin{cases}
    f_{X}\left(g^{-1}\left(y\right)\right)\left|\dfrac{\dif}{\dif y}g^{-1}\left(y\right)\right|, &amp; y\in\mathcal{Y}\\
    0, &amp; \text{otherwise}
  \end{cases}.
\]</span></p>
(This is Theorem 2.1.5 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From Theorem <a href="probability-theory.html#thm:cdf-of-function-of-rv">2.4</a> and applying the chain rule, we have</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \dfrac{\dif}{\dif y}F_{Y}\left(y\right)=
  f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]</span></p>
<p>in the case that g is increasing and</p>
<p><span class="math display">\[
f_{Y}\left(y\right)=
  \dfrac{\dif}{\dif y}F_{Y}\left(y\right)=
  0-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)=
  -f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]</span></p>
in the case that <span class="math inline">\(g\)</span> is decreasing, which can be expressed concisely as in the theorem.
</div>

<p>We will look at <span class="math inline">\(F_{X}^{-1}\)</span>, the inverse of the cdf <span class="math inline">\(F_{X}\)</span>. If <span class="math inline">\(F_{X}\)</span> is strictly increasing, then <span class="math inline">\(F_{X}^{-1}\)</span> is well defined by</p>
<span class="math display" id="eq:inverse-cdf-increasing">\[\begin{equation}
F_{X}^{-1}\left(y\right)=x\implies F_{X}\left(x\right)=y.
\tag{2.2}
\end{equation}\]</span>
<p>However, if <span class="math inline">\(F_{X}\)</span> is constant on some interval, then <span class="math inline">\(F_{X}^{-1}\)</span> is not well defined by Equation <a href="probability-theory.html#eq:inverse-cdf-increasing">(2.2)</a>. Any <span class="math inline">\(x\)</span> satisfying <span class="math inline">\(x_{1}\leq x\leq x_{2}\)</span> satisfies <span class="math inline">\(F_{X}\left(x\right)=y\)</span>. This problem is avoided by defining <span class="math inline">\(F_{X}^{-1}\left(y\right)\)</span> for <span class="math inline">\(0&lt;y&lt;1\)</span> by</p>
<p><span class="math display">\[
F_{X}^{-1}\left(y\right)=\inf\left\{ x:F_{X}\left(x\right)\geq y\right\},
\]</span>
a definition that agrees with Equation <a href="probability-theory.html#eq:inverse-cdf-increasing">(2.2)</a> when <span class="math inline">\(F_{X}\)</span> is nonconstant and provides an <span class="math inline">\(F_{X}^{-1}\)</span> that is single-valued even when <span class="math inline">\(F_{X}\)</span> is not strictly increasing. Using this definition, for some interval <span class="math inline">\(\left(x_{1},x_{2}\right)\)</span> on which <span class="math inline">\(F_{X}\)</span> is constant, we have <span class="math inline">\(F_{X}^{-1}\left(y\right)=x_{1}\)</span>. At the endpoints of the range of <span class="math inline">\(y\)</span>, <span class="math inline">\(F_{X}^{-1}\left(y\right)\)</span> can also be defined. <span class="math inline">\(F_{X}^{-1}\left(1\right)=\infty\)</span> if <span class="math inline">\(F_{X}\left(x\right)&lt;1\)</span> for all <span class="math inline">\(x\)</span> and, for any <span class="math inline">\(F_{X}\)</span>, <span class="math inline">\(F_{X}^{-1}\left(0\right)=-\infty\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 2.6  (Probability integral transformation)  </strong></span>Let <span class="math inline">\(X\)</span> have continuous cdf <span class="math inline">\(F_{X}\left(x\right)\)</span> and define the random variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y=F_{X}\left(X\right)\)</span>. Then <span class="math inline">\(Y\)</span> is uniformly distributed on <span class="math inline">\(\left(0,1\right)\)</span>, that is, <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=y\)</span>, <span class="math inline">\(0&lt;y&lt;1\)</span>.</p>
(This is Theorem 2.1.10 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For <span class="math inline">\(Y=F_{X}\left(X\right)\)</span> we have, for <span class="math inline">\(0&lt;y&lt;1\)</span>,</p>
<span class="math display">\[\begin{align*}
P\left(\left\{ Y\leq y\right\} \right) &amp; =P\left(\left\{ F_{X}\left(X\right)\leq y\right\} \right) \\
    &amp; =P\left(\left\{ F_{X}^{-1}\left[F_{X}\left(X\right)\right]\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{$F_{X}^{-1}$ is increasing} \\
    &amp; =P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{see paragraph below} \\
    &amp; =F_{X}\left(F_{X}^{-1}\left(y\right)\right)\tag{definition of $F_{X}$} \\
    &amp; =y.\tag{continuity of $F_{X}$}
\end{align*}\]</span>
<p>At the endpoints we have <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=1\)</span> for <span class="math inline">\(y\geq 1\)</span> and <span class="math inline">\(P\left(\left\{ Y\leq y\right\} \right)=0\)</span> for <span class="math inline">\(y\leq 0\)</span>, showing that <span class="math inline">\(Y\)</span> has a uniform distribution.</p>
<p>The reasoning behind the equality</p>
<p><span class="math display">\[
P\left(\left\{ F_{X}^{-1}\left(F_{X}\left(X\right)\right)\leq F_{X}^{-1}\left(y\right)\right\} \right)=P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)
\]</span></p>
is somewhat subtle and deserves additional attention. If <span class="math inline">\(F_{X}\)</span> is strictly increasing, then it is true that <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x\)</span>. However, if <span class="math inline">\(F_{X}\)</span> is flat, it may be that <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)\neq x\)</span>. Suppose <span class="math inline">\(F_{X}\)</span> contains an interval <span class="math inline">\(\left(x_{1},x_{2}\right)\)</span> on which <span class="math inline">\(F_{X}\)</span> is constant, and let <span class="math inline">\(x\in\left[x_{1},x_{2}\right]\)</span>. Then <span class="math inline">\(F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x_{1}\)</span> for any <span class="math inline">\(x\)</span> in this interval. Even in this case, though, the probability equality holds, since <span class="math inline">\(P\left(\left\{ X\leq x\right\} \right)=P\left(\left\{ X\leq x_{1}\right\} \right)\)</span> for any <span class="math inline">\(x\in\left[x_{1},x_{2}\right]\)</span>. The flat cdf denotes a region of <span class="math inline">\(0\)</span> probability <span class="math inline">\((P\left(\left\{ x_{1}&lt;X\leq x\right\} \right)=F_{X}\left(x\right)-F_{X}\left(x_{1}\right)=0)\)</span>.
</div>

</div>
<div id="expected-values" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Expected values</h3>

<div class="theorem">
<p><span id="thm:properties-of-expectation" class="theorem"><strong>Theorem 2.7  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> be constants. Then for any functions <span class="math inline">\(g_{1}\left(x\right)\)</span> and <span class="math inline">\(g_{2}\left(x\right)\)</span> whose expectations exist,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\E\left[ag_{1}\left(X\right)+bg_{2}\left(X\right)+c\right]=a\E\left[g_{1}\left(X\right)\right]+b\E\left[g_{2}\left(X\right)\right]+c\)</span>.</li>
<li>If <span class="math inline">\(g_{1}\left(x\right)\geq 0\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E\left[g_{1}\left(X\right)\right]\geq 0\)</span>.</li>
<li>If <span class="math inline">\(g_{1}\left(x\right)\geq g_{2}\left(x\right)\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E\left[g_{1}\left(X\right)\right]\geq\E\left[g_{2}\left(X\right)\right]\)</span>.</li>
<li>If <span class="math inline">\(a\leq g_{1}\left(x\right)\leq b\)</span> for all <span class="math inline">\(x\)</span>, then <span class="math inline">\(a\leq\E\left[g_{1}\left(X\right)\right]\leq b\)</span>.</li>
</ol>
(This is Theorem 2.2.5 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> PROOF GOES HERE
</div>

</div>
<div id="moments-and-moment-generating-functions" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Moments and moment generating functions</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-15" class="definition"><strong>Definition 2.2  </strong></span>For each integer <span class="math inline">\(n\)</span>, the <span class="math inline">\(n\text{th}\)</span> <em>moment</em> of <span class="math inline">\(X\)</span> or <span class="math inline">\((F_{X}\left(x\right))\)</span>, <span class="math inline">\(\mu&#39;_{n}\)</span>, is</p>
<p><span class="math display">\[
\mu&#39;_{n}=\E\left[X^{n}\right].
\]</span></p>
<p>The <span class="math inline">\(n\text{th}\)</span> <em>central moment</em> of <span class="math inline">\(X\)</span>, <span class="math inline">\(\mu_{n}\)</span>, is</p>
<p><span class="math display">\[
\mu_{n}=\E\left[\left(X-\mu\right)^{n}\right],
\]</span></p>
where <span class="math inline">\(\mu=\mu&#39;_{1}=\E\left[X\right]\)</span>.
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-16" class="definition"><strong>Definition 2.3  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with cdf <span class="math inline">\(F_{X}\)</span>. The <em>moment generating function</em> (mgf) of <span class="math inline">\(X\)</span> (or <span class="math inline">\(F_{X}\)</span>), denoted by <span class="math inline">\(M_{X}\left(t\right)\)</span>, is</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\E\left[\mathrm{e}^{tX}\right],
\]</span></p>
provided that the expectation exists for <span class="math inline">\(t\)</span> in some neighborhood of <span class="math inline">\(0\)</span>. That is, there is an <span class="math inline">\(h&gt;0\)</span> such that, for all <span class="math inline">\(t\)</span> in <span class="math inline">\(-h&lt;t&lt;h\)</span>, <span class="math inline">\(\E\left[\mathrm{e}^{tX}\right]\)</span> exists. If the expectation does not exist in a neighborhood of <span class="math inline">\(0\)</span>, we say that the moment generating function does not exist.
</div>

<p>More explicitly, we can write the mgf of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x
\]</span>
if <span class="math inline">\(X\)</span> is continuous, or</p>
<p><span class="math display">\[
M_{X}\left(t\right)=\sum_{x}\mathrm{e}^{tx}P\left(\left\{ X=x\right\} \right)
\]</span></p>
<p>if <span class="math inline">\(X\)</span> is discrete.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-17" class="theorem"><strong>Theorem 2.8  </strong></span>If <span class="math inline">\(X\)</span> has mgf <span class="math inline">\(M_{X}\left(t\right)\)</span>, then</p>
<p><span class="math display">\[
\E\left[X^{n}\right]=M_{X}^{\left(n\right)}\left(0\right),
\]</span></p>
<p>where we define</p>
<p><span class="math display">\[
M_{X}^{\left(n\right)}\left(0\right)=\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0}.
\]</span></p>
<p>That is, the <span class="math inline">\(n\text{th}\)</span> moment is equal to the <span class="math inline">\(n\text{th}\)</span> derivative of <span class="math inline">\(M_{X}\left(t\right)\)</span> evaluated at <span class="math inline">\(t=0\)</span>.</p>
(This is Theorem 2.3.7 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Assuming that we can differentiate under the integral sign, we have</p>
<span class="math display">\[\begin{align*}
\frac{\dif}{\dif t}M_{X}\left(t\right) &amp; =\frac{\dif}{\dif t}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x \\
  &amp; =\int_{-\infty}^{\infty}\left(\frac{\dif}{\dif t}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x \\
  &amp; =\int_{-\infty}^{\infty}\left(x\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x \\
  &amp; =\E\left[X\mathrm{e}^{tX}\right].
\end{align*}\]</span>
<p>Thus,</p>
<p><span class="math display">\[
\frac{\dif}{\dif t}M_{X}\left(t\right)\Bigr\vert_{t=0}=\E\left[X\mathrm{e}^{tX}\right]\Big\vert_{t=0}=\E\left[X\mathrm{e}^{0}\right]=\E\left[X\right].
\]</span></p>
<p>Noting that</p>
<p><span class="math display">\[
\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}=\frac{\dif^{n-1}}{\dif t^{n-1}}\left[\frac{\dif}{\dif t}\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}\left[\frac{\dif}{\dif t}x\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}x^{2}\mathrm{e}^{tx}=\frac{\dif}{\dif t}x^{n-1}\mathrm{e}^{tx}=x^{n}\mathrm{e}^{tx},
\]</span></p>
<p>we can establish that</p>
<span class="math display">\[\begin{align*}
\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0} &amp; =\frac{\dif^{n}}{\dif t^{n}}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\int_{-\infty}^{\infty}\left(\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\int_{-\infty}^{\infty}\left(x^{n}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0} \\
    &amp; =\E\left[X^{n}\mathrm{e}^{tX}\right]\Big\vert_{t=0} \\
    &amp; =\E\left[X^{n}\right].
\end{align*}\]</span>
</div>


<div class="lemma">
<p><span id="lem:limit-sequence-exponential" class="lemma"><strong>Lemma 2.1  </strong></span>Let <span class="math inline">\(a_{1},a_{2},\ldots\)</span> be a sequence of numbers converging to <span class="math inline">\(a\)</span>, that is, <span class="math inline">\(\lim_{n\rightarrow\infty}a_{n}=a\)</span>. Then</p>
<p><span class="math display">\[
\lim_{n\rightarrow\infty}\left(1+\frac{a_{n}}{n}\right)^{n}=\mathrm{e}^{a}.
\]</span></p>
(This is Lemma 2.3.14 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.)
</div>


<div class="theorem">
<p><span id="thm:mgf-affine-transform" class="theorem"><strong>Theorem 2.9  </strong></span>For any constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the mgf of the random variable <span class="math inline">\(aX+b\)</span> is given by</p>
<p><span class="math display">\[
M_{aX+b}\left(t\right)=\mathrm{e}^{bt}M_{X}\left(at\right).
\]</span></p>
(This is Theorem 2.3.15 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>.; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> By definition,</p>
<p><span class="math display">\[
M_{aX+b}\left(t\right)=
  \E\left[\mathrm{e}^{\left(aX+b\right)t}\right]=
  \E\left[\mathrm{e}^{\left(aX\right)t}\mathrm{e}^{bt}\right]=
  \mathrm{e}^{bt}\E\left[\mathrm{e}^{\left(at\right)X}\right]=
  \mathrm{e}^{bt}M_{X}\left(at\right),
\]</span></p>
proving the theorem.
</div>

</div>
</div>
<div id="multiple-random-variables" class="section level2">
<h2><span class="header-section-number">2.3</span> Multiple random variables</h2>
<div id="conditional-distributions-and-independence" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Conditional distributions and independence</h3>

<div class="definition">
<span id="def:unnamed-chunk-20" class="definition"><strong>Definition 2.4  </strong></span>Let <span class="math inline">\(\left(X,Y\right)\)</span> be a bivariate random vector with joint pdf or pmf <span class="math inline">\(f\left(x,y\right)\)</span> and marginal pdfs or pmfs <span class="math inline">\(f_{X}\left(x\right)\)</span> and <span class="math inline">\(f_{Y}\left(y\right)\)</span>. Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called <em>independent random variables</em> if, for every <span class="math inline">\(x\in\mathbb{R}\)</span> and <span class="math inline">\(y\in\mathbb{R}\)</span>, <span class="math inline">\(f\left(x,y\right)=f_{X}\left(x\right)f_{Y}\left(y\right)\)</span>.
</div>


<div class="theorem">
<p><span id="thm:expected-value-of-two-ind-rvs" class="theorem"><strong>Theorem 2.10  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables.</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(\mathcal{A}\subset\mathbb{R}\)</span> and <span class="math inline">\(\mathcal{B}\subset\mathbb{R}\)</span>, <span class="math inline">\(P\left(\left\{ X\in \mathcal{A}\right\} \cap\left\{ Y\in \mathcal{B}\right\} \right)=P\left(\left\{ X\in \mathcal{A}\right\} \right)P\left(\left\{ Y\in \mathcal{B}\right\} \right)\)</span>; that is, the events <span class="math inline">\(\left\{ X\in \mathcal{A}\right\}\)</span> and <span class="math inline">\(\left\{ Y\in \mathcal{B}\right\}\)</span> are independent events.</li>
<li>Let <span class="math inline">\(g\left(x\right)\)</span> be a function only of <span class="math inline">\(x\)</span> and <span class="math inline">\(h\left(y\right)\)</span> be a function only of <span class="math inline">\(y\)</span>. Then</li>
</ol>
<p><span class="math display">\[
\E\left[g\left(X\right)h\left(Y\right)\right]=\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\]</span></p>
(This is Theorem 4.2.10 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For continuous random variables, part (2) is proved by noting that</p>
<span class="math display">\[\begin{align*}
\E\left[g\left(X\right)h\left(Y\right)\right] &amp; =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f\left(x,y\right)\dif x\dif y \\
    &amp; =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f_{X}\left(x\right)f_{Y}\left(y\right)\dif x\dif y\tag{independence} \\
    &amp; =\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\dif y \\
    &amp; =\left(\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\right)\left(\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\dif y\right) \\
    &amp; =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\end{align*}\]</span>
<p>The result for discrete random variables is proved by replacing integrals by sums. Part (1) can be proved by series of steps similar to those above or by the following argument. Let <span class="math inline">\(g\left(x\right)\)</span> be the indicator function of the set <span class="math inline">\(\mathcal{A}\)</span>. Let <span class="math inline">\(h\left(y\right)\)</span> be the indicator function of the set <span class="math inline">\(\mathcal{B}\)</span>. Note that <span class="math inline">\(g\left(x\right)h\left(y\right)\)</span> is the indicator function of the set <span class="math inline">\(\mathcal{C}\subset\mathbb{R}^{2}\)</span> defined by <span class="math inline">\(\mathcal{C}=\left\{ \left(x,y\right):x\in \mathcal{A},y\in \mathcal{B}\right\}\)</span>. Thus using the expectation equality just proved, we have</p>
<span class="math display">\[\begin{align*}
P\left(\left\{ X\in A\right\} \cap\left\{ Y\in B\right\} \right) &amp; =P\left(\left\{ \left(X,Y\right)\in C\right\} \right) \\
    &amp; =\E\left[g\left(X\right)h\left(Y\right)\right] \\
    &amp; =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right] \\
    &amp; =P\left(\left\{ X\in A\right\} \right)P\left(\left\{ Y\in B\right\} \right).
\end{align*}\]</span>
</div>


<div class="theorem">
<p><span id="thm:mgf-of-sum-of-two-ind-rvs" class="theorem"><strong>Theorem 2.11  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with moment generating functions <span class="math inline">\(M_{X}\left(t\right)\)</span> and <span class="math inline">\(M_{Y}\left(t\right)\)</span>. Then the moment generating function of the random variable <span class="math inline">\(Z=X+Y\)</span> is given by <span class="math inline">\(M_{Z}\left(t\right)=M_{X}\left(t\right)M_{Y}\left(t\right)\)</span>.</p>
(This is Theorem 4.2.12 from <span class="citation">Casella and Berger (<a href="#ref-casella2002statistical">2002</a>)</span>; the following proof is given there.)
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Using the definition of the mgf and Theorem <a href="probability-theory.html#thm:expected-value-of-two-ind-rvs">2.10</a>, we have</p>
<span class="math display">\[
M_{Z}\left(t\right)=
  \E\left[\mathrm{e}^{tZ}\right]=
  \E\left[\mathrm{e}^{t\left(X+Y\right)}\right]=
  \E\left[\mathrm{e}^{tX}\mathrm{e}^{tY}\right]=
  \E\left[\mathrm{e}^{tX}\right]\E\left[\mathrm{e}^{tY}\right]=
  M_{X}\left(t\right)M_{Y}\left(t\right).
\]</span>
</div>

</div>
<div id="covariance-and-correlation" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Covariance and correlation</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-23" class="definition"><strong>Definition 2.5  </strong></span>The <em>covariance</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the number defined by</p>
<span class="math display">\[
\Cov\left(X,Y\right)=\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right].
\]</span>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-24" class="definition"><strong>Definition 2.6  </strong></span>The <em>correlation</em> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the number defined by</p>
<p><span class="math display">\[
\rho_{XY}=\frac{\Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}.
\]</span></p>
The value <span class="math inline">\(\rho_{XY}\)</span> is also called the <em>correlation coefficient</em>.
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-casella2002statistical">
<p>Casella, G., and R.L. Berger. 2002. <em>Statistical Inference</em>. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. <a href="https://books.google.com/books?id=0x\_vAAAAMAAJ" class="uri">https://books.google.com/books?id=0x\_vAAAAMAAJ</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["course-notes.pdf", "course-notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
