#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass amsbook
\begin_preamble
\usepackage{etoolbox}% http://ctan.org/pkg/etoolbox
\makeatletter
\makeatother
\allowdisplaybreaks
\newref{cor}{name=corollary~}
\newref{exa}{name=example~}
\newref{def}{name=definition~}
\newref{thm}{name=theorem~}
\newref{prop}{name=proposition~}
\usepackage{hyperref}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\usepackage{algpseudocode}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
theorems-chap
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, citecolor=blue, urlcolor=blue, plainpages=false, pdfstartview=XYZ, pdfpagelabels"
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Master's Companion
\begin_inset Newline newline
\end_inset

Course Notes For
\begin_inset Newline newline
\end_inset

Math 503.
 Mathematical Statistics
\begin_inset Newline newline
\end_inset

Math 504.
 Numerical Methods
\begin_inset Newline newline
\end_inset

Math 640.
 Bayesian Statistics
\end_layout

\begin_layout Author
Prepared by Sean Wilson
\end_layout

\begin_layout Email
\begin_inset CommandInset href
LatexCommand href
name "sdw62@georgetown.edu"
target "sdw62@georgetown.edu"
type "mailto:"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Part
Background material
\end_layout

\begin_layout Chapter
Analysis
\end_layout

\begin_layout Section
Upper bounds and suprema
\end_layout

\begin_layout Standard
This section is drawn from 
\shape italic
Analysis: with an introduction to proof
\shape default
 (4th ed.) by Steven R.
 Lay.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:inequality-of-reals"

\end_inset

Let 
\begin_inset Formula $x,y\in\mathbb{R}$
\end_inset

 such that 
\begin_inset Formula $x\leq y+\epsilon$
\end_inset

 for every 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Then 
\begin_inset Formula $x\leq y$
\end_inset

.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $S$
\end_inset

 be a subset of 
\begin_inset Formula $\mathbb{R}$
\end_inset

.
 If there exists a real number 
\begin_inset Formula $m$
\end_inset

 such that 
\begin_inset Formula $m\geq s$
\end_inset

 for all 
\begin_inset Formula $s\in S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is called an 
\shape italic
upper bound
\shape default
 for 
\begin_inset Formula $S$
\end_inset

, and we say that 
\begin_inset Formula $S$
\end_inset

 is bounded above.
 If 
\begin_inset Formula $m\leq s$
\end_inset

 for all 
\begin_inset Formula $s\in S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is a 
\shape italic
lower bound
\shape default
 for 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 is bounded below.
 The set 
\begin_inset Formula $S$
\end_inset

 is said to be 
\shape italic
bounded
\shape default
 if it is bounded above and bounded below.
\end_layout

\begin_layout Definition
If an upper bound 
\begin_inset Formula $m$
\end_inset

 for 
\begin_inset Formula $S$
\end_inset

 is a member of 
\begin_inset Formula $S$
\end_inset

, then 
\begin_inset Formula $m$
\end_inset

 is called the 
\shape italic
maximum
\shape default
 (or largest element) of 
\begin_inset Formula $S$
\end_inset

, and we write
\begin_inset Formula 
\[
m=\max S.
\]

\end_inset

Similarly, if a lower bound of 
\begin_inset Formula $S$
\end_inset

 is a member of 
\begin_inset Formula $S$
\end_inset

, then it is called the 
\shape italic
minimum
\shape default
 (or least element) of 
\begin_inset Formula $S$
\end_inset

, denoted by 
\begin_inset Formula $\min S$
\end_inset

.
\end_layout

\begin_layout Definition
A set may have upper or lower bounds, or it may have neither.
 If 
\begin_inset Formula $m$
\end_inset

 is an upper bound for 
\begin_inset Formula $S$
\end_inset

, then any number greater than 
\begin_inset Formula $m$
\end_inset

 is also an upper bound.
 While a set may have many upper and lower bounds, if it has a maximum or
 a minimum, then those values are unique.
 Thus we speak of 
\shape italic
an
\shape default
 upper bound and 
\shape italic
the
\shape default
 maximum.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $S$
\end_inset

 be a nonempty subset of 
\begin_inset Formula $\mathbb{R}$
\end_inset

.
 If 
\begin_inset Formula $S$
\end_inset

 is bounded above, then the least upper bound of 
\begin_inset Formula $S$
\end_inset

 is called its 
\shape italic
supremum
\shape default
 and is denoted by 
\begin_inset Formula $\sup S$
\end_inset

.
 Thus 
\begin_inset Formula $m=\sup S$
\end_inset

 iff
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\backslash
setcounter{enumi}{1}
\end_layout

\end_inset


\begin_inset Formula $m\geq s$
\end_inset

, for all 
\begin_inset Formula $s\in S$
\end_inset

, and
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

if 
\begin_inset Formula $m'<m$
\end_inset

, then there exists 
\begin_inset Formula $s'\in S$
\end_inset

 such that 
\begin_inset Formula $s'>m'$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Definition
If 
\begin_inset Formula $S$
\end_inset

 is bounded below, then the greatest lower bound of 
\begin_inset Formula $S$
\end_inset

 is called its 
\shape italic
infimum
\shape default
 and is denoted by 
\begin_inset Formula $\inf S$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
\begin_inset ERT
status open

\begin_layout Plain Layout

[completeness axiom]
\end_layout

\end_inset

Every nonempty subset 
\begin_inset Formula $S$
\end_inset

 of 
\begin_inset Formula $\mathbb{R}$
\end_inset

 that is bounded above has a least upper bound.
 That is, 
\begin_inset Formula $\sup S$
\end_inset

 exists and is a real number.
\end_layout

\begin_layout Theorem
Given nonempty subsets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 or 
\begin_inset Formula $\mathbb{R}$
\end_inset

, let 
\begin_inset Formula $C$
\end_inset

 denote the set
\begin_inset Formula 
\[
C=\left\{ x+y:x\in A\mbox{ and }y\in B\right\} .
\]

\end_inset

If 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 have suprema, then 
\begin_inset Formula $C$
\end_inset

 has a supremum and 
\begin_inset Formula 
\[
\sup C=\sup A+\sup B.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\sup A=a$
\end_inset

 and 
\begin_inset Formula $\sup B=b$
\end_inset

.
 If 
\begin_inset Formula $z\in C$
\end_inset

, then 
\begin_inset Formula $z=x+y$
\end_inset

 for some 
\begin_inset Formula $x\in A$
\end_inset

 and 
\begin_inset Formula $y\in B$
\end_inset

.
 Thus 
\begin_inset Formula $z=x+y\leq a+b$
\end_inset

, so 
\begin_inset Formula $a+b$
\end_inset

 is an upper bound for 
\begin_inset Formula $C$
\end_inset

.
 By the completeness axiom, 
\begin_inset Formula $C$
\end_inset

 has at least an upper bound, say 
\begin_inset Formula $\sup C=c$
\end_inset

.
 We must show that 
\begin_inset Formula $c=a+b$
\end_inset

.
 Since 
\begin_inset Formula $c$
\end_inset

 is the 
\shape italic
least
\shape default
 upper bound for 
\begin_inset Formula $C$
\end_inset

, we have 
\begin_inset Formula $c\leq a+b$
\end_inset

.
\end_layout

\begin_layout Proof
To see that 
\begin_inset Formula $a+b\leq c$
\end_inset

, choose any 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Since 
\begin_inset Formula $a=\sup A$
\end_inset

, 
\begin_inset Formula $a-\epsilon$
\end_inset

 is not an upper bound for 
\begin_inset Formula $A$
\end_inset

, and there must exist 
\begin_inset Formula $x\in A$
\end_inset

 such that 
\begin_inset Formula $a-\epsilon<x$
\end_inset

.
 Similarly, since 
\begin_inset Formula $b=\sup B$
\end_inset

, there exists 
\begin_inset Formula $y\in B$
\end_inset

 such that 
\begin_inset Formula $b-\epsilon<y$
\end_inset

.
 Combining these inequalities, we have
\begin_inset Formula 
\[
a+b-2\epsilon<x+y\leq c.
\]

\end_inset

That is, 
\begin_inset Formula $a+b<c+2\epsilon$
\end_inset

 for every 
\begin_inset Formula $\epsilon>0$
\end_inset

.
 Thus, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:inequality-of-reals"

\end_inset

, 
\begin_inset Formula $a+b\leq c$
\end_inset

.
 Finally, since 
\begin_inset Formula $c\leq a+b$
\end_inset

 and 
\begin_inset Formula $c\geq a+b$
\end_inset

, we conclude that 
\begin_inset Formula $c=a+b$
\end_inset

.
\end_layout

\begin_layout Chapter
Probability theory
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:prob-partitions"

\end_inset

If 
\begin_inset Formula $P$
\end_inset

 is a probability function, then
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[a.]
\end_layout

\end_inset


\begin_inset Formula $P\left(A\right)=\sum_{i=1}^{\infty}P\left(A\cap C_{i}\right)$
\end_inset

 for any partition 
\begin_inset Formula $C_{1},C_{2},\ldots$
\end_inset

;
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[b.]
\end_layout

\end_inset


\begin_inset Formula $P\left(\cup_{i=1}^{\infty}A_{i}\right)\leq\sum_{i=1}^{\infty}P\left(A_{i}\right)$
\end_inset

 for any sets 
\begin_inset Formula $A_{1},A_{2},\ldots$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bayes' Rule]
\end_layout

\end_inset

Let 
\begin_inset Formula $A_{1},A_{2},\ldots$
\end_inset

 be a partition of the sample space, and let 
\begin_inset Formula $B$
\end_inset

 be any set.
 Then, for each 
\begin_inset Formula $i=1,2,\ldots$
\end_inset

,
\begin_inset Formula 
\[
P\left(A_{i}|B\right)=\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.
\]

\end_inset


\end_layout

\begin_layout Proof
From the definition of conditional probability, we have
\begin_inset Formula 
\begin{flalign*}
P\left(A_{i}|B\right) & =\frac{P\left(A_{i}\cap B\right)}{P\left(B\right)}\\
 & =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{P\left(B\right)}\tag{conditional probability}\\
 & =\frac{P\left(B|A_{i}\right)P\left(A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B\cap A_{j}\right)}\tag{the \ensuremath{A_{i}} partition the sample space}\\
 & =\frac{P\left(B|A_{i}\right)}{\sum_{j=1}^{\infty}P\left(B|A_{j}\right)P\left(A_{j}\right)}.\tag{conditional probability}
\end{flalign*}

\end_inset


\end_layout

\begin_layout Definition
The 
\shape italic
cumulative distribution function
\shape default
 or 
\shape italic
cdf
\shape default
 of a random variable 
\begin_inset Formula $X$
\end_inset

, denoted by 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

, is defined by
\begin_inset Formula 
\[
F_{X}\left(x\right)=P_{X}\left(X\leq x\right),\quad\text{for all }x.
\]

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:properties-of-cdf"

\end_inset

The function 
\begin_inset Formula $F\left(x\right)$
\end_inset

 is a cdf if and only if the following three conditions hold:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset


\begin_inset Formula $\lim_{x\rightarrow-\infty}F\left(x\right)=0$
\end_inset

 and 
\begin_inset Formula $\lim_{x\rightarrow\infty}F\left(x\right)=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset


\begin_inset Formula $F\left(x\right)$
\end_inset

 is a nondecreasing function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(c)]
\end_layout

\end_inset


\begin_inset Formula $F\left(x\right)$
\end_inset

 is right-continuous; that is, for every number 
\begin_inset Formula $x_{0}$
\end_inset

, 
\begin_inset Formula $\lim_{x\downarrow x_{0}}F\left(x\right)=F\left(x_{0}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 1.5.3 from Casella & Berger.)
\end_layout

\begin_layout Proof
To prove necessity, we can write 
\begin_inset Formula $F$
\end_inset

 in terms of the probability function.
 We have
\begin_inset Formula 
\begin{flalign*}
\lim_{x\rightarrow-\infty}F\left(x\right) & =\lim_{x\rightarrow-\infty}P\left(\left\{ X\leq x\right\} \right)\\
 & =
\end{flalign*}

\end_inset

FINISH PROOF
\end_layout

\begin_layout Chapter
Transformations and expectations
\end_layout

\begin_layout Section
Distributions of functions of a random variable
\end_layout

\begin_layout Standard
When transformations are made, it is important to keep track of the sample
 spaces of the random variables; otherwise, much confusion can arise.
 When the transformation is from 
\begin_inset Formula $X$
\end_inset

 to 
\begin_inset Formula $Y=g\left(X\right)$
\end_inset

, it is most convenient to use
\begin_inset Formula 
\begin{equation}
\mathcal{X}=\left\{ x:f_{X}\left(x\right)>0\right\} \qquad\text{and}\qquad\mathcal{Y}=\left\{ y:y=g\left(x\right)\text{ for some }x\in\mathcal{X}\right\} .\label{eq:trans-rv-sample-space}
\end{equation}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:cdf-of-function-of-rv"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 have cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

, let 
\begin_inset Formula $Y=g\left(X\right)$
\end_inset

, and let 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 be defined as in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:trans-rv-sample-space"

\end_inset

.
 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $g$
\end_inset

 is an increasing function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

, 
\begin_inset Formula $F_{Y}\left(y\right)=F_{X}\left(g^{-1}\left(y\right)\right)$
\end_inset

 for 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $g$
\end_inset

 is a decreasing function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is a continuous random variable, 
\begin_inset Formula $F_{Y}\left(y\right)=1-F_{X}\left(g^{-1}\left(y\right)\right)$
\end_inset

 for 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 2.1.3 from Casella & Berger.)
\end_layout

\begin_layout Proof
\begin_inset Formula $g$
\end_inset

 is a monotone, i.e., it maps each 
\begin_inset Formula $x$
\end_inset

 to a single 
\begin_inset Formula $y$
\end_inset

, and each 
\begin_inset Formula $y$
\end_inset

 comes from at most one 
\begin_inset Formula $x$
\end_inset

.
 In this case that 
\begin_inset Formula $g$
\end_inset

 is increasing, we have
\begin_inset Formula 
\begin{flalign*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\}  & =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\leq g^{-1}\left(y\right)\right\} \\
 & =\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\} ,
\end{flalign*}

\end_inset

and the cdf of 
\begin_inset Formula $Y$
\end_inset

 is 
\begin_inset Formula 
\begin{flalign*}
F_{Y}\left(y\right) & =P\left(\left\{ Y\leq y\right\} \right)\\
 & =P\left(\left\{ g\left(X\right)\leq y\right\} \right)\\
 & =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right)\\
 & =P\left(\left\{ x\in\mathcal{X}:x\leq g^{-1}\left(y\right)\right\} \right)\\
 & =\int_{-\infty}^{g^{-1}\left(y\right)}f_{X}\left(x\right)\text{d}x\\
 & =F_{X}\left(g^{-1}\left(y\right)\right).
\end{flalign*}

\end_inset

In the case that 
\begin_inset Formula $g$
\end_inset

 is decreasing, we have
\begin_inset Formula 
\begin{flalign*}
\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\}  & =\left\{ x\in\mathcal{X}:g^{-1}\left(g\left(x\right)\right)\geq g^{-1}\left(y\right)\right\} \\
 & =\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\} ,
\end{flalign*}

\end_inset

and the cdf of 
\begin_inset Formula $Y$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
F_{Y}\left(y\right) & =P\left(\left\{ Y\leq y\right\} \right)\\
 & =P\left(\left\{ g\left(X\right)\leq y\right\} \right)\\
 & =P\left(\left\{ x\in\mathcal{X}:g\left(x\right)\leq y\right\} \right)\\
 & =P\left(\left\{ x\in\mathcal{X}:x\geq g^{-1}\left(y\right)\right\} \right)\\
 & =\int_{g^{-1}\left(y\right)}^{\infty}f_{X}\left(x\right)\text{d}x\\
 & =1-F_{X}\left(g^{-1}\left(y\right)\right).\tag{continuity of \ensuremath{X}}
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:pdf-of-function-of-rv"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 have pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

 and let 
\begin_inset Formula $Y=g\left(X\right)$
\end_inset

, where 
\begin_inset Formula $g$
\end_inset

 is a monotone function.
 Let 
\begin_inset Formula $\mathcal{X}=\left\{ x:f_{X}\left(x\right)>0\right\} $
\end_inset

 and let 
\begin_inset Formula $\mathcal{Y}=\left\{ y:y=g\left(x\right),x\in\mathcal{X}\right\} $
\end_inset

.
 Suppose that 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

 is continuous on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and that 
\begin_inset Formula $g^{-1}\left(y\right)$
\end_inset

 has a continuous derivative on 
\begin_inset Formula $\mathcal{Y}$
\end_inset

.
 Then the pdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{Y}\left(y\right)=\begin{cases}
f_{X}\left(g^{-1}\left(y\right)\right)\left|\dfrac{\dif}{\dif y}g^{-1}\left(y\right)\right|, & y\in\mathcal{Y}\\
0, & \text{otherwise}
\end{cases}.
\]

\end_inset

(This is Theorem 2.1.5 from Casella & Berger.)
\end_layout

\begin_layout Proof
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cdf-of-function-of-rv"

\end_inset

 and applying the chain rule, we have
\begin_inset Formula 
\[
f_{Y}\left(y\right)=\dfrac{\dif}{\dif y}F_{Y}\left(y\right)=f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]

\end_inset

in the case that 
\begin_inset Formula $g$
\end_inset

 is increasing and
\begin_inset Formula 
\[
f_{Y}\left(y\right)=\dfrac{\dif}{\dif y}F_{Y}\left(y\right)=0-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)=-f_{X}\left(g^{-1}\left(y\right)\right)\dfrac{\dif}{\dif y}g^{-1}\left(y\right)
\]

\end_inset

in the case that 
\begin_inset Formula $g$
\end_inset

 is decreasing, which can be expressed concisely as in the theorem.
\end_layout

\begin_layout Standard
We will look at 
\begin_inset Formula $F_{X}^{-1}$
\end_inset

, the inverse of the cdf 
\begin_inset Formula $F_{X}$
\end_inset

.
 If 
\begin_inset Formula $F_{X}$
\end_inset

 is strictly increasing, then 
\begin_inset Formula $F_{X}^{-1}$
\end_inset

 is well defined by 
\begin_inset Formula 
\begin{equation}
F_{X}^{-1}\left(y\right)=x\implies F_{X}\left(x\right)=y.\label{eq:inverse-cdf-increasing}
\end{equation}

\end_inset

However, if 
\begin_inset Formula $F_{X}$
\end_inset

 is constant on some interval, then 
\begin_inset Formula $F_{X}^{-1}$
\end_inset

 is not well defined by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:inverse-cdf-increasing"

\end_inset

.
 Any 
\begin_inset Formula $x$
\end_inset

 satisfying 
\begin_inset Formula $x_{1}\leq x\leq x_{2}$
\end_inset

 satisfies 
\begin_inset Formula $F_{X}\left(x\right)=y$
\end_inset

.
 This problem is avoided by defining 
\begin_inset Formula $F_{X}^{-1}\left(y\right)$
\end_inset

 for 
\begin_inset Formula $0<y<1$
\end_inset

 by
\begin_inset Formula 
\[
F_{X}^{-1}\left(y\right)=\inf\left\{ x:F_{X}\left(x\right)\geq y\right\} ,
\]

\end_inset

a definition that agrees with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:inverse-cdf-increasing"

\end_inset

 when 
\begin_inset Formula $F_{X}$
\end_inset

 is nonconstant and provides an 
\begin_inset Formula $F_{X}^{-1}$
\end_inset

 that is single-valued even when 
\begin_inset Formula $F_{X}$
\end_inset

 is not strictly increasing.
 Using this definition, for some interval 
\begin_inset Formula $\left(x_{1},x_{2}\right)$
\end_inset

 on which 
\begin_inset Formula $F_{X}$
\end_inset

 is constant, we have 
\begin_inset Formula $F_{X}^{-1}\left(y\right)=x_{1}$
\end_inset

.
 At the endpoints of the range of 
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $F_{X}^{-1}\left(y\right)$
\end_inset

 can also be defined.
 
\begin_inset Formula $F_{X}^{-1}\left(1\right)=\infty$
\end_inset

 if 
\begin_inset Formula $F_{X}\left(x\right)<1$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

 and, for any 
\begin_inset Formula $F_{X}$
\end_inset

, 
\begin_inset Formula $F_{X}^{-1}\left(0\right)=-\infty$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Probability integral transformation]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:prob-integral-transform"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 have continuous cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

 and define the random variable 
\begin_inset Formula $Y$
\end_inset

 as 
\begin_inset Formula $Y=F_{X}\left(X\right)$
\end_inset

.
 Then 
\begin_inset Formula $Y$
\end_inset

 is uniformly distributed on 
\begin_inset Formula $\left(0,1\right)$
\end_inset

, that is, 
\begin_inset Formula $P\left(\left\{ Y\leq y\right\} \right)=y$
\end_inset

, 
\begin_inset Formula $0<y<1$
\end_inset

.
 (This is Theorem 2.1.10 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
For 
\begin_inset Formula $Y=F_{X}\left(X\right)$
\end_inset

 we have, for 
\begin_inset Formula $0<y<1$
\end_inset

, 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y\leq y\right\} \right) & =P\left(\left\{ F_{X}\left(X\right)\leq y\right\} \right)\\
 & =P\left(\left\{ F_{X}^{-1}\left[F_{X}\left(X\right)\right]\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{\ensuremath{F_{X}^{-1}} is increasing}\\
 & =P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)\tag{see paragraph below}\\
 & =F_{X}\left(F_{X}^{-1}\left(y\right)\right)\tag{definition of \ensuremath{F_{X}}}\\
 & =y.\tag{continuity of \ensuremath{F_{X}}}
\end{flalign*}

\end_inset

At the endpoints we have 
\begin_inset Formula $P\left(\left\{ Y\leq y\right\} \right)=1$
\end_inset

 for 
\begin_inset Formula $y\geq1$
\end_inset

 and 
\begin_inset Formula $P\left(\left\{ Y\leq y\right\} \right)=0$
\end_inset

 for 
\begin_inset Formula $y\leq0$
\end_inset

, showing that 
\begin_inset Formula $Y$
\end_inset

 has a uniform distribution.
\end_layout

\begin_layout Proof
The reasoning behind the equality
\begin_inset Formula 
\[
P\left(\left\{ F_{X}^{-1}\left(F_{X}\left(X\right)\right)\leq F_{X}^{-1}\left(y\right)\right\} \right)=P\left(\left\{ X\leq F_{X}^{-1}\left(y\right)\right\} \right)
\]

\end_inset

is somewhat subtle and deserves additional attention.
 If 
\begin_inset Formula $F_{X}$
\end_inset

 is strictly increasing, then it is true that 
\begin_inset Formula $F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x$
\end_inset

.
 However, if 
\begin_inset Formula $F_{X}$
\end_inset

 is flat, it may be that 
\begin_inset Formula $F_{X}^{-1}\left(F_{X}\left(x\right)\right)\neq x$
\end_inset

.
 Suppose 
\begin_inset Formula $F_{X}$
\end_inset

 contains an interval 
\begin_inset Formula $\left(x_{1},x_{2}\right)$
\end_inset

 on which 
\begin_inset Formula $F_{X}$
\end_inset

 is constant, and let 
\begin_inset Formula $x\in\left[x_{1},x_{2}\right]$
\end_inset

.
 Then 
\begin_inset Formula $F_{X}^{-1}\left(F_{X}\left(x\right)\right)=x_{1}$
\end_inset

 for any 
\begin_inset Formula $x$
\end_inset

 in this interval.
 Even in this case, though, the probability equality holds, since 
\begin_inset Formula $P\left(\left\{ X\leq x\right\} \right)=P\left(\left\{ X\leq x_{1}\right\} \right)$
\end_inset

 for any 
\begin_inset Formula $x\in\left[x_{1},x_{2}\right]$
\end_inset

.
 The flat cdf denotes a region of 0 probability (
\begin_inset Formula $P\left(\left\{ x_{1}<X\leq x\right\} \right)=F_{X}\left(x\right)-F_{X}\left(x_{1}\right)=0$
\end_inset

).
\end_layout

\begin_layout Section
Expected values
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:properties-of-expectation"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 be a random variable and let 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $b$
\end_inset

, and 
\begin_inset Formula $c$
\end_inset

 be constants.
 Then for any functions 
\begin_inset Formula $g_{1}\left(x\right)$
\end_inset

 and 
\begin_inset Formula $g_{2}\left(x\right)$
\end_inset

 whose expectations exist,
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset


\begin_inset Formula $\E\left[ag_{1}\left(X\right)+bg_{2}\left(X\right)+c\right]=a\E\left[g_{1}\left(X\right)\right]+b\E\left[g_{2}\left(X\right)\right]+c$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

If 
\begin_inset Formula $g_{1}\left(x\right)\geq0$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $\E\left[g_{1}\left(X\right)\right]\geq0$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(c)]
\end_layout

\end_inset

If 
\begin_inset Formula $g_{1}\left(x\right)\geq g_{2}\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $\E\left[g_{1}\left(X\right)\right]\geq\E\left[g_{2}\left(X\right)\right]$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(d)]
\end_layout

\end_inset

If 
\begin_inset Formula $a\leq g_{1}\left(x\right)\leq b$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $a\leq\E\left[g_{1}\left(X\right)\right]\leq b$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 2.2.5 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Section
Moments and moment generating functions
\end_layout

\begin_layout Definition
For each integer 
\begin_inset Formula $n$
\end_inset

, the 
\begin_inset Formula $n\text{th}$
\end_inset

 
\shape italic
moment
\shape default
 of 
\begin_inset Formula $X$
\end_inset

 or 
\begin_inset Formula $(F_{X}\left(x\right)$
\end_inset

), 
\begin_inset Formula $\mu'_{n}$
\end_inset

, is
\begin_inset Formula 
\[
\mu'_{n}=\E\left[X^{n}\right].
\]

\end_inset

The 
\begin_inset Formula $n\text{th}$
\end_inset

 
\shape italic
central moment
\shape default
 of 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $\mu_{n}$
\end_inset

, is
\begin_inset Formula 
\[
\mu_{n}=\E\left[\left(X-\mu\right)^{n}\right],
\]

\end_inset

where 
\begin_inset Formula $\mu=\mu'_{1}=\E\left[X\right]$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $X$
\end_inset

 be a random variable with cdf 
\begin_inset Formula $F_{X}$
\end_inset

.
 The 
\shape italic
moment generating function
\shape default
 (mgf) of 
\begin_inset Formula $X$
\end_inset

 (or 
\begin_inset Formula $F_{X}$
\end_inset

), denoted by 
\begin_inset Formula $M_{X}\left(t\right)$
\end_inset

, is
\begin_inset Formula 
\[
M_{X}\left(t\right)=\E\left[\mathrm{e}^{tX}\right],
\]

\end_inset

provided that the expectation exists for 
\begin_inset Formula $t$
\end_inset

 in some neighborhood of 0.
 That is, there is an 
\begin_inset Formula $h>0$
\end_inset

 such that, for all 
\begin_inset Formula $t$
\end_inset

 in 
\begin_inset Formula $-h<t<h$
\end_inset

, 
\begin_inset Formula $\E\left[\mathrm{e}^{tX}\right]$
\end_inset

 exists.
 If the expectation does not exist in a neighborhood of 0, we say that the
 moment generating function does not exist.
\end_layout

\begin_layout Standard
More explicitly, we can write the mgf of 
\begin_inset Formula $X$
\end_inset

 as
\begin_inset Formula 
\[
M_{X}\left(t\right)=\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\text{d}x\qquad\text{if \ensuremath{X} is continuous,}
\]

\end_inset

or
\begin_inset Formula 
\[
M_{X}\left(t\right)=\sum_{x}\mathrm{e}^{tx}P\left(\left\{ X=x\right\} \right)\qquad\text{if \ensuremath{X} is discrete.}
\]

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $X$
\end_inset

 has mgf 
\begin_inset Formula $M_{X}\left(t\right)$
\end_inset

, then
\begin_inset Formula 
\[
\E\left[X^{n}\right]=M_{X}^{\left(n\right)}\left(0\right),
\]

\end_inset

where we define
\begin_inset Formula 
\[
M_{X}^{\left(n\right)}\left(0\right)=\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0}.
\]

\end_inset

That is, the 
\begin_inset Formula $n\text{th}$
\end_inset

 moment is equal to the 
\begin_inset Formula $n\text{th}$
\end_inset

 derivative of 
\begin_inset Formula $M_{X}\left(t\right)$
\end_inset

 evaluated at 
\begin_inset Formula $t=0$
\end_inset

.
 (This is Theorem 2.3.7 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Assuming that we can differentiate under the integral sign, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\dif}{\dif t}M_{X}\left(t\right) & =\frac{\dif}{\dif t}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x\\
 & =\int_{-\infty}^{\infty}\left(\frac{\dif}{\dif t}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\\
 & =\int_{-\infty}^{\infty}\left(x\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\\
 & =\E\left[X\mathrm{e}^{tX}\right].
\end{flalign*}

\end_inset

Thus,
\begin_inset Formula 
\[
\frac{\dif}{\dif t}M_{X}\left(t\right)\Bigr\vert_{t=0}=\E\left[X\mathrm{e}^{tX}\right]\Big\vert_{t=0}=\E\left[X\mathrm{e}^{0}\right]=\E\left[X\right].
\]

\end_inset

Noting that
\begin_inset Formula 
\[
\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}=\frac{\dif^{n-1}}{\dif t^{n-1}}\left[\frac{\dif}{\dif t}\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}\left[\frac{\dif}{\dif t}x\mathrm{e}^{tx}\right]=\frac{\dif^{n-2}}{\dif t^{n-2}}x^{2}\mathrm{e}^{tx}=\frac{\dif}{\dif t}x^{n-1}\mathrm{e}^{tx}=x^{n}\mathrm{e}^{tx},
\]

\end_inset

we can establish that
\begin_inset Formula 
\begin{flalign*}
\frac{\dif^{n}}{\dif t^{n}}M_{X}\left(t\right)\Bigr\vert_{t=0} & =\frac{\dif^{n}}{\dif t^{n}}\int_{-\infty}^{\infty}\mathrm{e}^{tx}f_{X}\left(x\right)\dif x\Big\vert_{t=0}\\
 & =\int_{-\infty}^{\infty}\left(\frac{\dif^{n}}{\dif t^{n}}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0}\\
 & =\int_{-\infty}^{\infty}\left(x^{n}\mathrm{e}^{tx}\right)f_{X}\left(x\right)\dif x\Big\vert_{t=0}\\
 & =\E\left[X^{n}\mathrm{e}^{tX}\right]\Big\vert_{t=0}\\
 & =\E\left[X^{n}\right].
\end{flalign*}

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:limit-sequence-exponential"

\end_inset

Let 
\begin_inset Formula $a_{1},a_{2},\ldots$
\end_inset

 be a sequence of numbers converging to 
\begin_inset Formula $a$
\end_inset

, that is, 
\begin_inset Formula $\lim_{n\rightarrow\infty}a_{n}=a$
\end_inset

.
 Then
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\left(1+\frac{a_{n}}{n}\right)^{n}=\mathrm{e}^{a}.
\]

\end_inset

(This is Lemma 2.3.14 from Casella & Berger.)
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mgf-affine-transform"

\end_inset

For any constants 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

, the mgf of the random variable 
\begin_inset Formula $aX+b$
\end_inset

 is given by
\begin_inset Formula 
\[
M_{aX+b}\left(t\right)=\mathrm{e}^{bt}M_{X}\left(at\right).
\]

\end_inset

(This is Theorem 2.3.15 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
By definition,
\begin_inset Formula 
\[
M_{aX+b}\left(t\right)=\E\left[\mathrm{e}^{\left(aX+b\right)t}\right]=\E\left[\mathrm{e}^{\left(aX\right)t}\mathrm{e}^{bt}\right]=\mathrm{e}^{bt}\E\left[\mathrm{e}^{\left(at\right)X}\right]=\mathrm{e}^{bt}M_{X}\left(at\right),
\]

\end_inset

proving the theorem.
\end_layout

\begin_layout Chapter
Multiple random variables
\end_layout

\begin_layout Section
Conditional distributions and independence
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 be a bivariate random vector with joint pdf or pmf 
\begin_inset Formula $f\left(x,y\right)$
\end_inset

 and marginal pdfs or pmfs 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

 and 
\begin_inset Formula $f_{Y}\left(y\right)$
\end_inset

.
 Then 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are called 
\shape italic
independent random variables
\shape default
 if, for every 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $y\in\mathbb{R}$
\end_inset

, 
\begin_inset Formula 
\[
f\left(x,y\right)=f_{X}\left(x\right)f_{Y}\left(y\right).
\]

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:expected-value-of-two-ind-rvs"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 be independent random variables.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset

For any 
\begin_inset Formula $A\subset\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $B\subset\mathbb{R}$
\end_inset

, 
\begin_inset Formula $P\left(\left\{ X\in A\right\} \cap\left\{ Y\in B\right\} \right)=P\left(\left\{ X\in A\right\} \right)P\left(\left\{ Y\in B\right\} \right)$
\end_inset

; that is, the events 
\begin_inset Formula $\left\{ X\in A\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ Y\in B\right\} $
\end_inset

 are independent events.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

Let 
\begin_inset Formula $g\left(x\right)$
\end_inset

 be a function only of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $h\left(y\right)$
\end_inset

 be a function only of 
\begin_inset Formula $y$
\end_inset

.
 Then
\begin_inset Formula 
\[
\E\left[g\left(X\right)h\left(Y\right)\right]=\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 4.2.10 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
For continuous random variables, part (b) is proved by noting that
\begin_inset Formula 
\begin{flalign*}
\E\left[g\left(X\right)h\left(Y\right)\right] & =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f\left(x,y\right)\dif x\dif y\\
 & =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g\left(x\right)h\left(y\right)f_{X}\left(x\right)f_{Y}\left(y\right)\dif x\dif y\tag{independence}\\
 & =\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\dif y\\
 & =\left(\int_{-\infty}^{\infty}g\left(x\right)f_{X}\left(x\right)\dif x\right)\left(\int_{-\infty}^{\infty}h\left(y\right)f_{Y}\left(y\right)\dif y\right)\\
 & =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right].
\end{flalign*}

\end_inset

The result for discrete random variables is proved by replacing integrals
 by sums.
 Part (a) can be proved by series of steps similar to those above or by
 the following argument.
 Let 
\begin_inset Formula $g\left(x\right)$
\end_inset

 be the indicator function of the set 
\begin_inset Formula $A$
\end_inset

.
 Let 
\begin_inset Formula $h\left(y\right)$
\end_inset

 be the indicator function of the set 
\begin_inset Formula $B$
\end_inset

.
 Note that 
\begin_inset Formula $g\left(x\right)h\left(y\right)$
\end_inset

 is the indicator function of the set 
\begin_inset Formula $C\subset\mathbb{R}^{2}$
\end_inset

 defined by 
\begin_inset Formula $C=\left\{ \left(x,y\right):x\in A,y\in B\right\} $
\end_inset

.
 Thus using the expectation equality just proved, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X\in A\right\} \cap\left\{ Y\in B\right\} \right) & =P\left(\left\{ \left(X,Y\right)\in C\right\} \right)\\
 & =\E\left[g\left(X\right)h\left(Y\right)\right]\\
 & =\E\left[g\left(X\right)\right]\E\left[h\left(Y\right)\right]\\
 & =P\left(\left\{ X\in A\right\} \right)P\left(\left\{ Y\in B\right\} \right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mgf-of-sum-of-two-ind-rvs"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 be independent random variables with moment generating functions 
\begin_inset Formula $M_{X}\left(t\right)$
\end_inset

 and 
\begin_inset Formula $M_{Y}\left(t\right)$
\end_inset

.
 Then the moment generating function of the random variable 
\begin_inset Formula $Z=X+Y$
\end_inset

 is given by
\begin_inset Formula 
\[
M_{Z}\left(t\right)=M_{X}\left(t\right)M_{Y}\left(t\right).
\]

\end_inset

(This is Theorem 4.2.12 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Using the definition of the mgf and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:expected-value-of-two-ind-rvs"

\end_inset

, we have
\begin_inset Formula 
\[
M_{Z}\left(t\right)=\E\left[\mathrm{e}^{tZ}\right]=\E\left[\mathrm{e}^{t\left(X+Y\right)}\right]=\E\left[\mathrm{e}^{tX}\mathrm{e}^{tY}\right]=\E\left[\mathrm{e}^{tX}\right]\E\left[\mathrm{e}^{tY}\right]=M_{X}\left(t\right)M_{Y}\left(t\right).
\]

\end_inset


\end_layout

\begin_layout Section
Covariance and correlation
\end_layout

\begin_layout Definition
The 
\shape italic
covariance
\shape default
 of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 is the number defined by
\begin_inset Formula 
\[
\Cov\left(X,Y\right)=\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right].
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
The 
\shape italic
correlation
\shape default
 of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 is the number defined by
\begin_inset Formula 
\[
\rho_{XY}=\frac{\Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}.
\]

\end_inset

The value 
\begin_inset Formula $\rho_{XY}$
\end_inset

 is also called the 
\shape italic
correlation coefficient
\shape default
.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:cov-of-ind-rvs"

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent random variables, then 
\begin_inset Formula $\Cov\left(X,Y\right)=0$
\end_inset

 and 
\begin_inset Formula $\rho_{XY}=0$
\end_inset

.
 (This is Theorem 4.5.5 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent, we have 
\begin_inset Formula $\E\left[XY\right]=\E\left[X\right]\E\left[Y\right]$
\end_inset

.
 Thus
\begin_inset Formula 
\begin{flalign*}
\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right] & =\E\left[XY-\mu_{Y}X-\mu_{X}Y+\mu_{X}\mu_{Y}\right]\\
 & =\E\left[XY\right]-\mu_{Y}\E\left[X\right]-\mu_{X}\E\left[Y\right]+\mu_{X}\mu_{Y}\\
 & =\E\left[X\right]\E\left[Y\right]-\E\left[Y\right]\E\left[X\right]-\E\left[X\right]\E\left[Y\right]+\E\left[X\right]\E\left[Y\right]\\
 & =0
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\[
\rho_{XY}=\frac{\Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}=\frac{0}{\sigma_{X}\sigma_{Y}}=0.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:variance-of-ind-rvs"

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are any two random variables and 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are any two constants, then
\begin_inset Formula 
\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right)+2ab\Cov\left(X,Y\right).
\]

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent random variables, then
\begin_inset Formula 
\[
\Var\left(aX+bY\right)=a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right).
\]

\end_inset

(This is Theorem 4.5.6 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
We have
\begin_inset Formula 
\begin{flalign*}
\Var\left(aX+bY\right) & =\E\left[\left(\left(aX+bY\right)-\E\left[aX+bY\right]\right)^{2}\right]\\
 & =\E\left[\left(aX+bY-a\E\left[X\right]-b\E\left[Y\right]\right)^{2}\right]\\
 & =\E\left[\left(aX+bY-a\mu_{X}-b\mu_{Y}\right)^{2}\right]\\
 & =\E\left[\left(a\left(X-\mu_{X}\right)+b\left(Y-\mu_{Y}\right)\right)^{2}\right]\\
 & =\E\left[\left(a\left(X-\mu_{X}\right)\right)^{2}-2ab\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)+\left(b\left(Y-\mu_{Y}\right)\right)^{2}\right]\\
 & =\E\left[a^{2}\left(X-\mu_{X}\right)^{2}\right]-\E\left[2ab\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]+\E\left[b^{2}\left(Y-\mu_{Y}\right)^{2}\right]\\
 & =a^{2}\E\left[\left(X-\mu_{X}\right)^{2}\right]-2ab\E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]+b^{2}\E\left[\left(Y-\mu_{Y}\right)^{2}\right]\\
 & =a^{2}\Var\left(X\right)+b^{2}\Var\left(Y\right)-2ab\Cov\left(X,Y\right).
\end{flalign*}

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cov-of-ind-rvs"

\end_inset

 that 
\begin_inset Formula $\Cov\left(X,Y\right)=0$
\end_inset

 and the second equality is immediate from the first.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:correlation-values"

\end_inset

For any random variables 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

,
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset


\begin_inset Formula $-1\leq\rho_{XY}\leq1$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset


\begin_inset Formula $\left|\rho_{XY}\right|=1$
\end_inset

 if and only if there exist numbers 
\begin_inset Formula $a\neq0$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 such that 
\begin_inset Formula $P\left(\left\{ Y=aX+b\right\} \right)=1$
\end_inset

.
 If 
\begin_inset Formula $\rho_{XY}=1$
\end_inset

, then 
\begin_inset Formula $a>0$
\end_inset

, and if 
\begin_inset Formula $\rho_{XY}=-1$
\end_inset

, then 
\begin_inset Formula $a<0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 4.5.7 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Section
Multivariate distributions
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mgf-of-sum-of-multiple-ind-rvs"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be mutually independent random variables with mgfs 
\begin_inset Formula $M_{X_{1}}\left(t\right),\ldots M_{X_{n}}\left(t\right)$
\end_inset

.
 Let 
\begin_inset Formula $Z=X_{1}+\cdots+X_{n}$
\end_inset

.
 Then the mgf of 
\begin_inset Formula $Z$
\end_inset

 is
\begin_inset Formula 
\[
M_{Z}\left(t\right)=M_{X_{1}}\left(t\right)\cdot\cdots\cdot M_{X_{n}}\left(t\right).
\]

\end_inset

In particular, if 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 all have the same distribution with mgf 
\begin_inset Formula $M_{X}\left(t\right)$
\end_inset

, then
\begin_inset Formula 
\[
M_{Z}\left(t\right)=\left(M_{X}\left(t\right)\right)^{n}.
\]

\end_inset

(This is Theorem 4.6.7 from Casella & Berger, which is a generalization of
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:mgf-of-sum-of-two-ind-rvs"

\end_inset

).
\end_layout

\begin_layout Section
Inequalities
\end_layout

\begin_layout Lemma
\begin_inset ERT
status open

\begin_layout Plain Layout

[Young's Inequality]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem:youngs-inequality"

\end_inset

Let 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 be any positive numbers, and let 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 be any positive numbers (necessarily greater than 1) satisfying
\begin_inset Formula 
\[
\frac{1}{p}+\frac{1}{q}=1.
\]

\end_inset

Then
\begin_inset Formula 
\[
\frac{1}{p}a^{p}+\frac{1}{q}b^{q}\geq ab
\]

\end_inset

with equality if and only if 
\begin_inset Formula $a^{p}=b^{q}$
\end_inset

.
 (This is Lemma 4.7.1 from Casella & Berger; the following proof is given
 there).
\end_layout

\begin_layout Proof
Fix 
\begin_inset Formula $b$
\end_inset

, and consider the function
\begin_inset Formula 
\[
g\left(a\right)=\frac{1}{p}a^{p}+\frac{1}{q}b^{q}-ab.
\]

\end_inset

To minimize 
\begin_inset Formula $g\left(a\right)$
\end_inset

, differentiate and set equal to 0:
\begin_inset Formula 
\[
\frac{\dif}{\dif a}g\left(a\right)=0\implies a^{p-1}-b=0\implies b=a^{p-1}.
\]

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $g\left(a\right)$
\end_inset

 with respect to 
\begin_inset Formula $a$
\end_inset

 at 
\begin_inset Formula 
\[
b=a^{p-1}\implies b^{1/\left(p-1\right)}=\left(a^{p-1}\right)^{1/\left(p-1\right)}\implies a=b^{1/\left(p-1\right)}
\]

\end_inset

 to verify that this is a minimum.
\begin_inset Formula 
\begin{flalign*}
\frac{\dif^{2}}{\dif a^{2}}\left[g\left(a\right)\right|_{a=b^{1/\left(p-1\right)}} & =\frac{\dif}{\dif a}\left[a^{p-1}-b\right|_{a=b^{1/\left(p-1\right)}}\\
 & =\left[\left(p-1\right)a^{p-2}\right|_{a=b^{1/\left(p-1\right)}}\\
 & =\left(p-1\right)\left(b^{1/\left(p-1\right)}\right)^{p-2}\\
 & =\left(p-1\right)b^{\left(p-2\right)/\left(p-1\right)}\\
 & =\left(p-1\right)b^{\left(p-1-1\right)/\left(p-1\right)}\\
 & =\left(p-1\right)b^{\left[\left(p-1\right)/\left(p-1\right)\right]-1/\left(p-1\right)}\\
 & =\left(p-1\right)b^{1-1/\left(p-1\right)}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $p>1$
\end_inset

 and 
\begin_inset Formula $b>0$
\end_inset

, so that 
\begin_inset Formula $p-1>0$
\end_inset

 and 
\begin_inset Formula $b^{1-1/\left(p-1\right)}>0$
\end_inset

.
 It follows that 
\begin_inset Formula $b=a^{p-1}$
\end_inset

 is a minimum.
 We have 
\begin_inset Formula 
\[
\frac{1}{p}+\frac{1}{q}=1\implies\frac{1}{q}=1-\frac{1}{p}=\frac{p-1}{p}\implies q\left(p-1\right)=p,
\]

\end_inset

so that the value of 
\begin_inset Formula $g\left(a\right)$
\end_inset

 at the minimum is
\begin_inset Formula 
\[
\frac{1}{p}a^{p}+\frac{1}{q}\left(a^{p-1}\right)^{q}-aa^{p-1}=\frac{1}{p}a^{p}+\frac{1}{q}a^{p}-a^{p}=a^{p}\left(\frac{1}{p}+\frac{1}{q}-1\right)=a^{p}\left(1-1\right)=0.
\]

\end_inset

Hence the minimum is 0 and the inequality is established.
 The domain of 
\begin_inset Formula $g\left(a\right)$
\end_inset

 is 
\begin_inset Formula $\left\{ a:0<a<\infty\right\} $
\end_inset

 and we have 
\begin_inset Formula $p>1$
\end_inset

, so that for some fixed 
\begin_inset Formula $b$
\end_inset

, 
\begin_inset Formula 
\[
g'\left(a\right)=a^{p-1}-b
\]

\end_inset

is increasing in 
\begin_inset Formula $a$
\end_inset

.
 Thus, the minimum we found is unique, so that equality holds only if 
\begin_inset Formula $a^{p-1}=b$
\end_inset

, which is equivalent to 
\begin_inset Formula 
\[
a^{p-1}=b\implies a^{p/q}=b\implies\left(a^{p/q}\right)^{q}=b^{q}\implies a^{p}=b^{q}.
\]

\end_inset

 
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Hölder's Inequality]
\end_layout

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 be any two random variables, and let 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 satisfy 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:youngs-inequality"

\end_inset

.
 Then
\begin_inset Formula 
\[
\left|\E\left[XY\right]\right|\leq\E\left[\left|XY\right|\right]\leq\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}.
\]

\end_inset

(This is Theorem 4.7.2 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
The first inequality follows from 
\begin_inset Formula $-\left|XY\right|\leq XY\leq\left|XY\right|$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:properties-of-expectation"

\end_inset

.
 To prove the second inequality, define
\begin_inset Formula 
\[
a=\frac{\left|X\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}}\quad\text{and}\quad b=\frac{\left|Y\right|}{\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}.
\]

\end_inset

Applying 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:youngs-inequality"

\end_inset

, we get
\begin_inset Formula 
\begin{flalign*}
\frac{1}{p}a^{p}+\frac{1}{q}b^{q} & \geq ab\\
\implies\frac{1}{p}\left(\frac{\left|X\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}}\right)^{p}+\frac{1}{q}\left(\frac{\left|Y\right|}{\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right)^{q} & \geq\frac{\left|X\right|\left|Y\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\\
\implies\frac{1}{p}\frac{\left|X\right|^{p}}{\E\left[\left|X\right|^{p}\right]}+\frac{1}{q}\frac{\left|Y\right|^{q}}{\E\left[\left|Y\right|^{q}\right]} & \geq\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}.
\end{flalign*}

\end_inset

Taking the expectation of both sides gives
\begin_inset Formula 
\begin{flalign*}
\E\left[\frac{1}{p}\frac{\left|X\right|^{p}}{\E\left[\left|X\right|^{p}\right]}+\frac{1}{q}\frac{\left|Y\right|^{q}}{\E\left[\left|Y\right|^{q}\right]}\right] & \geq\E\left[\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right]\\
\implies\frac{1}{p\E\left[\left|X\right|^{p}\right]}\E\left[\left|X\right|^{p}\right]+\frac{1}{q\E\left[\left|Y\right|^{q}\right]}\E\left[\left|Y\right|^{q}\right] & \geq\E\left[\frac{\left|XY\right|}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\right]\\
\implies\frac{1}{p}+\frac{1}{q} & \geq\frac{1}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\E\left[\left|XY\right|\right]\\
\implies1 & \geq\frac{1}{\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}}\E\left[\left|XY\right|\right]\\
\implies\E\left[\left|XY\right|\right] & \leq\left(\E\left[\left|X\right|^{p}\right]\right)^{1/p}\left(\E\left[\left|Y\right|^{q}\right]\right)^{1/q}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
Perhaps the most famous special case of Hölder's Inequality is that for
 which 
\begin_inset Formula $p=q=2$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Cauchy-Schwarz Inequality]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:cauchy-schwarz-inequality"

\end_inset

For any two random variables 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

,
\begin_inset Formula 
\[
\left|\E\left[XY\right]\right|\leq\E\left[\left|XY\right|\right]\leq\left(\E\left[\left|X\right|^{2}\right]\right)^{1/2}\left(\E\left[\left|Y\right|^{2}\right]\right)^{1/2}.
\]

\end_inset

(This is Theorem 4.7.3 from Casella & Berger.)
\end_layout

\begin_layout Chapter
Properties of a random sample
\end_layout

\begin_layout Section
Sums of random variables from a random sample
\end_layout

\begin_layout Definition
The 
\shape italic
sample variance
\shape default
 is the statistic defined by
\begin_inset Formula 
\[
S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}.
\]

\end_inset

The 
\shape italic
sample standard deviation
\shape default
 is the statistic defined by 
\begin_inset Formula $S=\sqrt{S^{2}}$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:computing-sums-rand-samples"

\end_inset

Let 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 be any numbers and 
\begin_inset Formula $\bar{x}=\left(x_{1}+\cdots+x_{n}\right)/n$
\end_inset

.
 Then
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset


\begin_inset Formula $\min_{a}\sum_{i=1}^{n}\left(x_{i}-a\right)^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset


\begin_inset Formula $\left(n-1\right)s^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 5.2.4 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
To prove part (a), add and subtract 
\begin_inset Formula $\bar{x}$
\end_inset

 to get
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right]\left[\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)\right]\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)\left(x_{i}-\bar{x}\right)+\left(\bar{x}-a\right)^{2}\right]\\
 & =\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)+\left(\bar{x}-a\right)^{2}\right]\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left[2\left(x_{i}-\bar{x}\right)\left(\bar{x}-a\right)\right]+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\sum_{i=1}^{n}\left(x_{i}\bar{x}-ax_{i}-\bar{x}^{2}+a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\sum_{i=1}^{n}x_{i}-a\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\bar{x}^{2}+\sum_{i=1}^{n}a\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}\left(n\bar{x}\right)-a\left(n\bar{x}\right)-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(n\bar{x}^{2}-na\bar{x}-n\bar{x}^{2}+na\bar{x}\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(0\right)+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}.
\end{flalign*}

\end_inset

It is now clear that the right-hand side is minimized at 
\begin_inset Formula $a=\bar{x}$
\end_inset

.
 To prove part (b), take 
\begin_inset Formula $a=0$
\end_inset

 in the above, i.e.,
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\left(x_{i}-a\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-a\right)^{2}\\
\implies\sum_{i=1}^{n}\left(x_{i}-0\right)^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-0\right)^{2}\\
\implies\sum_{i=1}^{n}x_{i}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\bar{x}^{2}\\
\implies\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}\bar{x}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\\
\implies\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2} & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.
\end{flalign*}

\end_inset

The sample variance is defined as 
\begin_inset Formula 
\[
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\implies\left(n-1\right)s^{2}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2},
\]

\end_inset

so the final equality of part (b) holds.
\end_layout

\begin_layout Section
Sampling from the normal distribution
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:rand-sample-from-normal-dist"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution, and let 
\begin_inset Formula $\bar{X}=\left(1/n\right)\sum_{i=1}^{n}X_{i}$
\end_inset

 and 
\begin_inset Formula $S^{2}=\left[1/\left(n-1\right)\right]\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$
\end_inset

.
 Then
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\bar{X}$
\end_inset

 and 
\begin_inset Formula $S^{2}$
\end_inset

 are independent random variables,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\bar{X}$
\end_inset

 has a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}/n\right)$
\end_inset

 distribution,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\left(n-1\right)S^{2}/\sigma^{2}$
\end_inset

 has a chi-squared distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 5.3.1 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution.
 The quantity 
\begin_inset Formula $\left(\bar{X}-\mu\right)/\left(S/\sqrt{n}\right)$
\end_inset

 has 
\shape italic
Student's 
\begin_inset Formula $t$
\end_inset

 distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom
\shape default
.
 Equivalently, a random variable 
\begin_inset Formula $T$
\end_inset

 has Student's 
\begin_inset Formula $t$
\end_inset

 distribution with 
\begin_inset Formula $p$
\end_inset

 degrees of freedom, and we write 
\begin_inset Formula $T\sim t_{p}$
\end_inset

 if it has pdf
\begin_inset Formula 
\[
f_{T}\left(t\right)=\frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)}\frac{1}{\left(p\pi\right)^{1/2}}\frac{1}{\left(1+t^{2}/p\right)^{\left(p+1\right)/2}},\quad-\infty<t<\infty.
\]

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:t-distribution"

\end_inset


\end_layout

\begin_layout Section
Order statistics
\end_layout

\begin_layout Standard
The order statistics of a random sample 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are the sample values placed in ascending order.
 They are denoted by 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

.
 The order statistics are random variables that satisfy 
\begin_inset Formula $X_{\left(1\right)}\leq\ldots\leq X_{\left(n\right)}$
\end_inset

, and in particular, 
\begin_inset Formula $X_{\left(1\right)}=\underset{1\leq i\leq n}{\min}X_{i}$
\end_inset

 and 
\begin_inset Formula $X_{\left(n\right)}=\underset{1\leq i\leq n}{\max}X_{i}$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-discrete"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from a discrete distribution with pmf 
\begin_inset Formula $f_{X}\left(x_{i}\right)=p_{i}$
\end_inset

, where 
\begin_inset Formula $x_{1}<x_{2}<\cdots$
\end_inset

 are the possible values of 
\begin_inset Formula $X$
\end_inset

 in ascending order.
 Define 
\begin_inset Formula 
\begin{flalign*}
P_{0} & =0\\
P_{1} & =p_{1}\\
P_{2} & =p_{1}+p_{2}\\
 & \vdots\\
P_{i} & =p_{1}+p_{2}+\cdots+p_{i}\\
 & \vdots
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics from the sample.
 Then
\begin_inset Formula 
\[
P\left(\left\{ X_{\left(j\right)}\leq x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}P_{i}^{k}\left(1-P_{i}\right)^{n-k}
\]

\end_inset

and
\begin_inset Formula 
\[
P\left(\left\{ X_{\left(j\right)}=x_{i}\right\} \right)=\sum_{k=j}^{n}\binom{n}{k}\left[P_{i}^{k}\left(1-P_{i}\right)^{n-k}-P_{i-1}^{k}\left(1-P_{i-1}\right)^{n-k}\right].
\]

\end_inset

(This is Theorem 5.4.3 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-continuous"

\end_inset

Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics of a random sample, 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, from a continuous population with cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

 and pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

.
 Then the pdf of 
\begin_inset Formula $X_{\left(j\right)}$
\end_inset

 is
\begin_inset Formula 
\[
f_{X_{\left(j\right)}}\left(x\right)=\frac{n!}{\left(j-1\right)!\left(n-j\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{j-1}\left[1-F_{X}\left(x\right)\right]^{n-j}.
\]

\end_inset

(This is Theorem 5.4.4 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:order-stat-joint-pdf"

\end_inset

Let 
\begin_inset Formula $X_{\left(1\right)},\ldots,X_{\left(n\right)}$
\end_inset

 denote the order statistics of a random sample, 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, from a continuous population with cdf 
\begin_inset Formula $F_{X}\left(x\right)$
\end_inset

 and pdf 
\begin_inset Formula $f_{X}\left(x\right)$
\end_inset

.
 Then the joint pdf of 
\begin_inset Formula $X_{\left(i\right)}$
\end_inset

 and 
\begin_inset Formula $X_{\left(j\right)}$
\end_inset

, 
\begin_inset Formula $1\leq i<j\leq n$
\end_inset

, is
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(i\right)},X_{\left(j\right)}}\left(u,v\right) & =\frac{n!}{\left(i-1\right)!\left(j-1-i\right)!\left(n-j\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(u\right)\right]^{i-1}\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{j-1-i}\left[1-F_{X}\left(v\right)\right]^{n-j}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $-\infty<u<v<\infty$
\end_inset

.
 (This is Theorem 5.4.6 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Section
Convergence concepts
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Strong Law of Large Numbers]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:strong-lln"

\end_inset

Let 
\begin_inset Formula $X_{1},X_{2},\ldots$
\end_inset

 be iid random variables with 
\begin_inset Formula $\E\left[X_{i}\right]=\mu$
\end_inset

 and 
\begin_inset Formula $\Var\left(X_{i}\right)=\sigma^{2}<\infty$
\end_inset

, and define 
\begin_inset Formula $\bar{X}_{n}=\left(1/n\right)\sum_{i=1}^{n}X_{i}$
\end_inset

.
 Then, for every 
\begin_inset Formula $\epsilon>0$
\end_inset

, 
\begin_inset Formula 
\[
P\left(\lim_{n\rightarrow\infty}\left|\bar{X}_{n}-\mu\right|<\epsilon\right)=1;
\]

\end_inset

that is, 
\begin_inset Formula $\bar{X}_{n}$
\end_inset

 converges almost surely to 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Central Limit Theorem]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:central-limit"

\end_inset

Let 
\begin_inset Formula $X_{1},X_{2},\ldots$
\end_inset

 be a sequence of iid random variables whose mgfs exist in a neighborhood
 of 0 (that is, 
\begin_inset Formula $M_{X_{i}}\left(t\right)$
\end_inset

 exists for 
\begin_inset Formula $\left|t\right|<h$
\end_inset

, for some positive 
\begin_inset Formula $h$
\end_inset

).
 Let 
\begin_inset Formula $\E\left[X_{i}\right]=\mu$
\end_inset

 and 
\begin_inset Formula $\Var\left(X_{i}\right)=\sigma^{2}>0$
\end_inset

.
 (Both 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 are finite since the mgf exists.) Define 
\begin_inset Formula $\bar{X}_{n}=\left(1/n\right)\sum_{i=1}^{n}X_{i}$
\end_inset

.
 Let 
\begin_inset Formula $G_{n}\left(x\right)$
\end_inset

 denote the cdf of 
\begin_inset Formula $\sqrt{n}\left(\bar{X}-\mu\right)/\sigma$
\end_inset

.
 Then, for any 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $-\infty<x<\infty$
\end_inset

, 
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}G_{n}\left(x\right)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-y^{2}/2}\dif y;
\]

\end_inset

that is, 
\begin_inset Formula $\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma$
\end_inset

 has a limiting standard normal distribution.
 (This is Theorem 5.5.14 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $Z\sim\mathcal{N}\left(0,1\right)$
\end_inset

, so that the mgf of 
\begin_inset Formula $Z$
\end_inset

 given by 
\begin_inset Formula $M_{Z}\left(t\right)=\mathrm{e}^{0\cdot t+\left(1\cdot t^{2}\right)/2}=\mathrm{e}^{t^{2}/2}$
\end_inset

.
 We will show that, for 
\begin_inset Formula $\left|t\right|<h$
\end_inset

, the mgf of 
\begin_inset Formula $\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma$
\end_inset

 converges to 
\begin_inset Formula $\mathrm{e}^{t^{2}/2}$
\end_inset

.
\end_layout

\begin_layout Proof
Define 
\begin_inset Formula $Y_{i}=\left(X_{i}-\mu\right)/\sigma$
\end_inset

, and let 
\begin_inset Formula $M_{Y}\left(t\right)$
\end_inset

 denote the common mgf of the 
\begin_inset Formula $Y_{i}\text{'s}$
\end_inset

, which exists for 
\begin_inset Formula $\left|t\right|<\sigma h$
\end_inset

 and is given by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:mgf-affine-transform"

\end_inset

.
 We have
\begin_inset Formula 
\[
\frac{X_{i}-\mu}{\sigma}=Y_{i}\implies X_{i}-\mu=\sigma Y_{i}\implies X_{i}=\sigma Y_{i}+\mu,
\]

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\frac{\sqrt{n}\left(\bar{X}_{n}-\mu\right)}{\sigma} & =\frac{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right)}{\sigma}\\
 & =\frac{\sqrt{n}}{\sigma}\left[\frac{1}{n}\sum_{i=1}^{n}\left(\sigma Y_{i}+\mu\right)-\mu\right]\\
 & =\frac{\sqrt{n}}{\sigma}\left[\frac{1}{n}\left(\sigma\sum_{i=1}^{n}Y_{i}+n\mu\right)-\mu\right]\\
 & =\frac{\sqrt{n}}{\sigma}\left[\frac{\sigma}{n}\sum_{i=1}^{n}Y_{i}+\mu-\mu\right]\\
 & =\frac{\sqrt{n}}{\sigma}\left(\frac{\sigma}{n}\sum_{i=1}^{n}Y_{i}\right)\\
 & =\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i}.
\end{flalign*}

\end_inset

Then, from the properties of mgfs (see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:mgf-affine-transform"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:mgf-of-sum-of-multiple-ind-rvs"

\end_inset

), we have
\begin_inset Formula 
\[
M_{\sqrt{n}\left(\bar{X}_{n}-\mu\right)/\sigma}\left(t\right)=M_{\sum_{i=1}^{n}Y_{i}/\sqrt{n}}\left(t\right)=M_{\sum_{i=1}^{n}Y_{i}}\left(\frac{t}{\sqrt{n}}\right)=\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}.
\]

\end_inset

We now expand 
\begin_inset Formula $M_{Y}\left(t/\sqrt{n}\right)$
\end_inset

 in a Taylor series (power series) around 0.
 We have
\begin_inset Formula 
\[
M_{Y}\left(\frac{t}{\sqrt{n}}\right)=\sum_{k=0}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!},
\]

\end_inset

where 
\begin_inset Formula $M_{Y}^{\left(k\right)}\left(0\right)=\left(\dif^{k}/\dif t^{k}\right)M_{Y}\left(t\right)\vert_{t=0}.$
\end_inset

 Since the mgfs exist for 
\begin_inset Formula $\left|t\right|<h$
\end_inset

, the power series expansion is valid if 
\begin_inset Formula $t<\sqrt{n}\sigma h$
\end_inset

.
 
\end_layout

\begin_layout Proof
We have 
\begin_inset Formula 
\[
M_{Y}^{\left(0\right)}=\E\left[Y^{0}\right]=\E\left[1\right]=1,
\]

\end_inset


\begin_inset Formula 
\[
M_{Y}^{\left(1\right)}=\E\left[Y^{1}\right]=\E\left[\frac{X-\mu}{\sigma}\right]=\frac{1}{\sigma}\left(\E\left[X\right]-\E\left[\mu\right]\right)=\frac{1}{\sigma}\left(\mu-\mu\right)=0,
\]

\end_inset

and, noting that 
\begin_inset Formula 
\[
\Var\left(X\right)=\E\left[X^{2}\right]-\left(\E\left[X\right]\right)^{2}\implies\sigma^{2}=\E\left[X^{2}\right]-\mu^{2}\implies\E\left[X^{2}\right]=\mu^{2}+\sigma^{2},
\]

\end_inset

we have 
\begin_inset Formula 
\begin{flalign*}
M_{Y}^{\left(2\right)} & =\E\left[Y^{2}\right]\\
 & =\E\left[\left(\frac{X-\mu}{\sigma}\right)^{2}\right]\\
 & =\frac{1}{\sigma^{2}}\E\left[X^{2}-2\mu X+\mu^{2}\right]\\
 & =\frac{1}{\sigma^{2}}\left(\E\left[X^{2}\right]-2\mu\E\left[X\right]+\E\left[\mu^{2}\right]\right)\\
 & =\frac{1}{\sigma^{2}}\left(\mu^{2}+\sigma^{2}-2\mu^{2}+\mu^{2}\right)\\
 & =\frac{1}{\sigma^{2}}\left(\sigma^{2}\right)\\
 & =1.
\end{flalign*}

\end_inset

(By construction, the mean and variance of 
\begin_inset Formula $Y$
\end_inset

 are 0 and 1, respectively.) Then, we have
\begin_inset Formula 
\begin{flalign*}
M_{Y}\left(\frac{t}{\sqrt{n}}\right) & =\sum_{k=0}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\left(\frac{t/\sqrt{n}}{k!}\right)^{k}\\
 & =1\frac{\left(t\sqrt{n}\right)^{0}}{0!}+0\frac{\left(t\sqrt{n}\right)}{1!}+1\frac{\left(t\sqrt{n}\right)^{2}}{2!}+\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!}\\
 & =T_{2}\left(\frac{t}{\sqrt{n}}\right)+R_{2}\left(\frac{t}{\sqrt{n}}\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula 
\[
T_{2}\left(\frac{t}{\sqrt{n}}\right)=1+\frac{\left(t/\sqrt{n}\right)^{2}}{2!}\quad\text{and}\quad R_{2}\left(\frac{t}{\sqrt{n}}\right)=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{\left(t/\sqrt{n}\right)^{k}}{k!}.
\]

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

, so for fixed 
\begin_inset Formula $t\neq0$
\end_inset

, the quantity 
\begin_inset Formula $t/\sqrt{n}\rightarrow0$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

.
 Then, noting that 
\begin_inset Formula $M_{Y}^{\left(2\right)}\left(0\right)$
\end_inset

 exists, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:taylor"

\end_inset

 that 
\begin_inset Formula 
\[
\lim_{t/\sqrt{n}\rightarrow0}\frac{M_{Y}\left(t/\sqrt{n}\right)-T_{2}\left(t/\sqrt{n}\right)}{\left(t/\sqrt{n}-0\right)^{2}}=0\implies\lim_{n\rightarrow\infty}\frac{R_{2}\left(t/\sqrt{n}\right)}{\left(t/\sqrt{n}\right)^{2}}=0.
\]

\end_inset

Since 
\begin_inset Formula $t$
\end_inset

 is fixed, we also have
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\frac{R_{2}\left(t/\sqrt{n}\right)}{\left(1/\sqrt{n}\right)^{2}}=\lim_{n\rightarrow\infty}nR_{2}\left(\frac{t}{\sqrt{n}}\right)=0,
\]

\end_inset

and this is also true at 
\begin_inset Formula $t=0$
\end_inset

 since 
\begin_inset Formula 
\[
R_{2}\left(\frac{0}{\sqrt{n}}\right)=R_{2}\left(0\right)=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\frac{0^{k}}{k!}=\sum_{k=3}^{\infty}M_{Y}^{\left(k\right)}\left(0\right)\cdot0=0.
\]

\end_inset

Thus, for any fixed 
\begin_inset Formula $t$
\end_inset

, we can write
\begin_inset Formula 
\begin{flalign*}
\lim_{n\rightarrow\infty}\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n} & =\lim_{n\rightarrow\infty}\left[1+\frac{\left(t/\sqrt{n}\right)^{2}}{2}+R_{2}\left(\frac{t}{\sqrt{n}}\right)\right]^{n}\\
 & =\lim_{n\rightarrow\infty}\left[1+\frac{t^{2}}{2n}+R_{2}\left(\frac{t}{\sqrt{n}}\right)\right]^{n}\\
 & =\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right)\right]^{n}.
\end{flalign*}

\end_inset

Setting 
\begin_inset Formula $a_{n}=\left(t^{2}/2\right)+nR_{2}\left(t/\sqrt{n}\right)$
\end_inset

, we have
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}a_{n}=\lim_{n\rightarrow\infty}\left[\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right]=\lim_{n\rightarrow\infty}\frac{t^{2}}{2}+\lim_{n\rightarrow\infty}nR_{2}\left(\frac{t}{\sqrt{n}}\right)=\frac{t^{2}}{2}+0=\frac{t^{2}}{2},
\]

\end_inset

i.e., the sequence 
\begin_inset Formula $a_{1},a_{2},\ldots$
\end_inset

 converges to 
\begin_inset Formula $a=t^{2}/2$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:limit-sequence-exponential"

\end_inset

 that
\begin_inset Formula 
\[
\lim_{n\rightarrow\infty}\left(M_{Y}\left(\frac{t}{\sqrt{n}}\right)\right)^{n}=\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^{2}}{2}+nR_{2}\left(\frac{t}{\sqrt{n}}\right)\right)\right]^{n}=\lim_{n\rightarrow\infty}\left[1+\frac{a_{n}}{n}\right]^{n}=\mathrm{e}^{a}=\mathrm{e}^{t^{2}/2}.
\]

\end_inset

Since 
\begin_inset Formula $\mathrm{e}^{t^{2}/2}$
\end_inset

 is the mgf of the 
\begin_inset Formula $\mathcal{N}\left(0,1\right)$
\end_inset

 distribution, the theorem is proved.
\end_layout

\begin_layout Definition
If a function 
\begin_inset Formula $g\left(x\right)$
\end_inset

 has derivatives of order 
\begin_inset Formula $r$
\end_inset

, that is, 
\begin_inset Formula $g^{\left(r\right)}\left(x\right)=\frac{\dif^{r}}{\dif x^{r}}g\left(x\right)$
\end_inset

 exists, then for any constant 
\begin_inset Formula $a$
\end_inset

, the 
\shape italic
Taylor polynomial of order 
\begin_inset Formula $r$
\end_inset

 about 
\begin_inset Formula $a$
\end_inset


\shape default
 is
\begin_inset Formula 
\[
T_{r}\left(x\right)=\sum_{i=0}^{r}\frac{g^{\left(i\right)}\left(a\right)}{i!}\left(x-a\right)^{i}.
\]

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:taylor-polynomial"

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Taylor]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:taylor"

\end_inset

If 
\begin_inset Formula 
\[
g^{\left(r\right)}\left(a\right)=\frac{\dif^{r}}{\dif x^{r}}g\left(x\right)\Bigr\vert_{x=a}
\]

\end_inset

 exists, then
\begin_inset Formula 
\[
\lim_{x\rightarrow a}\frac{g\left(x\right)-T_{r}\left(x\right)}{\left(x-a\right)^{r}}=0.
\]

\end_inset

(This is Theorem 5.5.21 from Casella & Berger).
\end_layout

\begin_layout Part
Lecture notes
\end_layout

\begin_layout Chapter
Common families of distributions
\end_layout

\begin_layout Section
Exponential families
\end_layout

\begin_layout Standard
A family of pdfs (or pmfs) indexed by a parameter 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is called a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family if it can be expressed as 
\begin_inset Formula 
\[
f\left(x|\boldsymbol{\theta}\right)=h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)\right\} 
\]

\end_inset

where 
\begin_inset Formula $h\left(x\right)\geq0$
\end_inset

, 
\begin_inset Formula $c\left(\boldsymbol{\theta}\right)\geq0$
\end_inset

, 
\begin_inset Formula $t_{1}\left(x\right),\ldots,t_{k}\left(x\right)$
\end_inset

 are real-valued functions of 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right)$
\end_inset

 are real-valued functions of the possibly vector-valued parameter 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 I.e., 
\begin_inset Formula $f\left(x|\boldsymbol{\theta}\right)$
\end_inset

 can be expressed in three parts: a part that depends only on the random
 variable(s), a part that depends only on the parameter(s), and a part that
 depends on both the random variable(s) and the parameter(s).
 Most of the parametric models you have studied in Math-501 are exponential
 families, e.g., normal, gamma, beta, binomial, negative binomial, Poisson,
 and multinomial.
 The uniform distribution is not an exponential family (see example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:exp-family-uniform"

\end_inset

 below).
\begin_inset CommandInset label
LatexCommand label
name "sec:defn-exp-family"

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Logistic regression]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:logistic-regression"

\end_inset

For 
\begin_inset Formula $Y_{1},Y_{2},\ldots,Y_{n}$
\end_inset

, let 
\begin_inset Formula $Y_{i}\sim\text{Bernoulli}\left(p\right)$
\end_inset

, i.e., 
\begin_inset Formula 
\[
Y_{i}=\begin{cases}
0, & \mbox{if no event}\\
1, & \mbox{if event.}
\end{cases}
\]

\end_inset

Then the logistic regression model is
\begin_inset Formula 
\[
\log\left(\frac{p}{1-p}\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}
\]

\end_inset

where 
\begin_inset Formula $\log\left(p/\left(1-p\right)\right)$
\end_inset

 is called the logit link.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-binomial"

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{B}\left(n,p\right)$
\end_inset

, where 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

.
 Recall that 
\begin_inset Formula $X$
\end_inset

 represents the number of successes in 
\begin_inset Formula $n$
\end_inset

 i.i.d.
 Bernoulli trials and its pmf is given by 
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}p^{x}\left(1-p\right)^{n-x}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x=0,1,\ldots,n$
\end_inset

 and 
\begin_inset Formula $f\left(x|p\right)=0$
\end_inset

 otherwise.
 Express 
\begin_inset Formula $f\left(x|p\right)$
\end_inset

 in exponential family form.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}p^{x}\left(1-p\right)^{n-x}\\
 & =\binom{n}{x}p^{x}\left(1-p\right)^{n}\left(1-p\right)^{-x}\\
 & =\binom{n}{x}\left(1-p\right)^{n}\left(\frac{p^{x}}{\left(1-p\right)^{x}}\right)\\
 & =\binom{n}{x}\left(1-p\right)^{n}\left(\frac{p}{1-p}\right)^{x}\\
 & =\binom{n}{x}\left(1-p\right)^{n}\exp\left\{ \log\left(\frac{p}{1-p}\right)^{x}\right\} \\
 & =\underbrace{\binom{n}{x}}_{h\left(x\right)}\underbrace{\left(1-p\right)^{n}}_{c\left(p\right)}\exp\left\{ \underbrace{x}_{t_{1}\left(x\right)}\underbrace{\log\left(\frac{p}{1-p}\right)}_{\omega_{1}\left(p\right)}\right\} 
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-poisson"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda>0$
\end_inset

.
 Recall that 
\begin_inset Formula $X$
\end_inset

 represents the frequency with which a specified event occurs given some
 fixed dimension, such as space or time, and its pmf is given by
\begin_inset Formula 
\[
f\left(x|\lambda\right)=\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
\]

\end_inset

for 
\begin_inset Formula $x=0,1,2,\ldots$
\end_inset

 and 
\begin_inset Formula $f\left(x|\lambda\right)=0$
\end_inset

 otherwise.
 Express 
\begin_inset Formula $f\left(x|\lambda\right)$
\end_inset

 in exponential family form.
\begin_inset Formula 
\[
f\left(x|\lambda\right)=\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}=\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ \log\left(\lambda^{x}\right)\right\} =\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ x\log\lambda\right\} 
\]

\end_inset

Then, we have 
\begin_inset Formula $h\left(x\right)=1/x!$
\end_inset

, 
\begin_inset Formula $c\left(\lambda\right)=\mathrm{e}^{-\lambda}$
\end_inset

, 
\begin_inset Formula $t_{1}\left(x\right)=x$
\end_inset

, and 
\begin_inset Formula $\omega_{1}\left(\lambda\right)=\log\lambda$
\end_inset

.
 In a Poisson regression, we have 
\begin_inset Formula $\log\left(\lambda\right)=\beta_{0}+\beta_{1}X_{1}+\ldots+\beta_{k}X_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-normal"

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\mu\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\sigma>0$
\end_inset

.
 A pdf for 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\} 
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\mu,\sigma^{2}\right)$
\end_inset

 in exponential family form.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $\sigma$
\end_inset

 is known.
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{-2\mu x}{2\sigma^{2}}\right\} \\
 & =\underbrace{\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{x^{2}}{2\sigma^{2}}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} }_{c\left(\mu\right)}\exp\left\{ \underbrace{\frac{\mu}{\sigma^{2}}}_{\omega_{1}\left(\mu\right)}\cdot\underbrace{x}_{t_{1}\left(x\right)}\right\} 
\end{flalign*}

\end_inset

Suppose 
\begin_inset Formula $\sigma$
\end_inset

 is unknown.
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi}}\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{x^{2}-2\mu x+\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\frac{1}{\sqrt{2\pi}}\exp\left\{ \log\left(\sigma^{2}\right)^{-1/2}\right\} \exp\left\{ -\frac{x^{2}-2\mu x}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}\right\} \\
 & =\underbrace{\frac{1}{\sqrt{2\pi}}}_{h\left(x\right)}\underbrace{\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} }_{c\left(\mu,\sigma^{2}\right)}\exp\left\{ \underbrace{\frac{1}{\sigma^{2}}}_{\omega_{1}\left(\mu,\sigma^{2}\right)}\cdot\underbrace{\left(-\frac{x^{2}}{2}\right)}_{t_{1}\left(x\right)}+\underbrace{\frac{\mu}{\sigma^{2}}}_{\omega_{2}\left(\mu,\sigma^{2}\right)}\cdot\underbrace{x}_{t_{2}\left(x\right)}\right\} 
\end{flalign*}

\end_inset

Thus, in the case that 
\begin_inset Formula $\sigma$
\end_inset

 is unknown, 
\begin_inset Formula $f\left(x|\mu,\sigma^{2}\right)$
\end_inset

 is a two-parameter exponential family, i.e., we have 
\begin_inset Formula $k=2$
\end_inset

 for 
\begin_inset Formula $\sum_{j=1}^{k}\omega_{j}\left(\theta\right)t_{j}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Definition
The 
\shape italic
indicator function
\shape default
 of a set 
\begin_inset Formula $A$
\end_inset

, most often denoted by 
\begin_inset Formula $I_{A}\left(x\right)$
\end_inset

, is the function
\begin_inset Formula 
\[
I_{A}\left(x\right)=\begin{cases}
1, & x\in A\\
0, & x\notin A
\end{cases}.
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:exp-family-uniform"

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{U}\left(0,\theta\right)$
\end_inset

, where 
\begin_inset Formula $\theta>0$
\end_inset

.
 A pdf for 
\begin_inset Formula $X$
\end_inset

 is given by 
\begin_inset Formula 
\[
f\left(x|\theta\right)=\frac{1}{\theta-0}=\frac{1}{\theta}
\]

\end_inset

for 
\begin_inset Formula $0<x<\theta$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 in exponential family form, if possible.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $A=\left\{ x:x\in\left(0,\theta\right)\right\} $
\end_inset

 and let 
\begin_inset Formula $I_{A}$
\end_inset

 be the indicator function of 
\begin_inset Formula $A$
\end_inset

, i.e.,
\begin_inset Formula 
\[
I_{A}\left(x\right)=\begin{cases}
1, & \mbox{if }x\in A\\
0, & \mbox{if }x\notin A
\end{cases}.
\]

\end_inset

Then, we can write 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 as
\begin_inset Formula 
\[
f\left(x|\theta\right)=\frac{1}{\theta}I_{A}\left(x\right)=\frac{1}{\theta}I_{\left(0,\theta\right)}\left(x\right).
\]

\end_inset

Notice that 
\begin_inset Formula $I_{\left(0,\theta\right)}\left(x\right)$
\end_inset

 is not a function of 
\begin_inset Formula $x$
\end_inset

 exclusively, not a function of 
\begin_inset Formula $\theta$
\end_inset

 exclusively, and cannot be written as an exponential.
 Because the entire pdf must be incorporated into 
\begin_inset Formula $h\left(x\right)$
\end_inset

, 
\begin_inset Formula $c\left(\theta\right)$
\end_inset

, 
\begin_inset Formula $t_{j}\left(x\right)$
\end_inset

, and 
\begin_inset Formula $\omega_{j}\left(\theta\right)$
\end_inset

, it follows that the familiy of pdfs given by 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 is not an exponential family.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Three-parameter exponential family distribution]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:3-param-exp-family"

\end_inset

Consider the family of distributions with densities
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

.
 Express 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 in exponential family form.
\end_layout

\begin_layout Example
Recall that the binomial theorem states that
\begin_inset Formula 
\begin{flalign*}
\left(x+y\right)^{n} & =\sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k},
\end{flalign*}

\end_inset

so we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]\\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\sum_{k=0}^{4}\binom{4}{k}x^{k}\left(-\theta\right)^{4-k}\right\} \\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\left[\binom{4}{0}x^{0}\left(-\theta\right)^{4}+\binom{4}{1}x\left(-\theta\right)^{3}+\binom{4}{2}x^{2}\left(-\theta\right)^{2}+\binom{4}{3}x^{3}\left(-\theta\right)+\binom{4}{4}x^{4}\left(-\theta\right)^{0}\right]\right\} \\
 & =\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -\left[1\cdot1\cdot\theta^{4}-4x\theta^{3}+6x^{2}\theta^{2}-4x^{3}\theta+1\cdot x^{4}\cdot1\right]\right\} \\
 & =\underbrace{\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -x^{4}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\theta^{4}\right\} }_{c\left(\theta\right)}\exp\left\{ \underbrace{4x^{3}}_{t_{1}\left(x\right)}\underbrace{\theta}_{\omega_{1}\left(\theta\right)}\underbrace{-6x^{2}}_{t_{2}\left(x\right)}\underbrace{\theta^{2}}_{\omega_{2}\left(\theta\right)}+\underbrace{4x}_{t_{3}\left(x\right)}\underbrace{\theta^{3}}_{\omega_{3}\left(\theta\right)}\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
Random samples from 
\begin_inset Formula $k$
\end_inset

-parameter exponential families have joint distributions which are 
\begin_inset Formula $k$
\end_inset

-parameter exponential families.
\end_layout

\begin_layout Proof
Suppose that a random variable 
\begin_inset Formula $X$
\end_inset

 has a pdf 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

, and that 
\begin_inset Formula $X_{1},X_{2},\ldots,X_{n}$
\end_inset

 is a random sample from a population having the distribution of 
\begin_inset Formula $X$
\end_inset

.
 It follows that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are independent and identically distributed, and that each 
\begin_inset Formula $X_{i}$
\end_inset

 has the same cdf as 
\begin_inset Formula $X$
\end_inset

, and therefore that 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 is a pdf for each 
\begin_inset Formula $X_{i}$
\end_inset

.
 Then, the joint pdf of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =\prod_{i=1}^{n}f\left(x_{i}|\theta\right)\tag{independence}\\
 & =\prod_{i=1}^{n}\left[h\left(x_{i}\right)c\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \right]\tag{\ensuremath{f} is part of an exponential family}\\
 & =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \tag{\ensuremath{\mathrm{e}^{x}\cdot\mathrm{e}^{y}=\mathrm{e}^{x+y}}}
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula 
\[
h^{*}\left(x\right)=\prod_{i=1}^{n}h\left(x_{i}\right)\quad\text{and}\quad c^{*}\left(\theta\right)=\left[c\left(\theta\right)\right]^{n},
\]

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =\left[\prod_{i=1}^{n}h\left(x_{i}\right)\right]\left[c\left(\theta\right)\right]^{n}\exp\left\{ \sum_{j=1}^{k}\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\omega_{j}\left(\theta\right)\right\} \\
 & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\left(\omega_{j}\left(\theta\right)\sum_{i=1}^{n}t_{j}\left(x_{i}\right)\right)\right\} .
\end{flalign*}

\end_inset

Now, let 
\begin_inset Formula 
\begin{flalign*}
T_{j}\left(x\right) & =\sum_{i=1}^{n}t_{j}\left(x_{i}\right),
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right) & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} .
\end{flalign*}

\end_inset

Thus, the joint pdf 
\begin_inset Formula $f\left(x_{1},x_{2},\ldots,x_{n}|\theta\right)$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family.
\end_layout

\begin_layout Subsection
Natural parameters
\begin_inset CommandInset label
LatexCommand label
name "sec:Natural-parameters"

\end_inset


\end_layout

\begin_layout Standard
An exponential family is sometimes reparametrized as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\boldsymbol{\eta}\right) & =h\left(x\right)c^{*}\left(\boldsymbol{\eta}\right)\exp\left(\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right),
\end{flalign*}

\end_inset

where the natural parameters are defined by 
\begin_inset Formula $\eta_{j}=\omega_{j}\left(\theta\right)$
\end_inset

 and the natural parameter space is
\begin_inset Formula 
\[
\left\{ \boldsymbol{\eta}=\left(\eta_{1},\ldots,\eta_{k}\right):\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x<\infty\right\} 
\]

\end_inset

so that 
\begin_inset Formula 
\[
c^{*}\left(\boldsymbol{\eta}\right)=\frac{1}{\int h\left(x\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\dif x\right\} },
\]

\end_inset

which ensures that the pdf integrates to 1.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial random variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:natural-param-binomial"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Binomial}\left(n,p\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-binomial"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{flalign*}
f\left(x|p\right) & =\binom{n}{x}\left(1-p\right)^{n}\exp\left\{ x\log\left(\frac{p}{1-p}\right)\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $k=1$
\end_inset

 and 
\begin_inset Formula 
\begin{flalign*}
\omega_{1}\left(p\right) & =\log\frac{p}{1-p}.
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta=\omega_{1}\left(p\right)$
\end_inset

, so that
\begin_inset Formula 
\[
\eta=\log\frac{p}{1-p}\implies\mathrm{e}^{\eta}=\frac{p}{1-p}\implies p=\mathrm{e}^{\eta}\left(1-p\right)=\mathrm{e}^{\eta}-\mathrm{e}^{\eta}p\implies\mathrm{e}^{\eta}=p\left(1+\mathrm{e}^{\eta}\right)\implies p=\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}}.
\]

\end_inset

Then, we have
\begin_inset Formula 
\[
c\left(p\right)=\left(1-p\right)^{n}\implies c\left(\eta\right)=\left(1-\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}}\right)^{n}=\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}
\]

\end_inset

and 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\exp\left(x\eta\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-poisson"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\lambda\right) & =\frac{1}{x!}\mathrm{e}^{-\lambda}\exp\left\{ x\log\lambda\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $k=1$
\end_inset

 and
\begin_inset Formula 
\begin{flalign*}
\omega_{1}\left(\lambda\right) & =\log\lambda.
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta=\omega_{1}\left(\lambda\right)$
\end_inset

, so that
\begin_inset Formula 
\[
\eta=\log\lambda\implies\mathrm{e}^{\eta}=\exp\left(\log\lambda\right)\implies\mathrm{e}^{\eta}=\lambda.
\]

\end_inset

Then, we have
\begin_inset Formula 
\[
c\left(\lambda\right)=\mathrm{e}^{-\lambda}\implies c\left(\eta\right)=\exp\left(-\mathrm{e}^{\eta}\right)
\]

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\frac{1}{x!}\exp\left(-\mathrm{e}^{\eta}\right)\exp\left(x\eta\right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bernoulli random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

, i.e., 
\begin_inset Formula $X\sim\text{Binomial}\left(1,p\right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:natural-param-binomial"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\exp\left(x\eta\right).
\end{flalign*}

\end_inset

With 
\begin_inset Formula $n=1$
\end_inset

, we have
\begin_inset Formula 
\[
f\left(x|\eta\right)=\binom{1}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)\exp\left(x\eta\right)=1\cdot\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)\exp\left(x\eta\right)=\frac{1}{1+\mathrm{e}^{\eta}}\exp\left(x\eta\right).
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal random variables]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma>0$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 is unknown.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-normal"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
f\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} \exp\left\{ -x^{2}\frac{1}{2\sigma^{2}}+x\frac{\mu}{\sigma^{2}}\right\} .
\end{flalign*}

\end_inset

Then, let 
\begin_inset Formula $\eta_{1}=\omega_{1}\left(\mu,\sigma^{2}\right)$
\end_inset

, so that
\begin_inset Formula 
\[
\eta_{1}=\frac{1}{\sigma^{2}}\implies\sigma^{2}\eta_{1}=1\implies\sigma^{2}=\frac{1}{\eta_{1}}
\]

\end_inset

and let 
\begin_inset Formula $\eta_{2}=\omega_{2}\left(\mu,\sigma^{2}\right)$
\end_inset

, so that
\begin_inset Formula 
\[
\eta_{2}=\frac{\mu}{\sigma^{2}}\implies\mu=\sigma^{2}\eta_{2}=\frac{\eta_{2}}{\eta_{1}}.
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
c\left(\mu,\sigma^{2}\right) & =\exp\left\{ -\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\log\sigma^{2}\right\} \\
c^{*}\left(\eta_{1},\eta_{2}\right) & =\exp\left\{ -\frac{\left(\frac{\eta_{2}}{\eta_{1}}\right)^{2}}{2\left(\frac{1}{\eta_{1}}\right)}-\frac{1}{2}\log\frac{1}{\eta_{1}}\right\} \\
 & =\exp\left\{ -\frac{\frac{\eta_{2}^{2}}{\eta_{1}^{2}}}{\frac{2}{\eta_{1}}}-\frac{1}{2}\log\frac{1}{\eta_{1}}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\left(\frac{1}{\eta_{1}}\right)^{-1/2}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\left(\left(\eta_{1}\right)^{-1}\right)^{-1/2}\right\} \\
 & =\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\sqrt{\eta_{1}}\right\} 
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta_{1},\eta_{2}\right) & =\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{\eta_{2}^{2}}{2\eta_{1}}+\log\sqrt{\eta_{1}}\right\} \exp\left\{ -\frac{\eta_{1}x^{2}}{2}+\eta_{2}x\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:expected-value-exp-family"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 have density in an exponential family.
 Then,
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\E\left[t_{j}\left(X\right)\right]=-\dfrac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right)$
\end_inset

,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Var\left(t_{j}\left(X\right)\right)=-\dfrac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right)$
\end_inset

,
\end_layout

\end_deeper
\begin_layout Theorem
and the moment-generating function for 
\begin_inset Formula $\left(X_{1},\ldots,X_{k}\right)$
\end_inset

 is
\begin_inset Formula 
\[
M_{\left(X_{1},\ldots,X_{k}\right)}\left(s_{1},\ldots,s_{k}\right)=\E\left[\mathrm{e}^{\sum_{j=1}^{k}s_{j}X_{j}}\right].
\]

\end_inset


\end_layout

\begin_layout Proof
We begin with the pdf of an exponential family, i.e., 
\begin_inset Formula 
\begin{flalign*}
1 & =\int f\left(x|\theta\right)\mbox{d}x\tag{definition of a pdf}\\
 & =\int h\left(x\right)c\left(\theta\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\theta\right)t_{i}\left(x\right)\right)\dif x\tag{\ensuremath{f} is in an exponential family}\\
 & =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x.\tag{natural parameterization}
\end{flalign*}

\end_inset

Taking the derivative of both sides with repect to 
\begin_inset Formula $\eta_{j}$
\end_inset

 gives 
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\eta_{j}}1 & =\frac{\partial}{\partial\eta_{j}}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
\implies0 & =\int\frac{\partial}{\partial\eta_{j}}\left[h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\dif x\\
 & =\int\left[h\left(x\right)\left[\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)+c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\right]\dif x\\
 & =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & \quad+\int h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & \quad+\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\int h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right],
\end{flalign*}

\end_inset


\end_layout

\begin_layout Proof
where the final equality follows from the definition of expected value.
 Observe that for some differentiable function 
\begin_inset Formula $g\left(x\right)$
\end_inset

, we have
\begin_inset Formula 
\[
g'\left(x\right)=\frac{g\left(x\right)}{g\left(x\right)}g'\left(x\right)=g\left(x\right)\frac{\dif}{\dif x}\log\left(g\left(x\right)\right),
\]

\end_inset

which leads to 
\begin_inset Formula 
\begin{flalign*}
0 & =\int h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right]\\
 & =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\int h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right]\\
 & =\frac{\partial}{\partial\eta_{j}}\left(\log c^{*}\left(\eta\right)\right)\cdot1+\E\left[\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(X\right)\right],
\end{flalign*}

\end_inset

where the final equality follows from the fact that the integral of a pdf
 over its range of positivity is equal to 1.
 Then,
\begin_inset Formula 
\begin{flalign*}
-\frac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right) & =\E\left[\frac{\partial}{\partial\eta_{j}}\eta_{1}t_{1}\left(X\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{j}t_{j}\left(X\right)+\ldots+\frac{\partial}{\partial\eta_{j}}\eta_{k}t_{k}\left(X\right)\right]\\
 & =\E\left[0\cdot t_{1}\left(X\right)+\ldots+1\cdot t_{j}\left(X\right)+\ldots+0\cdot t_{k}\left(X\right)\right]\\
 & =\E\left[t_{j}\left(X\right)\right],
\end{flalign*}

\end_inset

proving the first claim.
 Then,
\begin_inset Formula 
\begin{flalign*}
-\frac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right) & =\frac{\partial}{\partial\eta_{j}}\left(-\frac{\partial}{\partial\eta_{j}}\log c^{*}\left(\eta\right)\right)\\
 & =\frac{\partial}{\partial\eta_{j}}\E\left[t_{j}\left(X\right)\right]\\
 & =\frac{\partial}{\partial\eta_{j}}\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\int t_{j}\left(x\right)h\left(x\right)\frac{\partial}{\partial\eta_{j}}c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\int t_{j}\left(x\right)h\left(x\right)\left[\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)+c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\right]\dif x\\
 & =\int t_{j}\left(x\right)h\left(x\right)\frac{\partial}{\partial\eta_{j}}\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & \quad+\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x.
\end{flalign*}

\end_inset

The first summand becomes
\begin_inset Formula 
\begin{flalign*}
 & \quad\,\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\frac{\partial}{\partial\eta_{j}}\log\left(c^{*}\left(\eta\right)\right)\E\left[t_{j}\left(X\right)\right]\\
 & =\left(-\E\left[t_{j}\left(X\right)\right]\right)\E\left[t_{j}\left(X\right)\right]\\
 & =-\left(\E\left[t_{j}\left(X\right)\right]\right)^{2},
\end{flalign*}

\end_inset

where the penultimate equality follows from the first part of the proof.
 The second summand becomes 
\begin_inset Formula 
\begin{flalign*}
 & \quad\,\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\sum_{i=1}^{k}\frac{\partial}{\partial\eta_{j}}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(\frac{\partial}{\partial\eta_{1}}n_{1}t_{1}\left(x\right)+\cdots+\frac{\partial}{\partial\eta_{k}}n_{k}t_{k}\left(x\right)\right)\dif x\\
 & =\int t_{j}\left(x\right)h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\left(0+\cdots+1\cdot t_{j}\left(x\right)+\cdots+0\right)\dif x\\
 & =\int\left(t_{j}\left(x\right)\right)^{2}h\left(x\right)c^{*}\left(\eta\right)\exp\left(\sum_{i=1}^{k}\eta_{i}t_{i}\left(x\right)\right)\dif x\\
 & =\E\left[\left(t_{j}\left(X\right)\right)^{2}\right].
\end{flalign*}

\end_inset

For some random variable 
\begin_inset Formula $Y$
\end_inset

 with defined second central moment, we have 
\begin_inset Formula 
\begin{flalign*}
\Var\left(Y\right) & =\E\left[\left(Y-\E\left[Y\right]\right)^{2}\right]\\
 & =\E\left[Y^{2}-2Y\E\left[Y\right]+\left(\E\left[Y\right]\right)^{2}\right]\\
 & =\E\left[Y^{2}\right]-2\E\left[Y\E\left[Y\right]\right]+\E\left[\left(\E\left[Y\right]\right)^{2}\right]\\
 & =\E\left[Y^{2}\right]-2\E\left[Y\right]\E\left[Y\right]+\left(\E\left[Y\right]\right)^{2}\tag{\ensuremath{\E\left[Y\right]} is constant}\\
 & =\E\left[Y^{2}\right]-2\left(\E\left[Y\right]\right)^{2}+\left(\E\left[Y\right]\right)^{2}\\
 & =\E\left[Y^{2}\right]-\left(\E\left[Y\right]\right)^{2}.
\end{flalign*}

\end_inset

It follows that 
\begin_inset Formula 
\[
-\frac{\partial^{2}}{\partial\eta_{j}^{2}}\log c^{*}\left(\eta\right)=-\left(\E\left[t_{j}\left(X\right)\right]\right)^{2}+\E\left[\left(t_{j}\left(X\right)\right)^{2}\right]=\Var\left(t_{j}\left(X\right)\right),
\]

\end_inset

proving the second claim.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Expected value of a binomial random variable]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Binomial}\left(n,p\right)$
\end_inset

.
 We will find the expected value of 
\begin_inset Formula $X$
\end_inset

 by applying 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:expected-value-exp-family"

\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:natural-param-binomial"

\end_inset

, the pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =\binom{n}{x}\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\exp\left(x\eta\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\eta=\log\left(p/\left(1-p\right)\right)$
\end_inset

, so that 
\begin_inset Formula $p=1/\left(1+\mathrm{e}^{\eta}\right)$
\end_inset

.
 From the general form of a natural parameterization, we have 
\begin_inset Formula $k=1$
\end_inset

, 
\begin_inset Formula $t\left(x\right)=x$
\end_inset

, and 
\begin_inset Formula $c^{*}\left(\eta\right)=\left(1/\left(1+\mathrm{e}^{\eta}\right)\right)^{n}$
\end_inset

.
 Then, we have 
\begin_inset Formula 
\begin{flalign*}
\E\left[X\right] & =\E\left[t\left(X\right)\right]\\
 & =-\frac{\partial}{\partial\eta}\log\left(\frac{1}{1+\mathrm{e}^{\eta}}\right)^{n}\\
 & =-\frac{\partial}{\partial\eta}\log\left(1+\mathrm{e}^{\eta}\right)^{-n}\\
 & =-\frac{\partial}{\partial\eta}\left(-n\log\left(1+\mathrm{e}^{\eta}\right)\right)\\
 & =n\frac{\partial}{\partial\eta}\log\left(1+\mathrm{e}^{\eta}\right)\\
 & =n\frac{\mathrm{e}^{\eta}}{1+\mathrm{e}^{\eta}}\\
 & =np.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mgf-natural-param"

\end_inset

If 
\begin_inset Formula $X$
\end_inset

 has a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family distribution indexed by the natural parameters,
 then for any 
\begin_inset Formula $\eta$
\end_inset

 on the interior of the natural parameter space, the mgf of 
\begin_inset Formula $\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)$
\end_inset

 exists and is given by 
\begin_inset Formula 
\begin{flalign*}
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right) & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\eta+s$
\end_inset

 is the vector 
\begin_inset Formula $\left(\eta_{1}+s_{1},\ldots,\eta_{k}+s_{k}\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Suppose that 
\begin_inset Formula $X$
\end_inset

 is a 
\begin_inset Formula $k$
\end_inset

-parameter exponential family distribution indexed by the natural parameters.
 Then, from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Natural-parameters"

\end_inset

, it has a pdf given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\eta\right) & =h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} .
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:expected-value-exp-family"

\end_inset

 that 
\begin_inset Formula 
\[
M_{\left(t_{1}\left(X\right),\ldots,t_{k}\left(X\right)\right)}\left(s_{1},\ldots,s_{k}\right)=\E\left[\mathrm{e}^{\sum_{j=1}^{k}s_{j}t_{j}\left(X\right)}\right],
\]

\end_inset

with 
\begin_inset Formula $X_{i}$
\end_inset

 replaced by 
\begin_inset Formula $t_{i}\left(X\right)$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\E\left[\mathrm{e}^{\sum_{j=1}^{k}s_{j}t_{j}\left(X\right)}\right] & =\int\exp\left\{ \sum_{j=1}^{k}s_{j}t_{j}\left(x\right)\right\} h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x\\
 & =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}s_{j}t_{j}\left(x\right)+\sum_{j=1}^{k}\eta_{j}t_{j}\left(x\right)\right\} \dif x\\
 & =\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x\\
 & =\frac{c^{*}\left(\eta+s\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\int h\left(x\right)c^{*}\left(\eta+s\right)\exp\left\{ \sum_{j=1}^{k}\left(s_{j}+\eta_{j}\right)t_{j}\left(x\right)\right\} \dif x\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\cdot\int f\left(x|\eta+s\right)\dif x\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)}\cdot1\\
 & =\frac{c^{*}\left(\eta\right)}{c^{*}\left(\eta+s\right)},
\end{flalign*}

\end_inset

establishing the claim.
\end_layout

\begin_layout Definition
A 
\shape italic
curved exponential family
\shape default
 is a family of densities of the form given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:defn-exp-family"

\end_inset

 for which the dimension of the vector 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is equal to 
\begin_inset Formula $d<k$
\end_inset

, where 
\begin_inset Formula $k$
\end_inset

 is the number of terms in the sum in the exponent.
 If 
\begin_inset Formula $d=k$
\end_inset

, the family is a 
\shape italic
full exponential family
\shape default
.
\end_layout

\begin_layout Section
Location and scale families
\end_layout

\begin_layout Standard
Location families, scale families, and location-scale families are constructed
 by specifying a single pdf, 
\begin_inset Formula $f\left(x\right)$
\end_inset

, called the standard pdf for the family.
 Then, all other pdfs in the family are generated by transforming the standard
 pdf in a prescribed way.
\end_layout

\begin_layout Subsection
Location families
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, the family of pdfs indexed by 
\begin_inset Formula $\mu$
\end_inset

, 
\begin_inset Formula $f\left(x-\mu\right)$
\end_inset

, is called the 
\shape italic
location family
\shape default
 with respect to the standard pdf 
\begin_inset Formula $f$
\end_inset

, and 
\begin_inset Formula $\mu$
\end_inset

 is called the l
\shape italic
ocation parameter
\shape default
.
\end_layout

\begin_layout Standard
E.g., 
\begin_inset Formula $f\left(x\right)\sim\mathcal{N}\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $\mathcal{N}\left(\mu,1^{2}\right)$
\end_inset

 is a location family.
 The location parameter 
\begin_inset Formula $\mu$
\end_inset

 simply shifts the pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

 so that the shape of the graph is unchanged but the point on the graph
 that was above 
\begin_inset Formula $x=0$
\end_inset

 under 
\begin_inset Formula $f\left(x\right)$
\end_inset

 is above 
\begin_inset Formula $x=\mu$
\end_inset

 for 
\begin_inset Formula $f\left(x-\mu\right)$
\end_inset

, thus
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ -1\leq X\leq2|X\sim f\left(x\right)\right\} \right) & =P\left(\left\{ \mu-1\leq X\leq\mu+2|X\sim f\left(x-\mu\right)\right\} \right).
\end{flalign*}

\end_inset

Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-location-normal}
\end_layout

\end_inset

 shows the normal distribution with 
\begin_inset Formula $\sigma^{2}=1^{2}$
\end_inset

 and 
\begin_inset Formula $\mu=0$
\end_inset

, 
\begin_inset Formula $\mu=2$
\end_inset

, and 
\begin_inset Formula $\mu=-2$
\end_inset

 in red, blue, and green, respectively.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-of-location-normal, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='example of a normal location family'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(-5,5,length=1000)
\end_layout

\begin_layout Plain Layout

par(mar=c(2,2,0.2,2))
\end_layout

\begin_layout Plain Layout

plot(x,dnorm(x, mean=0, sd=1), type="l", col="blue", ylab="f(x)", cex.lab=0.75,
 cex.axis=0.75, yaxt="n")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=2, sd=1), type="l", col="red")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=-2, sd=1), type="l", col="green")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Scale families
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, for any 
\begin_inset Formula $\sigma>0$
\end_inset

, the family of pdfs 
\begin_inset Formula $\left(1/\sigma\right)f\left(x/\sigma\right)$
\end_inset

, indexed by the parameter 
\begin_inset Formula $\sigma$
\end_inset

, is called the 
\shape italic
scale family
\shape default
 with standard pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 is called the 
\shape italic
scale parameter
\shape default
 of the family.
\end_layout

\begin_layout Standard
E.g., 
\begin_inset Formula $f\left(x\right)\sim\mathcal{N}\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $\mathcal{N}\left(0,\sigma^{2}\right)$
\end_inset

 is a scale family.
 The effect of introducing the scale parameter 
\begin_inset Formula $\sigma$
\end_inset

 is either to stretch (
\begin_inset Formula $\sigma>1$
\end_inset

) or to contract (
\begin_inset Formula $\sigma<1$
\end_inset

) the graph of 
\begin_inset Formula $f\left(x\right)$
\end_inset

 while still maintaining the same basic shape of the graph.
 Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-scale-normal}
\end_layout

\end_inset

 shows the normal distribution with 
\begin_inset Formula $\mu=0$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}=1^{2}$
\end_inset

, 
\begin_inset Formula $\sigma^{2}=0.75^{2}$
\end_inset

, and 
\begin_inset Formula $\sigma^{2}=1.5^{2}$
\end_inset

 in red, blue, and green, respectively.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-of-scale-normal, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='example of a normal scale family'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(-5,5,length=1000)
\end_layout

\begin_layout Plain Layout

par(mar=c(2,2,0.2,2))
\end_layout

\begin_layout Plain Layout

plot(x,dnorm(x, mean=0, sd=0.75), type="l", col="blue", ylab="f(x)", cex.lab=0.75,
 cex.axis=0.75, yaxt="n")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=0, sd=1), type="l", col="red")
\end_layout

\begin_layout Plain Layout

lines(x,dnorm(x, mean=0, sd=1.5), type="l", col="green")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Location-scale families
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Then, for any 
\begin_inset Formula $-\infty<\mu<\infty$
\end_inset

 and any 
\begin_inset Formula $\sigma>0$
\end_inset

, the family of pdfs 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

 is called the location-scale family with standard pdf 
\begin_inset Formula $f\left(x\right)$
\end_inset

.
 
\begin_inset Formula $\mu$
\end_inset

 is called the location parameter and 
\begin_inset Formula $\sigma$
\end_inset

 is called the scale parameter.
 E.g., 
\begin_inset Formula $f\left(x\right)\sim\mathcal{N}\left(0,1^{2}\right)$
\end_inset

, 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 is a location-scale family.
 The effect of introducing both the location and scale parameters is to
 stretch/contract the graph with the scale parameter and shift the graph
 with the location parameter.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:location-scale-family"

\end_inset

Let 
\begin_inset Formula $f\left(x\right)$
\end_inset

 be any pdf.
 Let 
\begin_inset Formula $\mu$
\end_inset

 be any real number, and let 
\begin_inset Formula $\sigma$
\end_inset

 be any positive real number.
 Then 
\begin_inset Formula $X$
\end_inset

 is a random variable with pdf 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

 if and only if there exists a random variable 
\begin_inset Formula $Z$
\end_inset

 with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

 and 
\begin_inset Formula $X=\sigma Z+\mu$
\end_inset

.
 (This is Theorem 3.5.6 from Casella & Berger.)
\end_layout

\begin_layout Proof
Suppose that 
\begin_inset Formula $X\sim\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

.
 Then, the cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(x\right) & =P\left(\left\{ X\leq x\right\} \right)\\
 & =\int_{-\infty}^{x}f\left(x\right)\dif x\\
 & =\int_{-\infty}^{x}\frac{1}{\sigma}f\left(\frac{t-\mu}{\sigma}\right)\dif t\\
 & =\int_{-\infty}^{\left(x-\mu\right)/\sigma}\frac{1}{\sigma}f\left(z\right)\sigma\dif z\tag{\ensuremath{z=\left(t-\mu\right)/\sigma\implies\dif t=\sigma\dif z}}\\
 & =\int_{-\infty}^{\left(x-\mu\right)/\sigma}f\left(z\right)\dif z\\
 & =P\left(\left\{ Z\leq\frac{x-\mu}{\sigma}\right\} \right)\\
 & =P\left(\left\{ Z\leq z\right\} \right)\tag{\ensuremath{z=\left(x-\mu\right)/\sigma}}\\
 & =F_{Z}\left(z\right).
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula $X$
\end_inset

 will be a random variable with the specified pdf if and only if 
\begin_inset Formula $Z=\left(X-\mu\right)/\sigma$
\end_inset

 is also a random variable.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $Z$
\end_inset

 be a random variable with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

.
 Suppose 
\begin_inset Formula $\E\left[Z\right]$
\end_inset

 and 
\begin_inset Formula $\Var\left(Z\right)$
\end_inset

 exist.
 If 
\begin_inset Formula $X$
\end_inset

 is a random variable with pdf 
\begin_inset Formula $\left(1/\sigma\right)f\left(\left(x-\mu\right)/\sigma\right)$
\end_inset

, then 
\begin_inset Formula $\E\left[X\right]=\sigma\E\left[Z\right]+\mu$
\end_inset

, and 
\begin_inset Formula $\Var\left(X\right)=\sigma^{2}\Var\left(Z\right)$
\end_inset

.
 (This is Theorem 3.5.7 from Casella and Berger; the version in the lecture
 slides is a slight restatement.)
\end_layout

\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:location-scale-family"

\end_inset

, there is a random variable 
\begin_inset Formula $Z^{*}$
\end_inset

 with pdf 
\begin_inset Formula $f\left(z\right)$
\end_inset

 and 
\begin_inset Formula $X=\sigma Z^{*}+\mu$
\end_inset

.
 So 
\begin_inset Formula 
\begin{flalign*}
\E\left[X\right] & =\E\left[\sigma Z^{*}+\mu\right]\\
 & =\E\left[\sigma Z^{*}\right]+\E\left[\mu\right]\tag{linearity}\\
 & =\sigma\E\left[Z^{*}\right]+\mu\tag{\ensuremath{\mu} is constant}\\
 & =\sigma\int z^{*}\cdot f\left(z^{*}\right)\dif z^{*}+\mu\tag{definition of expected value}\\
 & =\sigma\int z\cdot f\left(z\right)\dif z+\mu\tag{\ensuremath{Z^{*}} has the same pdf as \ensuremath{Z}}\\
 & =\sigma\E\left[Z\right]+\mu.\tag{definition of expected value}
\end{flalign*}

\end_inset

Then, 
\begin_inset Formula 
\begin{flalign*}
\Var\left(X\right) & =\Var\left(\sigma Z^{*}+\mu\right)\\
 & =\E\left[\left(\left(\sigma Z^{*}+\mu\right)-\E\left[\sigma Z^{*}+\mu\right]\right)^{2}\right]\tag{definition of variance}\\
 & =\E\left[\left(\sigma Z^{*}+\mu-\left(\sigma\E\left[Z^{*}\right]+\mu\right)\right)^{2}\right]\tag{linearity}\\
 & =\E\left[\left(\sigma\left(Z^{*}-\E\left[Z^{*}\right]\right)\right)^{2}\right]\\
 & =\E\left[\sigma^{2}\left(Z^{*}-\E\left[Z^{*}\right]\right)^{2}\right]\\
 & =\sigma^{2}\E\left[\left(Z^{*}-\E\left[Z^{*}\right]\right)^{2}\right]\tag{linearity}\\
 & =\sigma^{2}\Var\left(Z^{*}\right)\tag{definition of variance}\\
 & =\sigma^{2}\Var\left(Z\right),\tag{\ensuremath{Z^{*}} has the same pdf as \ensuremath{Z}}
\end{flalign*}

\end_inset

and the result has been shown.
\end_layout

\begin_layout Chapter
Principles of data reduction
\end_layout

\begin_layout Section
Sufficiency
\end_layout

\begin_layout Standard
The concept of sufficiency attempts to find a statistic 
\begin_inset Formula $T\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 that contains all the information in the sample about the model parameter
 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Definition
A 
\shape italic
statistic
\shape default
 is a function 
\begin_inset Formula $T\left(X\right)$
\end_inset

 of the data, such as mean, variance, max, or min.
 
\begin_inset Formula $T\left(X\right)$
\end_inset

 is a random variable.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
An 
\shape italic
estimate
\shape default
 is a statistic that is intended to be close to a parameter.
\end_layout

\begin_layout Subsection
Data reduction
\end_layout

\begin_layout Standard
Any statistic 
\begin_inset Formula $T\left(X\right)$
\end_inset

 defines a form of data reduction or data summary.
 An investigator who uses only the observed value of the statistic rather
 than the entire observed sample 
\series bold

\begin_inset Formula $\mathbf{X}$
\end_inset


\series default
 will treat as equal two samples 
\series bold

\begin_inset Formula $\mathbf{X}$
\end_inset


\series default
 and 
\series bold

\begin_inset Formula $\mathbf{Y}$
\end_inset


\series default
 that satisfy 
\begin_inset Formula $T\left(x\right)=T\left(y\right)$
\end_inset

 even though the actual sample values may be different in some ways.
 Data reduction in terms of a particular statistic can be thought of as
 a partition of the sample space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 of 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 
\begin_inset Formula $T\left(X\right)$
\end_inset

 reduces the data by partitioning the sample space into sets 
\begin_inset Formula $A_{t}$
\end_inset

, 
\begin_inset Formula $t\in\mathcal{T}$
\end_inset

, defined by 
\begin_inset Formula $A_{t}=\left\{ x:T\left(x\right)=t\right\} $
\end_inset

 where 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=T\left(x\right)\right\} $
\end_inset

 for some 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Waiting time]
\end_layout

\end_inset

Suppose that 
\begin_inset Formula $X$
\end_inset

 represents waiting time, e.g., for a bus.
 We first collect data 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

.
 Suppose that we wish to find the mean waiting time 
\begin_inset Formula $\bar{x}=\left(1/n\right)\sum_{i=1}^{n}x_{i}$
\end_inset

, i.e., 
\begin_inset Formula $T\left(X\right)=\bar{x}$
\end_inset

.
 We calculate 
\begin_inset Formula $\bar{x}=8.32$
\end_inset

.
 Then, all sets of 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 that give 
\begin_inset Formula $\bar{x}=8.32$
\end_inset

 are a partition of the sample space.
 Many points in the sample space have this same mean, and we can consider
 them as belonging to the set 
\begin_inset Formula $\left\{ \left(x_{1},\ldots,x_{n}\right):\bar{x}=8.32\right\} $
\end_inset

, which is also the hyperplane 
\begin_inset Formula $x_{1}+\cdots+x_{n}=\left(8.32\right)n$
\end_inset

.
 In this case, the sample mean 
\begin_inset Formula $\bar{X}$
\end_inset

 (or any statistic 
\begin_inset Formula $T\left(X\right)$
\end_inset

) partitions the sample space into a collection of sets.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a binomial distribution]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "subsec:example-sampling-binom"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Binomial}\left(3,p\right)$
\end_inset

, where 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

.
 Then, 
\begin_inset Formula $X$
\end_inset

 represents the number of successes in 3 trials of a random experiment.
 Suppose that we draw a sample of size 2, so that the sample space consists
 of
\begin_inset Formula 
\[
\left\{ \left(0,0\right),\left(0,1\right),\left(0,2\right),\left(0,3\right),\left(1,1\right),\left(1,2\right),\left(1,3\right),\left(2,2\right),\left(2,3\right),\left(3,3\right)\right\} .
\]

\end_inset

Let 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 be the maximum of the sample, i.e., 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},X_{2}\right)$
\end_inset

.
 Then, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 reduces the data by partitioning the sample space into sets 
\begin_inset Formula $A_{t}$
\end_inset

 with 
\begin_inset Formula $\mathcal{T}=\left\{ 0,1,2,3\right\} $
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
A_{0} & =\left\{ \left(0,0\right)\right\} \\
A_{1} & =\left\{ \left(0,1\right),\left(1,2\right)\right\} \\
A_{2} & =\left\{ \left(0,2\right),\left(1,2\right),\left(2,2\right)\right\} \\
A_{3} & =\left\{ \left(0,3\right),\left(1,3\right),\left(2,3\right),\left(3,3\right)\right\} .
\end{flalign*}

\end_inset


\end_layout

\begin_layout Subsection
Sufficiency principle
\end_layout

\begin_layout Definition
A statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a 
\shape italic
sufficient statistic
\shape default
 for 
\begin_inset Formula $\theta$
\end_inset

 if the conditional distribution of the sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

 given the value of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

, i.e., if 
\begin_inset Formula $P\left(\mathbf{X}|T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
A sufficient statistic for a parameter 
\begin_inset Formula $\theta$
\end_inset

 contains all information that is in the data about 
\begin_inset Formula $\theta$
\end_inset

.
 I.e., given the value of 
\begin_inset Formula $T$
\end_inset

, the sufficient statistic, we can gain no more knowledge about 
\begin_inset Formula $\theta$
\end_inset

 from knowing more about the probability distribution of 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}$
\end_inset

 are two samples and 
\begin_inset Formula $T\left(\mathbf{X}\right)=T\left(\mathbf{Y}\right)$
\end_inset

, then inference about 
\begin_inset Formula $\theta$
\end_inset

 should be the same whether 
\begin_inset Formula $\mathbf{X=x}$
\end_inset

 is observed or 
\begin_inset Formula $\mathbf{Y=\mathbf{y}}$
\end_inset

 is observed.
 Note that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 must be a one-to-one function to be a sufficient statistic.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:suff-stat-ratio"

\end_inset

If 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 is the joint pdf or pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $q\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

 is the pdf or pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

 if, for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the sample space, the ratio 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}|\theta\right)\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

.
 (This is Theorem 6.2.2 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
By definition, if 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

 .
 
\begin_inset Formula 
\begin{flalign}
P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right) & =P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =\frac{P\left(\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}|\theta\right)\right)}
\end{flalign}

\end_inset

(1) is simply an expansion of the notation.
 (2) follows from the definition of conditional probability.
 (3) follows because 
\begin_inset Formula $\left\{ \mathbf{X}=\mathbf{x}\right\} $
\end_inset

 is a subset of 
\begin_inset Formula $\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

, and if we have two sets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $A\cap B=A$
\end_inset

, so that 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)$
\end_inset

.
 (4) follows from setting 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 equal to the numerator, which is the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and setting 
\begin_inset Formula $q\left(T\left(\mathbf{X}|\theta\right)\right)$
\end_inset

 equal to the denominator, which is the pdf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, if 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}|\theta\right)\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, i.e., does not depend on 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a binomial distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 be as in example 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:example-sampling-binom"

\end_inset

.
 If 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, show it, and if not, explain why not.
 We have 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

, where the 
\begin_inset Formula $X_{i}\text{'s}$
\end_inset

 are independent and identically distributed because 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a random sample (by assumption).
 Then, each 
\begin_inset Formula $X_{i}$
\end_inset

 has the same pmf as 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\[
p_{X}\left(x\right)=P\left(\left\{ X=x\right\} \right)=\binom{3}{x}p^{x}\left(1-p\right)^{3-x}
\]

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,3\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}\right)=P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)=P\left(\left\{ X_{1}=x_{1}\right\} \cap\left\{ X_{2}=x_{2}\right\} \right)=P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{2}=x_{2}\right\} \right)=\prod_{i=1}^{2}p_{X}\left(x_{i}\right)
\]

\end_inset

Each 
\begin_inset Formula $X_{i}$
\end_inset

 also has the same cdf as 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\[
F_{X}\left(t\right)=P\left(\left\{ X\leq t\right\} \right)=\sum_{k=0}^{t}p_{X}\left(k\right).
\]

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},X_{2}\right)$
\end_inset

, which is just the order statistic 
\begin_inset Formula $X_{\left(2\right)}$
\end_inset

.
 For some maximum value 
\begin_inset Formula $T\left(\mathbf{x}\right)=t$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X_{\left(2\right)}\leq t\right\} \right) & =P\left(\left\{ X_{1}\leq t\right\} \cap\left\{ X_{2}\leq t\right\} \right)\\
 & =P\left(\left\{ X_{1}\leq t\right\} \right)\cdot P\left(\left\{ X_{2}\leq t\right\} \right)\\
 & =\prod_{i=1}^{2}P\left(\left\{ X_{i}\leq t\right\} \right)\\
 & =\prod_{i=1}^{2}F_{X}\left(t\right)\\
 & =\left[F_{X}\left(t\right)\right]^{2}
\end{flalign*}

\end_inset

so that the pmf of 
\begin_inset Formula $X_{\left(2\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X_{\left(2\right)}}\left(t\right) & =P\left(\left\{ X_{\left(2\right)}=t\right\} \right)\\
 & =P\left(\left\{ X_{\left(2\right)}\leq t\right\} \right)-P\left(\left\{ X_{\left(2\right)}\leq t-1\right\} \right)\\
 & =F_{X_{\left(2\right)}}\left(t\right)-F_{X_{\left(2\right)}}\left(t-1\right)\\
 & =\left[F_{X}\left(t\right)\right]^{2}-\left[F_{X}\left(t-1\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\sum_{k=0}^{t-1}p_{X}\left(k\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right]^{2}\\
 & =\left[\sum_{k=0}^{t}p_{X}\left(k\right)\right]^{2}-\left[\left(\sum_{k=0}^{t}p_{X}\left(k\right)\right)^{2}-2p_{X}\left(t\right)\sum_{k=0}^{t}p_{X}\left(k\right)+\left[p_{X}\left(t\right)\right]^{2}\right]\\
 & =2p_{X}\left(t\right)\sum_{k=0}^{t}p_{X}\left(k\right)-\left[p_{X}\left(t\right)\right]^{2}\\
 & =p_{X}\left(t\right)\left[2\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right].
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula 
\begin{flalign*}
P\left(\mathbf{X}|T\left(\mathbf{X}\right)\right) & =\frac{P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)}{P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)}\\
 & =\frac{\prod_{i=1}^{2}p_{X}\left(x_{i}\right)}{p_{X}\left(t\right)\left[2\sum_{k=0}^{t}p_{X}\left(k\right)-p_{X}\left(t\right)\right]}\\
 & =\frac{\left[\binom{3}{x_{1}}p^{x_{1}}\left(1-p\right)^{3-x_{1}}\right]\left[\binom{3}{x_{2}}p^{x_{2}}\left(1-p\right)^{3-x_{2}}\right]}{\left[\binom{3}{t}p^{t}\left(1-p\right)^{3-t}\right]\left[2\sum_{k=0}^{t}\left(\binom{3}{k}p^{k}\left(1-p\right)^{3-k}\right)-\binom{3}{t}p^{t}\left(1-p\right)^{3-t}\right]}.
\end{flalign*}

\end_inset

Clearly, this expression depends on 
\begin_inset Formula $p$
\end_inset

, so 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(\mathbf{X}\right)$
\end_inset

 is not a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a uniform distribution]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:suff-statistic-uniform"

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{U}\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},\ldots,X_{n}\right)$
\end_inset

.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-uniform"

\end_inset

, The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{X}\left(x\right)=\frac{1}{\theta}I_{\left(0,\theta\right)}\left(x\right),
\]

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot\ldots\cdot P\left(\left\{ X_{n}=x_{n}\right\} \right)\\
 & =\prod_{i=1}^{n}f_{X}\left(x_{i}\right)\\
 & =\left(\frac{1}{\theta}\right)^{n}\prod_{i=1}^{n}I_{\left(0,\theta\right)}\left(x_{i}\right)\\
 & =\frac{1}{\theta^{n}}\left[I_{\left(0,\theta\right)}\left(x_{1}\right)\cdot I_{\left(0,\theta\right)}\left(x_{2}\right)\cdot\ldots I_{\left(0,\theta\right)}\left(x_{n}\right)\right]\\
 & =\frac{1}{\theta^{n}}I_{\left(0,\theta\right)}\left(x_{\left(n\right)}\right),
\end{flalign*}

\end_inset

where the final equality follows from the fact that if 
\begin_inset Formula $x_{\left(n\right)}=\max_{1\leq i\leq n}x_{i}\in\left(0,\theta\right)$
\end_inset

, then 
\begin_inset Formula $x_{\left(i\right)}\in\left(0,\theta\right)$
\end_inset

, 
\begin_inset Formula $1\leq i<n$
\end_inset

.
 The cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
F_{X}\left(x\right) & =\int_{-\infty}^{x}f_{X}\left(t\right)\dif t\\
 & =\int_{-\infty}^{x}\frac{1}{\theta}I_{\left(0,\theta\right)}\left(t\right)\dif t\\
 & =\int_{-\infty}^{0}0\dif t+\int_{0}^{x}\frac{1}{\theta}\dif t\\
 & =0+\left(\frac{1}{\theta}\int_{0}^{x}1\dif t\right)\\
 & =\frac{1}{\theta}\left(t\Big\rvert_{0}^{x}\right)\\
 & =\frac{1}{\theta}\left(x-0\right)\\
 & =\frac{x}{\theta}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\max\left(X_{1},\ldots,X_{n}\right)$
\end_inset

, which is just the order statistic 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-continuous"

\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(n\right)}}\left(t\right) & =\frac{n!}{\left(j-1\right)!\left(n-j\right)!}f_{X}\left(t\right)\left[F_{X}\left(t\right)\right]^{j-1}\left[1-F_{X}\left(t\right)\right]^{n-j}\\
 & =\frac{n!}{\left(n-1\right)!\left(n-n\right)!}f_{X}\left(t\right)\left[F_{X}\left(t\right)\right]^{n-1}\left[1-F_{X}\left(t\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-1\right)!}\left(\frac{1}{\theta}I_{\left(0,\theta\right)}\left(t\right)\right)\left[\frac{t}{\theta}\right]^{n-1}\left[1-\frac{t}{\theta}\right]^{0}\\
 & =n\left(\frac{1}{\theta}\right)\left(\frac{t}{\theta}\right)^{n-1}I_{\left(0,\theta\right)}\left(t\right)\\
 & =n\left(\frac{1}{\theta}\right)\frac{t^{n-1}}{\theta^{n-1}}I_{\left(0,\theta\right)}\left(t\right)\\
 & =nt^{n-1}\left(\frac{1}{\theta}\right)\left(\frac{1}{\theta^{n-1}}\right)I_{\left(0,\theta\right)}\left(t\right)\\
 & =nt^{n-1}\left(\frac{1}{\theta^{n}}\right)I_{\left(0,\theta\right)}\left(t\right).
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)$
\end_inset

 be the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(T\left(\mathbf{X}\right)|\theta\right)$
\end_inset

 be the pdf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{X}\right)|\theta\right)} & =\frac{\left(1/\theta^{n}\right)I_{\left(0,\theta\right)}\left(x_{\left(n\right)}\right)}{nt^{n-1}\left(1/\theta^{n}\right)I_{\left(0,\theta\right)}\left(t\right)}\\
 & =\frac{1}{nx^{n-1}}\frac{I_{\left(0,\theta\right)}\left(x_{\left(n\right)}\right)}{I_{\left(0,\theta\right)}\left(t\right)}\\
 & =\frac{1}{nx^{n-1}}\frac{I_{\left(0,\theta\right)}\left(x_{\left(n\right)}\right)}{I_{\left(0,\theta\right)}\left(x_{\left(n\right)}\right)}\tag{\ensuremath{t=x_{\left(n\right)}}}\\
 & =\frac{1}{nx^{n-1}}.
\end{flalign*}

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $p\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{X}\right)|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a Poisson distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{1}+X_{2}$
\end_inset

.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
The pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\left\{ X_{2}\cap x_{x}\right\} \right)\\
 & =P\left(\left\{ X_{2}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{2}=x_{2}\right\} \right)\\
 & =p_{X_{1}}\left(x_{1}\right)\cdot p_{X_{2}}\left(x_{2}\right)\\
 & =\left(\frac{\mathrm{e}^{-\lambda}\lambda^{x_{1}}}{x_{1}!}\right)\left(\frac{\mathrm{e}^{-\lambda}\lambda^{x_{2}}}{x_{2}!}\right)\\
 & =\frac{\mathrm{e}^{-2\lambda}\lambda^{x_{1}+x_{2}}}{x_{1}!x_{2}!}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $z=x_{1}+x_{2}$
\end_inset

.
 Then, the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign}
P\left(\left\{ X_{1}+X_{2}=z\right\} \right) & =\sum_{k=0}^{z}P\left(\left\{ X_{1}=k\right\} \cap\left\{ X_{2}=z-k\right\} \right)\\
 & =\sum_{k=0}^{z}P\left(\left\{ X_{1}=k\right\} \right)\cdot P\left(\left\{ X_{2}=z-k\right\} \right)\\
 & =\sum_{k=0}^{z}\left(\frac{\mathrm{e}^{-\lambda}\lambda^{k}}{k!}\right)\left(\frac{\mathrm{e}^{-\lambda}\lambda^{z-k}}{\left(z-k\right)!}\right)\\
 & =\sum_{k=0}^{z}\frac{\mathrm{e}^{-2\lambda}\lambda^{z}}{k!\left(z-k\right)!}\\
 & =\mathrm{e}^{-2\lambda}\lambda^{z}\sum_{k=0}^{z}\frac{1}{k!\left(z-k\right)!}\\
 & =\frac{\mathrm{e}^{-2\lambda}\lambda^{z}}{z!}\sum_{k=0}^{z}\frac{z!}{k!\left(z-k\right)!}\\
 & =\frac{\mathrm{e}^{-2\lambda}\lambda^{z}}{z!}\cdot2^{z}\\
 & =\frac{\mathrm{e}^{-2\lambda}\left(2\lambda\right)^{z}}{z!}\\
 & =\frac{\mathrm{e}^{-2\lambda}\left(2\lambda\right)^{x_{1}+x_{2}}}{\left(x_{1}+x_{2}\right)!}
\end{flalign}

\end_inset

so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{1}+X_{2}\sim P\left(2\lambda\right)$
\end_inset

.
 (1) follows from Proposition 6.18 in Weiss' text, which gives the pmf of
 the sum of two discrete random variables.
 (2) follows because 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 are independent.
 (3) follows because 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 (4) and (5) follow from algebra.
 (6) follows by multiplying by 
\begin_inset Formula $1=z!/z!$
\end_inset

 and rearrangement.
 (7) follows from the binomial theorem, which gives 
\begin_inset Formula 
\begin{flalign*}
\left(x+y\right)^{n} & =\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\[
2^{z}=\left(1+1\right)^{z}=\sum_{k=0}^{z}\binom{z}{k}1^{z-k}1^{k}=\sum_{k=0}^{z}\binom{z}{k}=\sum_{k=0}^{z}\frac{z!}{k!\left(z-k\right)!}.
\]

\end_inset

(8) and (9) follow from algebra.
 Let 
\begin_inset Formula $p\left(\mathbf{x}|\lambda\right)$
\end_inset

 be the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(T\left(\mathbf{X}\right)|\lambda\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\lambda\right)}{q\left(T\left(\mathbf{X}\right)|\lambda\right)} & =\frac{\frac{e^{-2\lambda}\lambda^{x_{1}+x_{2}}}{x_{1}!x_{2}!}}{\frac{e^{-2\lambda}\left(2\lambda\right)^{x_{1}+x_{2}}}{\left(x_{1}+x_{2}\right)!}}\\
 & =\frac{\lambda^{x_{1}+x_{2}}\left(x_{1}+x_{2}\right)!}{\left(2\lambda\right)^{x_{1}+x_{2}}x_{1}!x_{2}!}\\
 & =\frac{\lambda^{x_{1}+x_{2}}\left(x_{1}+x_{2}\right)!}{2^{x_{1}+x_{2}}\lambda^{x_{1}+x_{2}}x_{1}!x_{2}!}\\
 & =\frac{\left(x_{1}+x_{2}\right)!}{x_{1}!x_{2}!}\left(\frac{1}{2}\right)^{x_{1}+x_{2}}.
\end{flalign*}

\end_inset

Clearly, this expression does not depend on 
\begin_inset Formula $\lambda$
\end_inset

, so it follows that 
\begin_inset Formula $p\left(\mathbf{x}|\lambda\right)/q\left(T\left(\mathbf{X}\right)|\lambda\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\lambda$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sampling from a Bernoulli distribution]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

.
 Let 
\begin_inset Formula $T\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

 (this is the MLE 
\begin_inset Formula $\hat{p}$
\end_inset

).
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
A Bernoulli random variable is a binomial random variable with 
\begin_inset Formula $n=1$
\end_inset

, so the pmf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{X}\left(x\right)=\binom{1}{x}p^{x}\left(1-p\right)^{1-x}=p^{x}\left(1-p\right)^{1-x}
\]

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Then, the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}\right) & =P\left(\left\{ X_{1}=x_{1}\right\} \cap\cdots\cap\left\{ X_{n}=x_{n}\right\} \right)\\
 & =P\left(\left\{ X_{1}=x_{1}\right\} \right)\cdot\ldots\cdot P\left(\left\{ X_{n}=x_{n}\right\} \right)\\
 & =\prod_{i=1}^{n}p_{X}\left(x_{i}\right)\\
 & =\prod_{i=1}^{n}p^{x_{i}}\left(1-p\right)^{1-x_{i}}\\
 & =p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{\sum_{i=1}^{n}\left(1-x_{i}\right)}\\
 & =p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}.
\end{flalign*}

\end_inset

Proposition 6.20 from Weiss' text states that if 
\begin_inset Formula $Y_{1},\ldots,Y_{m}$
\end_inset

 are independent random variables with 
\begin_inset Formula $Y_{j}\sim\mathcal{B}\left(n_{j},p\right)$
\end_inset

 for 
\begin_inset Formula $1\leq j\leq m$
\end_inset

, then 
\begin_inset Formula $Y_{1}+\cdots+Y_{m}\sim\mathcal{B}\left(n_{1}+\cdots n_{m},p\right)$
\end_inset

.
 It follows that 
\begin_inset Formula 
\[
\sum_{i=1}^{n}X_{i}\sim\mathcal{B}\left(\sum_{i=1}^{n}1,p\right)=\mathcal{B}\left(n,p\right).
\]

\end_inset

We have 
\begin_inset Formula $T\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$
\end_inset

, so it follows that 
\begin_inset Formula $nT\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

, so that 
\begin_inset Formula $nT\left(\mathbf{X}\right)\sim\mathcal{B}\left(n,p\right)$
\end_inset

.
 Then, the pmf of 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ nT\left(\mathbf{X}\right)=\sum_{i=1}^{n}x_{i}\right\} \right) & =\binom{n}{\sum_{i=1}^{n}x_{i}}p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $p\left(\mathbf{x}|p\right)$
\end_inset

 be the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and let 
\begin_inset Formula $q\left(nT\left(\mathbf{X}\right)|p\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

, we have 
\begin_inset Formula 
\[
\frac{p\left(\mathbf{x}|p\right)}{q\left(nT\left(\mathbf{X}\right)|p\right)}=\frac{p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}}{\binom{n}{\sum_{i=1}^{n}x_{i}}p^{\sum_{i=1}^{n}x_{i}}\left(1-p\right)^{n-\sum_{i=1}^{n}x_{i}}}=\frac{1}{\binom{n}{\sum_{i=1}^{n}x_{i}}}.
\]

\end_inset

Clearly, this expression does not depend on 
\begin_inset Formula $p$
\end_inset

, so it follows that 
\begin_inset Formula $nT\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\hat{p}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Subsection
Factorization theorem
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Factorization Theorem]
\end_layout

\end_inset

Let 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 be the joint pdf or pmf of a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 A statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

 if and only if there exist functions 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

 for all sample points 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and all 
\begin_inset Formula $\theta$
\end_inset

.
 (This is Theorem 6.2.6 from Casella and Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:factorization"

\end_inset


\end_layout

\begin_layout Proof
We will give the proof only for the discrete case.
\end_layout

\begin_layout Proof
Suppose 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic.
 Choose 
\begin_inset Formula $g\left(t|\theta\right)=P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=t\right\} \right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)=P\left(\left\{ \mathbf{X}=\mathbf{x}|T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)$
\end_inset

.
 Because 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient, the conditional probability defining 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 Thus, this choice of 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 is legitimate, and for this choice we have
\begin_inset Formula 
\begin{flalign}
f\left(\mathbf{x}|\theta\right) & =P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)\\
 & =P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \cap\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\cdot P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right).
\end{flalign}

\end_inset

(1) follows from the definition of a pmf.
 (2) follows because 
\begin_inset Formula $\left\{ \mathbf{X}=\mathbf{x}\right\} $
\end_inset

 is a subset of 
\begin_inset Formula $\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

, and if we have two sets 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 such that 
\begin_inset Formula $A\subset B$
\end_inset

, then 
\begin_inset Formula $A\cap B=A$
\end_inset

, so that 
\begin_inset Formula $P\left(A\cap B\right)=P\left(A\right)$
\end_inset

.
 (3) follows from the definition of conditional probability.
 (4) follows from our definitions of 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

.
 So factorization has been exhibited.
 We also see from the last two lines above that 
\begin_inset Formula $P_{\theta}\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)=g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

, so 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

 is the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 
\end_layout

\begin_layout Proof
Now assume the factorization 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

 exists.
 Let 
\begin_inset Formula $q\left(t|\theta\right)$
\end_inset

 be the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 To now show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is sufficient, we examine the ratio 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)/q\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

.
 Define 
\begin_inset Formula $A_{T\left(\mathbf{x}\right)}=\left\{ \mathbf{y}:T\left(\mathbf{y}\right)=T\left(\mathbf{x}\right)\right\} $
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign}
\frac{f\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}\right)|\theta\right)} & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{q\left(T\left(\mathbf{x}\right)|\theta\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{y}\right)|\theta\right)h\left(\mathbf{y}\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{y}\right)}\\
 & =\frac{g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{g\left(T\left(\mathbf{x}\right)|\theta\right)\sum_{A_{T\left(\mathbf{x}\right)}}h\left(\mathbf{y}\right)}\\
 & =\frac{h\left(\mathbf{x}\right)}{\sum_{A_{T\left(\mathbf{x}\right)}}h\left(\mathbf{y}\right)}.
\end{flalign}

\end_inset

(1) follows from our assumption that the factorization exists.
 (2) follows from the definition of the pmf of 
\begin_inset Formula $T$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{T\left(\mathbf{X}\right)}\left(T\left(\mathbf{x}\right)\right) & =P\left(\left\{ T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)\right\} \right)\\
 & =\sum_{\mathbf{y}\in T^{-1}\left(\left\{ T\left(\mathbf{x}\right)\right\} \right)}P\left(\left\{ \mathbf{X}=\mathbf{y}\right\} \right)\\
 & =\sum_{\mathbf{y}\in T^{-1}\left(\left\{ T\left(\mathbf{x}\right)\right\} \right)}f_{\mathbf{X}}\left(\mathbf{y}\right)\\
 & =\sum_{\mathbf{y}\in\left\{ \mathbf{y}:T\left(\mathbf{y}\right)=T\left(\mathbf{x}\right)\right\} }f_{\mathbf{X}}\left(\mathbf{y}\right)\\
 & =\sum_{\mathbf{y}\in A_{T\left(\mathbf{x}\right)}}g\left(T\left(\mathbf{y}\right)|\theta\right)h\left(\mathbf{y}\right).
\end{flalign*}

\end_inset

(3) and (4) follow from the fact that 
\begin_inset Formula $T$
\end_inset

 is constant on 
\begin_inset Formula $A_{T\left(\mathbf{x}\right)}$
\end_inset

, i.e., 
\series bold

\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset


\series default
.
 (5) follows from algebra.
 Because (5) does not depend on 
\begin_inset Formula $\theta$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a Poisson random variable]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-poisson"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x|\lambda\right) & =\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, so that the joint pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\prod_{i=1}^{n}p_{X_{i}}\left(x_{i}\right)\\
 & =\prod_{i=1}^{n}\left(\frac{\mathrm{e}^{-\lambda}\lambda^{x_{i}}}{x_{i}!}\right)\\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\left(\prod_{i=1}^{n}\mathrm{e}^{-\lambda}\lambda^{x_{i}}\right)\\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-n\lambda}\lambda^{x_{1}+\cdots+x_{n}}\\
 & =\underbrace{\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)}_{h\left(\mathbf{x}\right)}\underbrace{\mathrm{e}^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}}_{g\left(T\left(\mathbf{x}\right)|\lambda\right)}.
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Sufficient statistic for a Bernoulli random variable]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-bernoulli"

\end_inset

Consider a sequence of independent Bernoulli random variables 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 where 
\begin_inset Formula $P\left(\left\{ X_{i}=x\right\} \right)=\theta^{x}\left(1-\theta\right)^{1-x}$
\end_inset

 where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

.
 Then, the joint pmf of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\left(X_{1},\ldots,X_{n}|\theta\right)}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}\left(\theta^{x_{i}}\left(1-\theta\right)^{1-x_{i}}\right)\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\prod_{i=1}^{n}\left[\left(1-\theta\right)^{1}\left(1-\theta\right)^{-x_{i}}\right]\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n}\left(1-\theta\right)^{-\sum_{i=1}^{n}x_{i}}\\
 & =\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n}\left(\frac{1}{1-\theta}\right)^{\sum_{i=1}^{n}x_{i}}\\
 & =\underbrace{\left(1-\theta\right)^{n}\left(\frac{\theta}{1-\theta}\right)^{\sum_{i=1}^{n}x_{i}}}_{g\left(T\left(\mathbf{x}\right)|\theta\right)}.
\end{flalign*}

\end_inset

We set 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 Then, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal sufficient statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\mu,\sigma^{2}\right) & =\frac{1}{\sqrt{2\pi\sigma^{2}}}\mathrm{e}^{-\left(x-\mu\right)^{2}/\left(2\sigma^{2}\right)}
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, where 
\begin_inset Formula $\mu\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\sigma>0$
\end_inset

.
 Find a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:suff-stat-normal"

\end_inset


\end_layout

\begin_layout Example
A joint pdf for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\mu,\sigma^{2}\right) & =\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \right]\\
 & =\prod_{i=1}^{n}\left[\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \right]\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ \sum_{i=1}^{n}-\frac{1}{2}\left(\frac{\left(x_{i}-\mu\right)^{2}}{\sigma^{2}}\right)\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(\frac{x_{i}^{2}}{\sigma^{2}}-\frac{2\mu x_{i}}{\sigma^{2}}+\frac{\mu^{2}}{\sigma^{2}}\right)\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2}\left(\frac{\sum_{i=1}^{n}x_{i}^{2}}{\sigma^{2}}-\frac{2\mu\sum_{i=1}^{n}x_{i}}{\sigma^{2}}+\frac{n\mu^{2}}{\sigma^{2}}\right)\right\} \\
 & =\underbrace{\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{n\mu^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{\sum_{i=1}^{n}x_{i}^{2}}{2\sigma^{2}}+\frac{\mu\sum_{i=1}^{n}x_{i}}{\sigma^{2}}\right\} }_{g\left(T\left(\mathbf{x}\right)|\theta\right)}.
\end{flalign*}

\end_inset

We set 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 Then, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}X_{i}^{2},\sum_{i=1}^{n}X_{i}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform sufficient statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{U}\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\frac{1}{\theta}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left(0,\theta\right)$
\end_inset

 and 
\begin_inset Formula $f\left(x|\theta\right)=0$
\end_inset

 otherwise, so that a joint pdf for 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}\left(\frac{1}{\theta}I_{\left\{ 0<x_{i}<\theta\right\} }\right)=\left(\frac{1}{\theta}\right)^{n}I_{\left\{ x_{\left(1\right)}>0\right\} }I_{\left\{ x_{\left(n\right)}<\theta\right\} }.
\]

\end_inset

We have 
\begin_inset Formula $h\left(\mathbf{x}\right)=I_{\left\{ x_{\left(1\right)}>0\right\} }$
\end_inset

 and 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)=\theta^{-n}I_{\left\{ x_{\left(n\right)}<\theta\right\} }$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{x}\right)=x_{\left(n\right)}=\underset{1\leq i\leq n}{\max}x_{i}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:suff-statistic-exp-family"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a pdf or pmf 
\begin_inset Formula $f\left(x|\boldsymbol{\theta}\right)$
\end_inset

 that belongs to an exponential family given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\boldsymbol{\theta}\right) & =h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\boldsymbol{\theta}\right)t_{i}\left(x\right)\right)
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\ldots,\theta_{d}\right)$
\end_inset

, 
\begin_inset Formula $d\leq k$
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign*}
T\left(\mathbf{X}\right) & =\left(\sum_{j=1}^{n}t_{1}\left(X_{j}\right),\ldots,\sum_{j=1}^{n}t_{k}\left(X_{j}\right)\right)
\end{flalign*}

\end_inset

is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called the 
\series bold
natural sufficient statistic.

\series default
 (This is Theorem 6.2.10 from Casella & Berger.)
\end_layout

\begin_layout Proof
The joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{j=1}^{n}\left[h\left(x_{j}\right)c\left(\theta\right)\exp\left(\sum_{i=1}^{k}\omega_{i}\left(\theta\right)t_{i}\left(x_{j}\right)\right)\right]\\
 & =\underbrace{c\left(\theta\right)^{n}\exp\left\{ \sum_{i=1}^{k}\left(\omega_{i}\left(\theta\right)\sum_{j=1}^{n}t_{i}\left(x_{j}\right)\right)\right\} }_{g\left(T\left(\mathbf{x}\right)|\theta\right)}\underbrace{\prod_{j=1}^{n}h\left(x_{j}\right)}_{h\left(\mathbf{x}\right)}.
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{j=1}^{n}t_{1}\left(X_{j}\right),\ldots,\sum_{j=1}^{n}t_{k}\left(X_{j}\right)\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Subsection
Minimal sufficient statistics
\end_layout

\begin_layout Standard
In the preceding section, we found one sufficient statistic for each model
 considered.
 In fact, there are many sufficient statistics.
 It is always true that the data 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a sufficient statistic; we can factor the pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 as 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=f\left(T\left(\mathbf{X}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

, where 
\begin_inset Formula $T\left(\mathbf{x}\right)=\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

 for all 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Any one-to-one function of a sufficient statistic is sufficient.
 We might ask whether one sufficient statistic is any better than another.
\end_layout

\begin_layout Definition
A sufficient statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called a 
\shape italic
minimal sufficient statistic
\shape default
 if, for any other sufficient statistic 
\begin_inset Formula $T'\left(\mathbf{X}\right)$
\end_inset

, 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is a function of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

, i.e., 
\begin_inset Formula $T\left(\mathbf{x}\right)=g\left(T'\left(\mathbf{x}\right)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Of all sufficient statistics, a minimal sufficient statistic provides the
 greatest reduction of the data.
 In terms of the partition sets described above, if 
\begin_inset Formula $\left\{ B_{t'}:t'\in\mathcal{T}'\right\} $
\end_inset

 are the partition sets of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $\left\{ A_{t}:t\in\mathcal{T}\right\} $
\end_inset

 are the partition sets for 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

, then every 
\begin_inset Formula $B_{t'}$
\end_inset

 is a subset of 
\begin_inset Formula $A_{t}$
\end_inset

.
 The partition associated with the minimal sufficient statistic is the coarest
 possible partition (among those induced by sufficient statistics).
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 be the pmf or pdf of a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 Suppose there exists a function 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 such that, for every two sample points 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

, the ratio 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)/f\left(\mathbf{y}|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

 if and only if 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 Then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 (This is Theorem 6.2.13 from Casella & Berger; the following proof is given
 there.)
\begin_inset CommandInset label
LatexCommand label
name "thm:minimal-sufficient"

\end_inset


\end_layout

\begin_layout Proof
To simplify the proof, we assume 
\begin_inset Formula $f\left(x|\theta\right)>0$
\end_inset

 for all 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Proof
First, we show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic.
 Let 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=T\left(\mathbf{x}\right),\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

.
 Define the partition sets induced by 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 as 
\begin_inset Formula $A_{t}=\left\{ \mathbf{x}:T\left(\mathbf{x}\right)=t\right\} $
\end_inset

.
 For each 
\begin_inset Formula $A_{t}$
\end_inset

, choose and fix one element 
\begin_inset Formula $\mathbf{x}_{t}\in A_{t}$
\end_inset

.
 For any 
\begin_inset Formula $\mathbf{x}\in\mathcal{X}$
\end_inset

, 
\begin_inset Formula $\mathbf{x}_{T\left(\mathbf{x}\right)}$
\end_inset

 is the fixed element that is in the same set, 
\begin_inset Formula $A_{t}$
\end_inset

, as 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Since 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{T\left(\mathbf{x}\right)}$
\end_inset

 are in the same set 
\begin_inset Formula $A_{t}$
\end_inset

, 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{x}_{T\left(\mathbf{x}\right)}\right)$
\end_inset

 and, hence, 
\begin_inset Formula $f\left(x|\theta\right)/f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

.
 Thus, we can define a function on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 by 
\begin_inset Formula $h\left(\mathbf{x}\right)=f\left(\mathbf{x}|\theta\right)/f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)$
\end_inset

 and 
\begin_inset Formula $h$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 Define a function on 
\begin_inset Formula $\mathcal{T}$
\end_inset

 by 
\begin_inset Formula $g\left(t|\theta\right)=f\left(\mathbf{x}_{t}|\theta\right)$
\end_inset

.
 Then it can be seen that
\begin_inset Formula 
\[
f\left(\mathbf{x}|\theta\right)=\frac{f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)f\left(\mathbf{x}|\theta\right)}{f\left(\mathbf{x}_{T\left(\mathbf{x}\right)}|\theta\right)}=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)
\]

\end_inset

and, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Proof
Now to show that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is minimal, let 
\begin_inset Formula $T'\left(\mathbf{X}\right)$
\end_inset

 be any other sufficient statistic.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, there exist functions 
\begin_inset Formula $g'$
\end_inset

 and 
\begin_inset Formula $h'$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g'\left(T'\left(\mathbf{x}\right)|\theta\right)h'\left(\mathbf{x}\right)$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be any two sample points with 
\begin_inset Formula $T'\left(\mathbf{x}\right)=T'\left(\mathbf{y}\right)$
\end_inset

.
 Then
\begin_inset Formula 
\[
\frac{f\left(\mathbf{x}|\theta\right)}{f\left(\mathbf{y}|\theta\right)}=\frac{g'\left(T'\left(\mathbf{x}\right)|\theta\right)h'\left(\mathbf{x}\right)}{g'\left(T'\left(\mathbf{y}\right)|\theta\right)h'\left(\mathbf{y}\right)}=\frac{h'\left(\mathbf{x}\right)}{h'\left(\mathbf{y}\right)}.
\]

\end_inset

Since this ratio does not depend on 
\begin_inset Formula $\theta$
\end_inset

, the assumptions of the theorem imply that 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 Thus, 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is a function of 
\begin_inset Formula $T'\left(\mathbf{x}\right)$
\end_inset

 and 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is minimal.
\end_layout

\begin_layout Corollary
If the partition of the sample space induced by 
\begin_inset Formula $f\left(x|\theta\right)/f\left(y|\theta\right)$
\end_inset

 is equivalent to that induced by 
\begin_inset Formula $T$
\end_inset

, then 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient.
\begin_inset CommandInset label
LatexCommand label
name "cor:minimal-sufficient-partition"

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bernoulli minimal sufficient statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},X_{2}\sim\text{Bernoulli}\left(p\right)$
\end_inset

.
 Let 
\begin_inset Formula $V=X_{1}$
\end_inset

, 
\begin_inset Formula $T=\sum_{i}X_{i}$
\end_inset

, and 
\begin_inset Formula $U=\left(T,X_{1}\right)$
\end_inset

.
 Determine whether 
\begin_inset Formula $V$
\end_inset

, 
\begin_inset Formula $T$
\end_inset

, or 
\begin_inset Formula $U$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 
\end_layout

\begin_layout Example
The set of outcomes and the statistics are shown below.
\end_layout

\begin_layout Example
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $X_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $X_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $V$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $T$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $U$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(0,0\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,0\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,1\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(2,1\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{V}=\left\{ v:v=x_{1},\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

, the sample space of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, under 
\begin_inset Formula $V$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{V}=\left\{ 0,1\right\} $
\end_inset

.
 Let 
\begin_inset Formula $W_{t}=\left\{ \mathbf{x}:X_{1}=v\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $V$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
W_{0} & =\left\{ \left(0,0\right),\left(0,1\right)\right\} \\
W_{1} & =\left\{ \left(1,0\right),\left(1,1\right)\right\} .
\end{flalign*}

\end_inset

The joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},X_{2}$
\end_inset

 conditioned on 
\begin_inset Formula $V$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\left(\mathbf{X}|V\right)}\left(\mathbf{x}|v\right) & =P\left(\left\{ \mathbf{X}=\mathbf{x}\right\} |\left\{ V=v\right\} \right)\\
 & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{V}\left(v|p\right)}\\
 & =\frac{\left(p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right)\left(p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right)}{p^{x_{1}}\left(1-p\right)^{1-x_{1}}}\\
 & =p^{x_{2}}\left(1-p\right)^{1-x_{2}}.
\end{flalign*}

\end_inset

Clearly, this expression depends on 
\begin_inset Formula $p$
\end_inset

, so 
\begin_inset Formula $V$
\end_inset

 is not a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{T}=\left\{ t:t=\sum_{i}x_{i},\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $T$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{T}=\left\{ 0,1,2\right\} $
\end_inset

.
 Let 
\begin_inset Formula $A_{t}=\left\{ \mathbf{x}:\sum_{i}x_{i}=t\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $T$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
A_{0} & =\left\{ \left(0,0\right)\right\} \\
A_{1} & =\left\{ \left(0,1\right),\left(1,0\right)\right\} \\
A_{2} & =\left\{ \left(1,1\right)\right\} .
\end{flalign*}

\end_inset

We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

 that 
\begin_inset Formula $T$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 We will now show that 
\begin_inset Formula $T$
\end_inset

 is minimal.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 can be written as 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)=g\left(T\left(\mathbf{x}\right)|p\right)h\left(\mathbf{x}\right)$
\end_inset

, where 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

 and 
\begin_inset Formula 
\[
g\left(T\left(\mathbf{x}\right)|p\right)=\left(1-p\right)^{n}\left(\frac{p}{1-p}\right)^{\sum_{i=1}^{n}x_{i}}=\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}.
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{X}}\left(\mathbf{y}|p\right)} & =\frac{g\left(T\left(\mathbf{x}\right)|p\right)h\left(\mathbf{x}\right)}{g\left(T\left(\mathbf{y}\right)|p\right)h\left(\mathbf{y}\right)}\\
 & =\frac{\left[\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}\right]\cdot1}{\left[\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}\right]\cdot1}\\
 & =\frac{\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}{\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $p/\left(1-p\right)>0$
\end_inset

, so that the ratio shown above will be defined.
 This ratio will be constant as a function of 
\begin_inset Formula $p$
\end_inset

 if and only if 
\begin_inset Formula $x_{1}+x_{2}=y_{1}+y_{2}$
\end_inset

, i.e., if 
\begin_inset Formula $T\left(\mathbf{x}\right)=T\left(\mathbf{y}\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $\mathcal{U}=\left\{ \mathbf{u}:\mathbf{u}=\left(T\left(\mathbf{x}\right),x_{1}\right),\mathbf{x}\in\mathcal{X}\right\} $
\end_inset

 be the image of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 under 
\begin_inset Formula $\mathbf{U}$
\end_inset

.
 From the table above, we have 
\begin_inset Formula $\mathcal{U}=\left\{ \left(0,0\right),\left(1,0\right),\left(1,1\right),\left(2,1\right)\right\} $
\end_inset

.
 Let 
\begin_inset Formula $B_{t}=\left\{ \mathbf{x}:\left(T\left(\mathbf{x}\right),x_{1}\right)=\mathbf{u}\right\} $
\end_inset

 be the partition sets induced by 
\begin_inset Formula $\mathbf{U}$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
B_{\left(0,0\right)} & =\left\{ \left(0,0\right)\right\} \\
B_{\left(1,0\right)} & =\left\{ \left(1,0\right)\right\} \\
B_{\left(1,1\right)} & =\left\{ \left(0,1\right)\right\} \\
B_{\left(2,1\right)} & =\left\{ \left(1,1\right)\right\} .
\end{flalign*}

\end_inset

The pmf of 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{U}}\left(\mathbf{u}|p\right) & =P\left(\left\{ T\left(\mathbf{X}\right)=t\right\} \cap\left\{ X_{1}=x_{1}\right\} \right)\\
 & =P\left(\left\{ T\left(\mathbf{X}\right)=t\right\} |\left\{ X_{1}=x_{1}\right\} \right)\cdot P\left(\left\{ X_{1}=x_{1}\right\} \right)\\
 & =\left(\frac{p_{T\left(\mathbf{x}\right)}\left(t|p\right)}{p_{X_{1}}\left(x_{1}|p\right)}\right)\cdot p_{X_{1}}\left(x_{1}|p\right)\\
 & =p_{T\left(\mathbf{x}\right)}\left(t|p\right).
\end{flalign*}

\end_inset

To show that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

, we examine the ratio
\begin_inset Formula 
\begin{flalign*}
\frac{p\left(\mathbf{x}|\theta\right)}{q\left(T\left(\mathbf{x}|\theta\right)\right)} & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{U}}\left(\mathbf{u}|p\right)}\\
 & =\frac{\left(p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right)\left(p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right)}{\left(1-p\right)^{2}\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}\\
 & =\frac{p^{x_{1}+x_{2}}\left(1-p\right)^{2-x_{1}-x_{2}}}{\left(1-p\right)^{2}p^{x_{1}+x_{2}}\left(1-p\right)^{-x_{1}-x_{2}}}\\
 & =\frac{p^{x_{1}+x_{x}}\left(1-p\right)^{2-x_{1}-x_{2}}}{p^{x_{1}+x_{2}}\left(1-p\right)^{2-x_{1}-x_{x}}}\\
 & =1.
\end{flalign*}

\end_inset

Because the ratio 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)/p_{\mathbf{U}}\left(\mathbf{u}|p\right)$
\end_inset

 is constant as a function of 
\begin_inset Formula $p$
\end_inset

, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

 that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 We will now check whether 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is minimal.
 We showed above that 
\begin_inset Formula 
\begin{flalign*}
\frac{p_{\mathbf{X}}\left(\mathbf{x}|p\right)}{p_{\mathbf{X}}\left(\mathbf{y}|p\right)} & =\frac{\left(\frac{p}{1-p}\right)^{x_{1}+x_{2}}}{\left(\frac{p}{1-p}\right)^{y_{1}+y_{2}}}.
\end{flalign*}

\end_inset

By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

, if this ratio is constant if and only if 
\begin_inset Formula $\mathbf{U}\left(\mathbf{x}\right)=\mathbf{U}\left(\mathbf{y}\right)$
\end_inset

, then 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is minimal.
 We have 
\begin_inset Formula 
\[
\mathbf{U}\left(\mathbf{x}\right)=\left(T\left(\mathbf{x}\right),x_{1}\right)=\left(x_{1}+x_{2},x_{1}\right)
\]

\end_inset

and
\begin_inset Formula 
\[
\mathbf{U}\left(\mathbf{y}\right)=\left(T\left(\mathbf{y}\right),y_{1}\right)=\left(y_{1}+y_{2},y_{1}\right).
\]

\end_inset

As shown above, the ratio 
\begin_inset Formula $p_{\mathbf{X}}\left(\mathbf{x}|p\right)/p_{\mathbf{X}}\left(\mathbf{y}|p\right)$
\end_inset

 will be constant if and only if 
\begin_inset Formula $x_{1}+y_{1}=x_{2}+y_{2}$
\end_inset

, which does not imply 
\begin_inset Formula $x_{1}=y_{1}$
\end_inset

.
 Thus, it is not the case that the ratio will be constant if and only if
 
\begin_inset Formula $\mathbf{U}\left(\mathbf{x}\right)=\mathbf{U}\left(\mathbf{y}\right)$
\end_inset

, so it follows that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 Another way to see that 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not minimal is to observe that 
\begin_inset Formula 
\begin{flalign*}
B_{\left(0,0\right)} & =\left\{ \left(0,0\right)\right\} =A_{0}\\
B_{\left(1,0\right)} & =\left\{ \left(1,0\right)\right\} \subset A_{1}\\
B_{\left(1,1\right)} & =\left\{ \left(0,1\right)\right\} \subset A_{1}\\
B_{\left(2,1\right)} & =\left\{ \left(1,1\right)\right\} =A_{2}.
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula $B_{t}\subseteq A_{t}$
\end_inset

, i.e., the partition of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 induced by 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not equivalent to that induced by 
\begin_inset Formula $T$
\end_inset

, which is minimal sufficient, so by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:minimal-sufficient-partition"

\end_inset

, 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is not a minimal sufficient statistic for 
\begin_inset Formula $p$
\end_inset

.
 Conversely, note that the statistic 
\begin_inset Formula $W=17T$
\end_inset

 generates the same partition as 
\begin_inset Formula $T$
\end_inset

, so 
\begin_inset Formula $W$
\end_inset

 is also minimal sufficient.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Exponential minimal sufficient statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mbox{Exp}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f\left(x|\lambda\right) & =\lambda\mathrm{e}^{-\lambda x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x>0$
\end_inset

 and 
\begin_inset Formula $f\left(x|\lambda\right)=0$
\end_inset

 otherwise, so that a joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)=\prod_{i=1}^{n}\lambda\mathrm{e}^{-\lambda x}I_{\left\{ x_{i}>0\right\} }=\lambda^{n}\mathrm{e}^{-\sum_{i=1}^{n}\lambda x_{i}}I_{\left\{ x_{\left(1\right)}>0\right\} }.
\]

\end_inset

Find a minimal sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:minimal-sufficient-exp-rv"

\end_inset


\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\begin{flalign*}
\frac{f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)}{f_{\mathbf{X}}\left(\mathbf{y}|\lambda\right)} & =\frac{\lambda^{n}\mathrm{e}^{-\lambda\sum_{i=1}^{n}x_{i}}I_{\left\{ x_{\left(1\right)}>0\right\} }}{\lambda^{n}\mathrm{e}^{-\lambda\sum_{i=1}^{n}y_{i}}I_{\left\{ y_{\left(1\right)}>0\right\} }}\\
 & =\mathrm{e}^{-\lambda\sum_{i=1}^{n}x_{i}}\mathrm{e}^{\lambda\sum_{i=1}^{n}y_{i}}\left(\frac{I_{\left\{ x_{\left(1\right)}>0\right\} }}{I_{\left\{ y_{\left(1\right)}>0\right\} }}\right)\\
 & =\mathrm{e}^{-\lambda\left(\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}y_{i}\right)}\left(\frac{I_{\left\{ x_{\left(1\right)}>0\right\} }}{I_{\left\{ y_{\left(1\right)}>0\right\} }}\right).
\end{flalign*}

\end_inset

This ratio will be constant as a function of 
\begin_inset Formula $\lambda$
\end_inset

 if and only if 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}y_{i}=0$
\end_inset

, i.e., if 
\begin_inset Formula $\sum_{i=1}^{n}x_{i}=\sum_{i=1}^{n}y_{i}$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Minimal sufficient statistics for an exponential family]
\end_layout

\end_inset

The set of sufficient statistics 
\begin_inset Formula $T\left(X\right)=\left\{ T_{1}\left(X\right),\ldots,T_{n}\left(X\right)\right\} $
\end_inset

 from an exponential family are minimal sufficient.
 Recall that the pmf or pdf of an exponential family can be written as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} .
\end{flalign*}

\end_inset

We have
\begin_inset Formula 
\begin{flalign*}
\frac{f\left(x|\theta\right)}{f\left(y|\theta\right)} & =\frac{h^{*}\left(x\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} }{h^{*}\left(y\right)c^{*}\left(\theta\right)\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right\} }\\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right\} \exp\left\{ -\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \left[\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(x\right)\right]-\left[\sum_{j=1}^{k}\omega_{j}\left(\theta\right)T_{j}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \left[\omega_{1}\left(\theta\right)T_{1}\left(x\right)+\cdots+\omega_{k}T_{k}\left(x\right)\right]-\left[\omega_{1}\left(\theta\right)T_{1}\left(y\right)+\cdots+\omega_{k}\left(\theta\right)T_{k}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \omega_{1}\left(\theta\right)\left[T_{1}\left(x\right)-T_{1}\left(y\right)\right]+\cdots+\omega_{k}\left[T_{k}\left(x\right)-T_{k}\left(y\right)\right]\right\} \\
 & =\frac{h^{*}\left(x\right)}{h^{*}\left(y\right)}\exp\left\{ \sum_{j=1}^{k}\omega_{j}\left(\theta\right)\left[T_{j}\left(x\right)-T_{j}\left(y\right)\right]\right\} .
\end{flalign*}

\end_inset

This ratio will be constant as a function of 
\begin_inset Formula $\theta$
\end_inset

 if and only if 
\begin_inset Formula $T\left(X\right)=T\left(Y\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:minimal-sufficient"

\end_inset

 that 
\begin_inset Formula $T\left(X\right)=\left\{ T_{1}\left(X\right),\ldots,T_{n}\left(X\right)\right\} $
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "exa:min-sufficient-exp-family"

\end_inset


\end_layout

\begin_layout Subsection
Ancillary statistics
\end_layout

\begin_layout Definition
A statistic 
\begin_inset Formula $S\left(\mathbf{X}\right)$
\end_inset

 whose distribution does not depend on the parameter 
\begin_inset Formula $\theta$
\end_inset

 is called an 
\shape italic
ancillary statistic
\shape default
.
\end_layout

\begin_layout Standard
Alone, an ancillary statistic contains no information about 
\begin_inset Formula $\theta$
\end_inset

.
 When used in conjunction with other statistics, it sometimes contains valuable
 information for inference about 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Bivariate transformations
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 be a random vector with a known probability distribution.
 Now consider a new bivariate random vector 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 defined by 
\begin_inset Formula $U=g_{1}\left(X,Y\right)$
\end_inset

 and 
\begin_inset Formula $V=g_{2}\left(X,Y\right)$
\end_inset

, where 
\begin_inset Formula $g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $g_{2}\left(x,y\right)$
\end_inset

 are some specified functions.
 If 
\begin_inset Formula $B$
\end_inset

 is any subset of 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset

, then 
\begin_inset Formula $\left(U,V\right)\in B$
\end_inset

 if and only if 
\begin_inset Formula $\left(X,Y\right)\in A$
\end_inset

, where 
\begin_inset Formula $A=\left\{ \left(x,y\right):\left(g_{1}\left(x,y\right),g_{2}\left(x,y\right)\right)\in B\right\} $
\end_inset

.
 Thus 
\begin_inset Formula $P\left(\left\{ \left(U,V\right)\in B\right\} \right)=P\left(\left\{ \left(X,Y\right)\in A\right\} \right)$
\end_inset

, and the probability distribution of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 is completely determined by the probability distribution of 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\left(X,Y\right)$
\end_inset

 is a continuous random vector with joint pdf 
\begin_inset Formula $f_{X,Y}\left(x,y\right)$
\end_inset

, then the joint pdf of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 can be expressed in terms of 
\begin_inset Formula $f_{X,Y}\left(x,y\right)$
\end_inset

.
 Let 
\begin_inset Formula 
\begin{flalign*}
\mathcal{A} & =\left\{ \left(x,y\right):f_{X,Y}\left(x,y\right)>0\right\} 
\end{flalign*}

\end_inset

 and 
\begin_inset Formula 
\begin{flalign*}
\mathcal{B} & =\left\{ \left(u,v\right):u=g_{1}\left(x,y\right),v=g_{2}\left(x,y\right),\left(x,y\right)\in\mathcal{A}\right\} .
\end{flalign*}

\end_inset

The joint pdf 
\begin_inset Formula $f_{U,V}$
\end_inset


\begin_inset Formula $\left(u,v\right)$
\end_inset

 will be positive on the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 For the simplest version of this result we assume that the transformation
 
\begin_inset Formula $u=g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $v=g_{2}\left(x,y\right)$
\end_inset

 defines a one-to-one transformation of 
\begin_inset Formula $\mathcal{A}$
\end_inset

 onto 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 We are assuming that for each 
\begin_inset Formula $\left(u,v\right)\in\mathcal{B}$
\end_inset

, there is only one 
\begin_inset Formula $\left(x,y\right)\in\mathcal{A}$
\end_inset

 such that 
\begin_inset Formula $\left(u,v\right)=\left(g_{1}\left(x,y\right),g_{2}\left(x,y\right)\right)$
\end_inset

.
 For such a one-to-one, onto transformation, we can solve the equations
 
\begin_inset Formula $u=g_{1}\left(x,y\right)$
\end_inset

 and 
\begin_inset Formula $v=g_{2}\left(x,y\right)$
\end_inset

 for 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 in terms of 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

.
 We will denote this inverse transformation by 
\begin_inset Formula $x=h_{1}\left(u,v\right)$
\end_inset

 and 
\begin_inset Formula $y=h_{2}\left(u,v\right)$
\end_inset

.
 The role played by the derivative in the univariate case is now played
 by quantity called the Jacobian of the transformation.
 This function of 
\begin_inset Formula $\left(u,v\right)$
\end_inset

, denoted by 
\begin_inset Formula $J$
\end_inset

, is the determinant of a matrix of partial derivatives, and is defined
 by
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{2.5}
\end_layout

\end_inset


\begin_inset Formula 
\begin{flalign*}
J= & \left|\begin{array}{cc}
\dfrac{\partial x}{\partial u} & \dfrac{\partial x}{\partial v}\\
\dfrac{\partial y}{\partial u} & \dfrac{\partial y}{\partial v}
\end{array}\right|=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial y}{\partial u}\frac{\partial x}{\partial v},
\end{flalign*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

where 
\begin_inset Formula 
\[
\frac{\partial x}{\partial u}=\frac{\partial h_{1}\left(u,v\right)}{\partial u},\quad\frac{\partial x}{\partial v}=\frac{\partial h_{1}\left(u,v\right)}{\partial v},\quad\frac{\partial y}{\partial u}=\frac{\partial h_{2}\left(u,v\right)}{\partial u},\quad\mbox{and}\quad\frac{\partial y}{\partial v}=\frac{\partial h_{2}\left(u,v\right)}{\partial v}.
\]

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\left(U,V\right)$
\end_inset

 is 0 outside the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

 and on the set 
\begin_inset Formula $\mathcal{B}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{U,V}\left(u,v\right) & =f_{X,Y}\left(h_{1}\left(u,v\right),h_{2}\left(u,v\right)\right)\left|J\right|.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark*
This technique, which is easily generalized to more than two variables,
 can be used to find the pdf of some function of interest by first finding
 the joint pdf of that function and another, conveniently chosen function,
 then integrating the resulting joint pdf with respect to the second function,
 which gives the (marginal) pdf of the function of interest.
 This technique is demonstrated in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:ancillary-stat-uniform-range"

\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Distribution of the sum of Poisson variables]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:dist-of-sum-of-poisson-rv"

\end_inset

If 
\begin_inset Formula $X\sim\text{Poisson}\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $Y\sim\text{Poisson}\left(\lambda\right)$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent, then 
\begin_inset Formula $X+Y\sim\text{Poisson}\left(\theta+\lambda\right)$
\end_inset

.
 (This is Theorem 4.3.2 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform ancillary statistic]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:ancillary-stat-uniform-range"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid uniform observations on the interval 
\begin_inset Formula $\left(\theta,\theta+1\right)$
\end_inset

, 
\begin_inset Formula $-\infty<\theta<\infty$
\end_inset

.
 Let 
\begin_inset Formula $X_{\left(1\right)}<\cdots<X_{\left(n\right)}$
\end_inset

 be the order statistics from the sample.
 Show that the range statistic 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 is an ancillary statistic.
 (This is example 6.2.17 from Casella & Berger.)
\end_layout

\begin_layout Example
The pdf of each 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x|\theta\right) & =\frac{1}{\left(\theta+1\right)-\theta}\\
 & =1,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta<x<\theta+1$
\end_inset

 and 
\begin_inset Formula $f_{X_{i}}\left(x|\theta\right)=0$
\end_inset

 otherwise, so that the cdf of each 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{X_{i}}\left(x|\theta\right)=\begin{cases}
0, & x\leq\theta\\
\int_{\theta}^{x}1\mbox{d}x=x-\theta, & \theta<x<\theta+1\\
1, & x\geq\theta+1
\end{cases}.
\]

\end_inset

Let 
\begin_inset Formula $u=x_{\left(1\right)}$
\end_inset

 and let 
\begin_inset Formula $v=x_{\left(n\right)}$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-joint-pdf"

\end_inset

, the joint pdf of 
\begin_inset Formula $X_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right) & =\frac{n!}{\left(1-1\right)!\left(n-1-1\right)!\left(n-n\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(u\right)\right]^{1-1}\\
 & \quad\cdot\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{n-1-1}\left[1-F_{X}\left(v\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-2\right)!}f_{X}\left(u\right)f_{X}\left(v\right)\left[F_{X}\left(v\right)-F_{X}\left(u\right)\right]^{n-2}\\
 & =\frac{n!}{\left(n-2\right)!}\cdot1\cdot1\cdot\left[\left(v-\theta\right)-\left(u-\theta\right)\right]^{n-2}\\
 & =\frac{n!}{\left(n-2\right)!}\left(v-u\right)^{n-2}\\
 & =\frac{n\left(n-1\right)\left(n-2\right)!}{\left(n-2\right)!}\left(v-u\right)^{n-2}\\
 & =n\left(n-1\right)\left(v-u\right)^{n-2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta<u<v<\theta+1$
\end_inset

 and 
\begin_inset Formula $f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right)=0$
\end_inset

 otherwise.
 Let 
\begin_inset Formula 
\begin{flalign*}
\mathcal{A} & =\left\{ \left(u,v\right):f_{X_{\left(1\right)},X_{\left(n\right)}}\left(u,v\right)>0\right\} ,
\end{flalign*}

\end_inset

and make the transformation 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $M=\left(X_{\left(1\right)}+X_{\left(n\right)}\right)/2$
\end_inset

, so that we have 
\begin_inset Formula 
\begin{flalign*}
\mathcal{B} & =\left\{ \left(r,m\right):r=v-u,m=\left(u+v\right)/2,\left(u,v\right)\in\mathcal{A}\right\} .
\end{flalign*}

\end_inset

Then, 
\begin_inset Formula $f_{R,M}\left(r,m\right)$
\end_inset

 will be positive on 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 We have the inverse transformation 
\begin_inset Formula 
\begin{flalign*}
X_{\left(1\right)} & =2M-X_{\left(n\right)}\\
 & =2M-\left(R+X_{\left(1\right)}\right)\\
\Leftrightarrow2X_{\left(1\right)} & =2M-R\\
\Leftrightarrow X_{\left(1\right)} & =\left(2M-R\right)/2\\
\Leftrightarrow u & =\left(2m-r\right)/2
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
X_{\left(n\right)} & =2M-X_{\left(1\right)}\\
 & =2M-\left(X_{\left(n\right)}-R\right)\\
\Leftrightarrow2X_{\left(n\right)} & =2M+R\\
\Leftrightarrow X_{\left(n\right)} & =\left(2M+R\right)/2\\
\Leftrightarrow v & =\left(2m+r\right)/2.
\end{flalign*}

\end_inset

The Jacobian of the transformation is given by
\begin_inset Formula 
\begin{flalign*}
J & =\begin{vmatrix}\frac{\partial u}{\partial r} & \frac{\partial u}{\partial m}\\
\frac{\partial v}{\partial r} & \frac{\partial v}{\partial m}
\end{vmatrix}\\
 & =\begin{vmatrix}-\frac{1}{2} & 1\\
\frac{1}{2} & 1
\end{vmatrix}\\
 & =\left(-\frac{1}{2}\right)\left(1\right)-\left(\frac{1}{2}\right)\left(1\right)\\
 & =-1.
\end{flalign*}

\end_inset

We will now find the range for 
\begin_inset Formula $f_{R,M}$
\end_inset

.
 We have
\begin_inset Formula 
\begin{flalign*}
u>\theta & \Rightarrow m-\frac{r}{2}>\theta\\
 & \Rightarrow m>\theta+\frac{r}{2}\\
v>u & \Rightarrow m+\frac{r}{2}>m-\frac{r}{2}\\
 & \Rightarrow\frac{r}{2}>-\frac{r}{2}\\
 & \Rightarrow r>0\\
v<\theta+1 & \Rightarrow\frac{2m+r}{2}<\theta+1\\
 & \Rightarrow m<\theta+1-\frac{r}{2}
\end{flalign*}

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3.5, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,4,0.5,4))
\end_layout

\begin_layout Plain Layout

r <- seq(-1, 4, length=5000) 
\end_layout

\begin_layout Plain Layout

# set xaxs="i" to remove the default 4% padding beyond the ylim
\end_layout

\begin_layout Plain Layout

plot(r, 1 + r / 2, type = "l", xlab = "r", ylab = "m", yaxt = "n", xaxt
 = "n", xaxs = "i", xlim = c(0, 4), ylim = c(0,3), cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

lines(r, 2 - r / 2, type="l")
\end_layout

\begin_layout Plain Layout

polygon(c(-1,1,-1), c(2.5,1.5,0.5), density=20, angle=0)
\end_layout

\begin_layout Plain Layout

axis(1, at = c(0,1), labels = c(0,1), cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

axis(2, at = c(1,2), labels = c(expression(theta), expression(theta + 1)),
 cex.axis = 0.75, las = 2)
\end_layout

\begin_layout Plain Layout

# use las to rotate the strings and padj to move them along the axis
\end_layout

\begin_layout Plain Layout

mtext(expression(m == theta + frac(r, 2)), side=4, line=0.5, las=2, padj=-1.5,
 cex=0.75)
\end_layout

\begin_layout Plain Layout

mtext(expression(m == theta + 1 - frac(r, 2)), side=4, line=0.5, las=2, padj=2.5,
 cex=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
From the figure above, we see that the region where 
\begin_inset Formula $f_{R,M}$
\end_inset

 is positive is bounded by 
\begin_inset Formula $r>0$
\end_inset

, 
\begin_inset Formula $m>\theta+\left(r/2\right)$
\end_inset

, and 
\begin_inset Formula $m<\theta+1-\left(r/2\right)$
\end_inset

.
 The upper limit of integration for 
\begin_inset Formula $r$
\end_inset

 is given by the intersection of 
\begin_inset Formula $m=\theta+\left(r/2\right)$
\end_inset

 and 
\begin_inset Formula $m=\theta+1-\left(r/2\right)$
\end_inset

.
 Setting them equal, we have
\begin_inset Formula 
\[
\theta+\frac{r}{2}=\theta+1-\frac{r}{2}\implies r=1.
\]

\end_inset

Then, the joint pdf of 
\begin_inset Formula $R$
\end_inset

 and 
\begin_inset Formula $M$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{R,M}\left(r,m\right) & =f_{X_{\left(1\right)},X_{\left(n\right)}}\left(\frac{2m-r}{2},\frac{2m+r}{2}\right)\left|J\right|\\
 & =n\left(n-1\right)\left(\frac{2m+r}{2}-\frac{2m-r}{2}\right)^{n-2}\left|-1\right|\\
 & =n\left(n-1\right)\left(\frac{2m+r-2m+r}{2}\right)^{n-2}\\
 & =n\left(n-1\right)\left(\frac{2r}{2}\right)^{n-2}\\
 & =n\left(n-1\right)r^{n-2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $0<r<1$
\end_inset

 and 
\begin_inset Formula $\theta+\left(r/2\right)<m<\theta+1-\left(r/2\right)$
\end_inset

 and 
\begin_inset Formula $f_{R,M}\left(r,m\right)=0$
\end_inset

 otherwise.
 We will find the (marginal) pdf of 
\begin_inset Formula $R$
\end_inset

 by integrating the joint pdf with respect to 
\begin_inset Formula $m$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
f_{R}\left(r|\theta\right) & =\int_{\theta+\left(r/2\right)}^{\theta+1-\left(r/2\right)}n\left(n-1\right)r^{n-2}\mbox{d}m\\
 & =n\left(n-1\right)r^{n-2}\left[m\Big\rvert_{\theta+\left(r/2\right)}^{\theta+1-\left(r/2\right)}\right]\\
 & =n\left(n-1\right)r^{n-2}\left[\theta+1-\frac{r}{2}-\left(\theta+\frac{r}{2}\right)\right]\\
 & =n\left(n-1\right)r^{n-2}\left[1-\frac{r}{2}-\frac{r}{2}\right]\\
 & =n\left(n-1\right)r^{n-2}\left(1-r\right)
\end{flalign*}

\end_inset

where 
\begin_inset Formula $0<r<1$
\end_inset

.
 This expression is independent of 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $R$
\end_inset

 is ancillary.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ancillary statistic for location family]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a location parameter family with cdf 
\begin_inset Formula $F\left(x-\theta\right)$
\end_inset

, 
\begin_inset Formula $-\infty<\theta<\infty$
\end_inset

.
 We will show that the range, 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

, is an ancillary statistic.
 (This is example 6.2.18 from Casella & Berger.)
\end_layout

\begin_layout Example
We will use 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:location-scale-family"

\end_inset

 and work with 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 iid observations from 
\begin_inset Formula $F\left(x\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\theta=0$
\end_inset

) with 
\begin_inset Formula $X_{1}=Z_{1}+\theta,\ldots,X_{n}=Z_{n}+\theta$
\end_inset

.
 Thus, the cdf of the range statistic, 
\begin_inset Formula $R$
\end_inset

, is
\begin_inset Formula 
\begin{flalign*}
F_{R}\left(r|\theta\right) & =P_{\theta}\left(\left\{ R\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max_{i}X_{i}-\min_{i}X_{i}\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max\left(X_{1},\ldots,X_{n}\right)-\min\left(X_{1},\ldots,X_{n}\right)\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ \max\left(Z_{1}+\theta,\ldots,Z_{n}+\theta\right)-\min\left(Z_{1}+\theta,\ldots,Z_{n}+\theta\right)\leq r\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z_{\left(n\right)}+\theta-\left(Z_{\left(1\right)}+\theta\right)\right\} \leq r\right)\\
 & =P_{\theta}\left(\left\{ Z_{\left(n\right)}-Z_{\left(1\right)}\leq r\right\} \right).
\end{flalign*}

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that the cdf of 
\begin_inset Formula $R$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

 and therefore 
\begin_inset Formula $R$
\end_inset

 is an ancillary statistic.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Ancillary statistic for scale family]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from a scale parameter family with cdf 
\begin_inset Formula $F\left(x/\sigma\right)$
\end_inset

, 
\begin_inset Formula $\sigma>0$
\end_inset

.
 Then, any statistic that depends on the sample only through the 
\begin_inset Formula $n-1$
\end_inset

 values 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is an ancillary statistic.
 (This is example 6.2.19 from Casella & Berger.)
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 be iid observations from 
\begin_inset Formula $F\left(x\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\sigma=1$
\end_inset

) with 
\begin_inset Formula $X_{i}=\sigma Z_{i}$
\end_inset

.
 The joint cdf of 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
F\left(y_{1},\ldots,y_{n-1}|\sigma\right) & =P_{\sigma}\left(\left\{ \frac{X_{1}}{X_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{X_{n-1}}{X_{n}}\leq y_{n-1}\right\} \right)\\
 & =P_{\sigma}\left(\left\{ \frac{\sigma Z_{1}}{\sigma Z_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{\sigma Z_{n-1}}{\sigma Z_{n}}\leq y_{n-1}\right\} \right)\\
 & =P_{\sigma}\left(\left\{ \frac{Z_{1}}{Z_{n}}\leq y_{1}\right\} \cap\cdots\cap\left\{ \frac{Z_{n-1}}{Z_{n}}\leq y_{n-1}\right\} \right).
\end{flalign*}

\end_inset

The last probability does not depend on 
\begin_inset Formula $\sigma$
\end_inset

 because the distribution of 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 does not depend on 
\begin_inset Formula $\sigma$
\end_inset

.
 So the distribution of 
\begin_inset Formula $X_{1}/X_{n},\ldots,X_{n-1}/X_{n}$
\end_inset

 is independent of 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Mixture of normal distributions]
\end_layout

\end_inset

Sometimes, an ancillary statistic is viewed in conjunction with another
 statistic, and together they form a minimal sufficient statistic, i.e., 
\begin_inset Formula $S=\left(T,C\right)$
\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is ancillary for 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient conditional on 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $Y$
\end_inset

 is a mixture of normal distributions 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma_{0}^{2}\right)$
\end_inset

 and 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma_{1}^{2}\right)$
\end_inset

 with 
\begin_inset Formula $\sigma_{0}^{2}$
\end_inset

 and 
\begin_inset Formula $\sigma_{1}^{2}$
\end_inset

 known (for example, any two of the three distributions in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-of-scale-normal}
\end_layout

\end_inset

).
 Let 
\begin_inset Formula $C$
\end_inset

 be defined as
\begin_inset Formula 
\[
C=\begin{cases}
0, & \mbox{if }Y\sim\mathcal{N}\left(\mu,\sigma_{0}^{2}\right)\\
1, & \mbox{if }Y\sim\mathcal{N}\left(\mu,\sigma_{1}^{2}\right)
\end{cases}.
\]

\end_inset


\begin_inset Formula $Y$
\end_inset

 is equally likely to be 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma_{0}^{2}\right)$
\end_inset

 or 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma_{1}^{2}\right)$
\end_inset

, so 
\begin_inset Formula $P\left(\left\{ Y\sim\mathcal{N}\left(\mu,\sigma_{0}^{2}\right)\right\} \right)=P\left(\left\{ Y\sim\mathcal{N}\left(\mu,\sigma_{1}^{2}\right)\right\} \right)=1/2$
\end_inset

.
 Then, the joint pdf of 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $C$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{C,Y}\left(c,y\right) & =P\left(\left\{ C=c\right\} \cap\left\{ Y=y\right\} \right)\\
 & =P\left(\left\{ Y=y\right\} |\left\{ C=c\right\} \right)\cdot P\left(\left\{ C=c\right\} \right)\\
 & =\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}e^{-\left(y-\mu\right)^{2}/\left(2\sigma_{c}^{2}\right)}\\
 & =\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}\exp\left\{ -\frac{y^{2}}{2\sigma_{c}^{2}}-\frac{\mu^{2}}{2\sigma_{c}^{2}}+\frac{\mu y}{\sigma_{c}^{2}}\right\} \\
 & =\underbrace{\frac{1}{2}\frac{1}{\sqrt{2\pi\sigma_{c}^{2}}}\exp\left\{ -\frac{y^{2}}{2\sigma_{c}^{2}}\right\} }_{h\left(y\right)}\exp\left\{ \underbrace{-\frac{\mu^{2}}{2}}_{\omega_{1}\left(\mu\right)}\underbrace{\frac{1}{\sigma_{c}^{2}}}_{t_{1}\left(y\right)}+\underbrace{\mu}_{\omega_{2}\left(\mu\right)}\underbrace{\frac{y}{\sigma_{c}^{2}}}_{t_{2}\left(y\right)}\right\} .
\end{flalign*}

\end_inset

This last expression has the form of an exponential family, with 
\begin_inset Formula $c\left(\mu\right)=1$
\end_inset

.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:min-sufficient-exp-family"

\end_inset

, we showed that a minimal sufficient statistic for an exponential family
 is given by 
\begin_inset Formula $T\left(Y\right)=\left(T_{1}\left(Y\right),\ldots,T_{n}\left(Y\right)\right)$
\end_inset

.
 It follows that 
\begin_inset Formula 
\begin{flalign*}
T\left(Y\right) & =\left(\frac{1}{\sigma_{c}^{2}},\frac{Y}{\sigma_{c}^{2}}\right)
\end{flalign*}

\end_inset

is a minimal sufficient statistic for 
\begin_inset Formula $\mu$
\end_inset

 conditioned on 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Expanded definition of ancillarity
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $\theta=\left(\psi,\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda$
\end_inset

 is not of direct interest (
\begin_inset Formula $\lambda$
\end_inset

 is a nuisance parameter).
 Suppose that 
\begin_inset Formula $S=\left(T,C\right)$
\end_inset

 is minimal sufficient for 
\begin_inset Formula $\theta$
\end_inset

, where the pdf of 
\begin_inset Formula $C$
\end_inset

 depends on 
\begin_inset Formula $\lambda$
\end_inset

 but not on 
\begin_inset Formula $\psi$
\end_inset

, and the conditional pdf of 
\begin_inset Formula $T$
\end_inset

 given 
\begin_inset Formula $C$
\end_inset

 depends on 
\begin_inset Formula $\psi$
\end_inset

 but not 
\begin_inset Formula $\lambda$
\end_inset

.
 Then 
\begin_inset Formula $C$
\end_inset

 is called ancillary in the extended sense.
\end_layout

\begin_layout Subsection
Complete statistics
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $f\left(t|\theta\right)$
\end_inset

 be a family of pdfs or pmfs for a statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 The family of probability distributions is called 
\shape italic
complete
\shape default
 if 
\begin_inset Formula $\E_{\theta}\left[g\left(T\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

 implies 
\begin_inset Formula $P_{\theta}\left(\left\{ g\left(T\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
 Equivalently, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is called a 
\shape italic
complete statistic
\shape default
.
 I.e., 
\begin_inset Formula $T$
\end_inset

 is complete if 
\begin_inset Formula 
\[
\E_{\theta}\left[g\left(T\right)\right]=0\quad\forall\theta\quad\implies\quad P_{\theta}\left(\left\{ g\left(T\right)=0\right\} \right)=1\quad\forall g.
\]

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $T\left(X\right)$
\end_inset

 is sufficient and complete for 
\begin_inset Formula $\theta$
\end_inset

, then 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Complete statistics in the exponential family]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:complete-stat-exp-family"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid observations from an exponential family with pdf or pmf of the form
\begin_inset Formula 
\begin{flalign*}
f\left(x|\boldsymbol{\theta}\right) & =h\left(x\right)c\left(\boldsymbol{\theta}\right)\exp\left(\sum_{j=1}^{k}\omega_{j}\left(\boldsymbol{\theta}\right)t_{j}\left(x\right)\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\theta_{2},\ldots,\theta_{k}\right)$
\end_inset

.
 Then the statistic
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}t_{1}\left(X_{i}\right),\sum_{i=1}^{n}t_{2}\left(X_{i}\right),\ldots,\sum_{i=1}^{n}t_{k}\left(X_{i}\right)\right)
\]

\end_inset

is complete if 
\begin_inset Formula $\left\{ \left(\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right)\right):\boldsymbol{\theta}\in\Theta\right\} $
\end_inset

 contains an open set in 
\begin_inset Formula $\mathbb{R}^{k}$
\end_inset

.
 (This is Theorem 6.2.25 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Theorem
If a minimal sufficient statistic exists, then any complete sufficient statistic
 is also a minimal sufficient statistic.
 (This is Theorem 6.2.28 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Remark
Minimal sufficiency does not imply completeness.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:3-param-exp-family"

\end_inset

, we expressed the family of distributions with densities 
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\frac{2}{\Gamma\left(1/4\right)}\exp\left[-\left(x-\theta\right)^{4}\right]
\end{flalign*}

\end_inset

for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f\left(x|\theta\right) & =\underbrace{\frac{2}{\Gamma\left(1/4\right)}\exp\left\{ -x^{4}\right\} }_{h\left(x\right)}\underbrace{\exp\left\{ -\theta^{4}\right\} }_{c\left(\theta\right)}\exp\left\{ \underbrace{4x^{3}}_{t_{1}\left(x\right)}\underbrace{\theta}_{\omega_{1}\left(\theta\right)}\underbrace{-6x^{2}}_{t_{2}\left(x\right)}\underbrace{\theta^{2}}_{\omega_{2}\left(\theta\right)}+\underbrace{4x}_{t_{3}\left(x\right)}\underbrace{\theta^{3}}_{\omega_{3}\left(\theta\right)}\right\} ,
\end{flalign*}

\end_inset

which is in exponential form, so that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(4x^{3},-6x^{2},4x\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:min-sufficient-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 The parameter space for this distribution is given by 
\begin_inset Formula $\left\{ \omega_{1}\left(\theta\right),\omega_{2}\left(\theta\right),\omega_{3}\left(\theta\right)\right\} =\left\{ \theta,\theta^{2},\theta^{3}\right\} $
\end_inset

, i.e., 
\begin_inset Formula $k=3$
\end_inset

.
 We have 
\begin_inset Formula $d=1$
\end_inset

, so by definition this familiy of densities is a curved exponential family.
 Its graph in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

 is the curve shown in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
figref{curved-exp-family}
\end_layout

\end_inset

.
 This graph does not have positive length (volume in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

), i.e., 
\begin_inset Formula $\left\{ \theta,\theta^{2},\theta^{3}\right\} $
\end_inset

 does not contain an open set in 
\begin_inset Formula $\mathbb{R}^{3}$
\end_inset

, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is not complete.
\end_layout

\begin_layout Remark
\begin_inset ERT
status open

\begin_layout Plain Layout

<<curved-exp-family, echo=FALSE, fig.height=3, fig.align='center', fig.pos='!htb',
 fig.cap='graph of $x=
\backslash

\backslash
theta,y=
\backslash

\backslash
theta^{2},z=
\backslash

\backslash
theta^{3}$'>>=
\end_layout

\begin_layout Plain Layout

library(lattice)
\end_layout

\begin_layout Plain Layout

t <- seq(-2, 2, length.out=500)
\end_layout

\begin_layout Plain Layout

cloud(z~x+y, data.frame(x=t, y=t^2, z=t^3), pch=".", ylim=c(0,1.5), xlim=c(-1,1),
 zlim=c(-2,2))
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Remark
Although the condition in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that the set 
\begin_inset Formula $\left\{ \left(\omega_{1}\left(\boldsymbol{\theta}\right),\ldots,\omega_{k}\left(\boldsymbol{\theta}\right):\boldsymbol{\theta}\in\Theta\right)\right\} $
\end_inset

 contain an open set is sufficient to guarantee completeness, it is not
 necessary.
 That is, the failure of this condition does not show that a statistic is
 not complete, as shown in the following example.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

 and the parameter space consists of the two points 
\begin_inset Formula $p=1/4$
\end_inset

 and 
\begin_inset Formula $p=3/4$
\end_inset

.
 The pmf for 
\begin_inset Formula $X$
\end_inset

 can be written as
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =p^{x}\left(1-p\right)^{1-x}\\
 & =p^{x}\left(1-p\right)^{-x}\left(1-p\right)\\
 & =\left(1-p\right)\left(\frac{p}{1-p}\right)^{x}\\
 & =\left(1-p\right)\exp\left\{ \log\left(\left(\frac{p}{1-p}\right)^{x}\right)\right\} \\
 & =\underbrace{\left(1-p\right)}_{c\left(p\right)}\exp\left\{ \underbrace{x}_{t_{1}\left(x\right)}\underbrace{\log\left(\frac{p}{1-p}\right)}_{\omega_{1}\left(p\right)}\right\} 
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise, i.e., 
\begin_inset Formula $X$
\end_inset

 has an exponential family distribution.
 Then, the parameter space is given by
\begin_inset Formula 
\begin{flalign*}
\left\{ \omega_{1}\left(p\right):p=\left\{ \frac{1}{4},\frac{3}{4}\right\} \right\}  & =\left\{ \log\left(\frac{1/4}{1-\left(1/4\right)}\right),\log\left(\frac{3/4}{1-\left(3-4\right)}\right)\right\} \\
 & =\left\{ \log\left(\frac{1/4}{3/4}\right),\log\left(\frac{3/4}{1/4}\right)\right\} \\
 & =\left\{ \log\frac{1}{3},\log3\right\} 
\end{flalign*}

\end_inset

The condition of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 is not satisfied because the parameter space has only two points in it,
 and hence the range of 
\begin_inset Formula $\log\left(p/\left(1-p\right)\right)$
\end_inset

 as 
\begin_inset Formula $p$
\end_inset

 varies over the parameter space does not contain an open set (an interval)
 in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

.
\end_layout

\begin_layout Example
Yet, the distribution of 
\begin_inset Formula $X$
\end_inset

 is complete.
 To see this, suppose 
\begin_inset Formula $g$
\end_inset

 is a function defined on the sample space such that 
\begin_inset Formula $\E\left[g\left(X\right)\right]=0$
\end_inset

.
 To show that the distribution of 
\begin_inset Formula $X$
\end_inset

 is complete, we need to show that the only function satisfying this equality
 for all 
\begin_inset Formula $p$
\end_inset

 in the parameter space is the function which is identically zero.
 The sample space consists of the two points 
\begin_inset Formula $X=0$
\end_inset

 and 
\begin_inset Formula $X=1$
\end_inset

, so we need to show that this implies 
\begin_inset Formula $g\left(0\right)=g\left(1\right)=0$
\end_inset

.
\end_layout

\begin_layout Example
The expected value of 
\begin_inset Formula $g\left(X\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[g\left(X\right)\right] & =\sum_{x=0}^{1}g\left(x\right)\cdot p_{X}\left(x\right)\\
 & =\sum_{x=0}^{1}g\left(x\right)\cdot p^{x}\left(1-p\right)^{1-x}\\
 & =g\left(0\right)\cdot p^{0}\left(1-p\right)^{1-0}+g\left(1\right)\cdot p^{1}\left(1-p\right)^{1-1}\\
 & =\left(1-p\right)g\left(0\right)+pg\left(1\right)
\end{flalign*}

\end_inset

so that if 
\begin_inset Formula $p=1/4$
\end_inset

, we have
\begin_inset Formula 
\[
0=\E_{p=1/4}\left[g\left(X\right)\right]=\frac{3}{4}g\left(0\right)+\frac{1}{4}g\left(1\right)
\]

\end_inset

and if 
\begin_inset Formula $p=3/4$
\end_inset

, we have
\begin_inset Formula 
\[
0=\E_{p=3/4}\left[g\left(X\right)\right]=\frac{1}{4}g\left(0\right)+\frac{3}{4}g\left(1\right).
\]

\end_inset

So, we have 
\begin_inset Formula $g\left(1\right)=-3g\left(0\right)$
\end_inset

, which gives
\begin_inset Formula 
\[
0=\frac{1}{4}g\left(0\right)+\frac{3}{4}g\left(1\right)=\frac{1}{4}g\left(0\right)+\frac{3}{4}\left(-3g\left(0\right)\right)=-2g\left(0\right)\implies0=g\left(0\right)
\]

\end_inset

and thus 
\begin_inset Formula $g\left(1\right)=-3\left(0\right)=0$
\end_inset

.
 Thus, the only solution is 
\begin_inset Formula $g\left(1\right)=g\left(0\right)=0$
\end_inset

, and with just two points in the parameter space we have that the family
 of distributions is complete.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:complete-stat-bernoulli-part-a"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Bernoulli}\left(p\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =p^{x}\left(1-p\right)^{1-x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Show that 
\begin_inset Formula $T_{1}=X_{2}-X_{1}$
\end_inset

 is not a complete statistic.
\end_layout

\begin_layout Example
By definition, 
\begin_inset Formula $T_{1}$
\end_inset

 will be a complete statistic if 
\begin_inset Formula $\E\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

 implies 
\begin_inset Formula $P\left(\left\{ g\left(T_{1}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 Suppose that 
\begin_inset Formula $g\left(T_{1}\right)=T_{1}$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\E\left[g\left(T_{1}\right)\right] & =\E\left[T_{1}\right]\\
 & =\E\left[X_{2}-X_{1}\right]\\
 & =\E\left[X_{2}\right]-\E\left[X_{1}\right]\\
 & =\sum_{x_{2}=0}^{1}\left[x_{2}\cdot p^{x_{2}}\left(1-p\right)^{1-x_{2}}\right]-\sum_{x_{1}=0}^{1}\left[x_{1}\cdot p^{x_{1}}\left(1-p\right)^{1-x_{1}}\right]\\
 & =\left[0\cdot p^{0}\left(1-p\right)^{1-0}+1\cdot p^{1}\left(1-p\right)^{1-1}\right]-\left[0\cdot p^{0}\left(1-p\right)^{1-0}+1\cdot p^{1}\left(1-p\right)^{1-1}\right]\\
 & =\left[0+p\right]-\left[0+p\right]\\
 & =0.
\end{flalign*}

\end_inset

So, this choice of 
\begin_inset Formula $g$
\end_inset

 satisfies 
\begin_inset Formula $\E\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ g\left(T_{1}\right)=0\right\} \right) & =P\left(\left\{ T_{1}=0\right\} \right)\\
 & =P\left(\left\{ X_{2}-X_{1}=0\right\} \right).
\end{flalign*}

\end_inset

This probability will be equal to 1 if and only if 
\begin_inset Formula $X_{1}=X_{2}$
\end_inset

, but this will not always be the case, e.g., suppose that 
\begin_inset Formula $X_{1}=0$
\end_inset

 and 
\begin_inset Formula $X_{2}=1$
\end_inset

.
 Even though our choice of 
\begin_inset Formula $g$
\end_inset

 satisfies 
\begin_inset Formula $\E\left[g\left(T_{1}\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

, it does not imply 
\begin_inset Formula $P\left(\left\{ g\left(T_{1}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 The condition of completeness applies to all functions (all choices of
 
\begin_inset Formula $g$
\end_inset

), so it follows that 
\begin_inset Formula $T_{1}$
\end_inset

 is not complete.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bernoulli complete statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:complete-stat-bernoulli-part-a"

\end_inset

.
 Show that 
\begin_inset Formula $T_{2}=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a complete statistic.
 (This is example 6.2.22 from Casella & Berger.)
\end_layout

\begin_layout Example
\begin_inset Formula $T_{2}$
\end_inset

 is the sum of 
\begin_inset Formula $n$
\end_inset

 independent Bernoulli random variables each having the same success probability
 
\begin_inset Formula $p$
\end_inset

.
 It follows that 
\begin_inset Formula $T_{2}\sim\text{Binomial}\left(n,p\right)$
\end_inset

, so that the pmf of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{T_{2}}\left(t\right) & =\binom{n}{t}p^{t}\left(1-p\right)^{n-t}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $t\in\left\{ 0,1,2,\ldots,n\right\} $
\end_inset

 and 
\begin_inset Formula $p_{T_{2}}\left(t\right)=0$
\end_inset

 otherwise.
 Suppose 
\begin_inset Formula $g\left(T_{2}\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\E\left[g\left(T_{2}\right)\right]=0$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
0 & =\E\left[g\left(T_{2}\right)\right]\\
 & =\sum_{k=0}^{n}\left[g\left(k\right)\cdot p_{T_{2}}\left(k\right)\right]\\
 & =\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}p^{k}\left(1-p\right)^{n-k}\right]\\
 & =\left(1-p\right)^{n}\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}p^{k}\left(1-p\right)^{-k}\right]\\
 & =\left(1-p\right)^{n}\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}\left(\frac{p}{1-p}\right)^{k}\right].
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $p\in\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $\left(1-p\right)^{n}>0$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 It follows that this expression will be equal to zero if and only if 
\begin_inset Formula 
\[
\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}\left(\frac{p}{1-p}\right)^{k}\right]=0.
\]

\end_inset

Let 
\begin_inset Formula $r=\left(p/\left(1-p\right)\right)$
\end_inset

.
 Then, this sum is a polynomial function of 
\begin_inset Formula $r$
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
\sum_{k=0}^{n}\left[g\left(k\right)\cdot\binom{n}{k}r^{k}\right] & =g\left(0\right)\cdot\binom{n}{0}r^{0}+\cdots+g\left(n\right)\cdot\binom{n}{n}r^{n}\\
 & =g\left(0\right)+g\left(1\right)\cdot nr^{1}+g\left(2\right)\binom{n}{2}r^{2}+\cdots+g\left(n-1\right)\cdot nr^{n-1}+g\left(n\right)r^{n}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $r=p/\left(1-p\right)>0$
\end_inset

, so that for the 
\begin_inset Formula $k\mbox{th}$
\end_inset

 term, we will have 
\begin_inset Formula $\binom{n}{k}>0$
\end_inset

 and 
\begin_inset Formula $r^{k}>0$
\end_inset

.
 It follows that this sum is equal to zero if and only if 
\begin_inset Formula $g\left(k\right)=0$
\end_inset

 for 
\begin_inset Formula $k=\left\{ 0,1,\ldots,n\right\} $
\end_inset

.
 
\begin_inset Formula $T_{2}$
\end_inset

 takes on the values 
\begin_inset Formula $0,1,\ldots,n$
\end_inset

 with probability 1 (recall that 
\begin_inset Formula $T_{2}$
\end_inset

 represents the probability of 
\begin_inset Formula $k$
\end_inset

 successes in 
\begin_inset Formula $n$
\end_inset

 trials), so that 
\begin_inset Formula $P\left(\left\{ g\left(T_{2}\right)=0\right\} \right)=1$
\end_inset

 for all 
\begin_inset Formula $p$
\end_inset

.
 It follows that 
\begin_inset Formula $T_{2}$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson complete statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\text{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pmf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
p_{X}\left(x\right) & =\frac{\mathrm{e}^{-\lambda}\lambda^{x}}{x!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Example
It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:dist-of-sum-of-poisson-rv"

\end_inset

 that 
\begin_inset Formula $X_{1}+X_{2}\sim\text{Poisson}\left(\lambda+\lambda\right)=\text{Poisson}\left(2\lambda\right)$
\end_inset

, that 
\begin_inset Formula $\left(X_{1}+X_{2}\right)+X_{3}\sim\text{Poisson}\left(2\lambda+\lambda\right)=\text{Poisson}\left(3\lambda\right)$
\end_inset

, and therefore that 
\begin_inset Formula 
\[
T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}\sim\text{Poisson}\left(n\lambda\right),
\]

\end_inset

so that the pmf of 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{T\left(\mathbf{X}\right)}\left(t\right) & =\frac{\mathrm{e}^{-n\lambda}\left(n\lambda\right)^{t}}{t!}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $t\in\left\{ 0,1,2,\ldots\right\} $
\end_inset

 and 
\begin_inset Formula $p_{T\left(\mathbf{X}\right)}\left(t\right)=0$
\end_inset

 otherwise.
 Suppose that 
\begin_inset Formula $g\left(T\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\E\left[g\left(T\right)\right]=0$
\end_inset

, so that we have
\begin_inset Formula 
\[
0=\E\left[g\left(T\right)\right]=\sum_{t=0}^{\infty}\left[g\left(t\right)\cdot\frac{\mathrm{e}^{-n\lambda}\left(n\lambda\right)^{t}}{t!}\right]=\mathrm{e}^{-n\lambda}\sum_{t=0}^{\infty}g\left(t\right)\cdot\frac{\left(n\lambda\right)^{t}}{t!}.
\]

\end_inset

We will have 
\begin_inset Formula $\mathrm{e}^{-n\lambda}>0$
\end_inset

 for all 
\begin_inset Formula $n$
\end_inset

 and all 
\begin_inset Formula $\lambda\geq0$
\end_inset

.
 It follows that this expression will be equal to zero if and only if
\begin_inset Formula 
\[
\sum_{t=0}^{\infty}g\left(t\right)\cdot\frac{\left(n\lambda\right)^{t}}{t!}=0.
\]

\end_inset

This sum is a polynomial function of 
\begin_inset Formula $\left(n\lambda\right)$
\end_inset

.
 For the sum to be equal to zero, the coefficient 
\begin_inset Formula $g\left(t\right)/t!$
\end_inset

 must be equal to zero for all 
\begin_inset Formula $t$
\end_inset

.
 Because 
\begin_inset Formula $t!\geq1$
\end_inset

, it follows that 
\begin_inset Formula $g\left(t\right)=0$
\end_inset

 for all 
\begin_inset Formula $t$
\end_inset

.
 Thus, we have 
\begin_inset Formula 
\[
\E\left[g\left(T\right)\right]=0\quad\forall\lambda\implies P\left(\left\{ g\left(T\right)=0\right\} \right)=1\quad\forall\lambda,
\]

\end_inset

so by definition 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform complete statistic]
\end_layout

\end_inset

Let 
\begin_inset Formula $X\sim\mathcal{U}\left(0,\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x\right) & =\frac{1}{\theta}I_{\left\{ 0<x<\theta\right\} }.
\end{flalign*}

\end_inset

Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}$
\end_inset

 is a complete statistic.
 (This is example 6.2.23 from Casella & Berger.)
\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-statistic-uniform"

\end_inset

, the cdf of 
\begin_inset Formula $X$
\end_inset

 is given by 
\begin_inset Formula $F_{X}\left(x\right)=x/\theta$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-continuous"

\end_inset

, the pdf of 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(n\right)}}\left(x\right) & =\frac{n!}{\left(n-1\right)!\left(n-n\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{n-1}\left[1-F_{X}\left(x\right)\right]^{n-n}\\
 & =\frac{n!}{\left(n-1\right)!0!}\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1}\left(1-\frac{x}{\theta}\right)^{0}I_{\left\{ 0<x<\theta\right\} }\\
 & =n\left(\frac{\left(n-1\right)!}{\left(n-1\right)!}\right)\left(\frac{1}{\theta}\right)\left(\frac{x^{n-1}}{\theta^{n-1}}\right)I_{\left\{ 0<x<\theta\right\} }\\
 & =\frac{nx^{n-1}}{\theta^{n}}I_{\left\{ 0<x<\theta\right\} }.
\end{flalign*}

\end_inset

Suppose 
\begin_inset Formula $g\left(T\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\E\left[g\left(T\right)\right]=0$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

, so that we have
\begin_inset Formula 
\[
0=\E\left[g\left(T\right)\right]=\int_{0}^{\theta}g\left(t\right)\cdot f_{X_{\left(n\right)}}\left(t\right)\dif t=\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\dif t.
\]

\end_inset

By assumption, 
\begin_inset Formula $\E\left[g\left(T\right)\right]=0$
\end_inset

, i.e., it is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, so that its derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 is zero.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
0 & =\frac{\dif}{\dif\theta}\left[\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\dif t\right]\\
 & =\frac{\dif}{\dif\theta}\left[n\theta^{-n}\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\dif t\right]\\
 & =n\theta^{-n}\frac{\dif}{\dif\theta}\left[\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\dif t\right]+\left(\frac{\dif}{\dif\theta}n\theta^{-n}\right)\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\dif t\tag{product rule}\\
 & =n\theta^{-n}\left[g\left(\theta\right)\cdot\theta^{n-1}-g\left(0\right)\cdot0^{n-1}\right]+n\left(-n\theta^{-n-1}\right)\int_{0}^{\theta}g\left(t\right)\cdot t^{n-1}\dif t\tag{Fundamental Theorem of Calculus}\\
 & =n\theta^{-1}g\left(\theta\right)+\left[-n\theta^{-1}\int_{0}^{\theta}g\left(t\right)\cdot nt^{n-1}\theta^{-n}\dif t\right]\\
 & =n\theta^{-1}g\left(\theta\right)+\left(-n\theta^{-1}\cdot0\right)\tag{by assumption, \ensuremath{\E\left[g\left(T\right)\right]=0}}\\
 & =n\theta^{-1}g\left(\theta\right).
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\theta>0,$
\end_inset

 so that 
\begin_inset Formula $\theta^{-1}>0$
\end_inset

.
 It follows that 
\begin_inset Formula $n\theta^{-1}g\left(\theta\right)$
\end_inset

 is equal to zero if and only if 
\begin_inset Formula $g\left(\theta\right)=0$
\end_inset

, which is true for for all 
\begin_inset Formula $\theta>0$
\end_inset

.
 Noting that we will always have 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}>0$
\end_inset

, it follows that 
\begin_inset Formula $P\left(\left\{ g\left(T\right)=0\right\} \right)=1$
\end_inset

, and thus that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Remark
The Fundamental Theorem of Calculus applies only to functions that are Riemann-i
ntegrable.
 Thus, the equation 
\begin_inset Formula 
\begin{flalign*}
\frac{\dif}{\dif\theta}\int_{0}^{\theta}g\left(t\right)\dif t & =g\left(\theta\right)
\end{flalign*}

\end_inset

is valid only at points of continuity of Riemann-integrable 
\begin_inset Formula $g$
\end_inset

.
 The condition of completeness applies to all functions, not just Riemann-integr
able ones, so the argument above does not, strictly speaking, show that
 
\begin_inset Formula $T$
\end_inset

 is a complete statistic.
 From a more practical view, though, this distinction is not of concern
 since the condition of Riemann-integrability is so general that it includes
 virtually any function we could think of.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Basu's Theorem]
\end_layout

\end_inset

If 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete and minimal sufficient statistic, then 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is independent of every ancillary statistic.
 (This is Theorem 6.2.24 from Casella & Berger.)
\begin_inset CommandInset label
LatexCommand label
name "thm:basu"

\end_inset


\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
That is, if 
\begin_inset Formula $T$
\end_inset

 depends on some ancillary statistic and 
\begin_inset Formula $T$
\end_inset

 is minimal sufficient, then 
\begin_inset Formula $T$
\end_inset

 cannot be complete.
 Basu's Theorem allows us to deduce the independence of two statistics without
 finding their joint distribution.
 To use Basu's Theorem, we need to show that a statistic is complete.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\text{Exp}\left(\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\theta\mathrm{e}^{-\theta x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

, so that the joint pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\theta\mathrm{e}^{-\theta x_{i}}=\theta^{n}\mathrm{e}^{\sum_{i=1}^{n}-\theta x_{i}}=\theta^{n}\mathrm{e}^{-\theta\sum_{i=1}^{n}x_{i}}.
\]

\end_inset

Show that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 is independent of 
\begin_inset Formula $Y=X_{n}/\sum_{i=1}^{n}X_{i}$
\end_inset

.
\end_layout

\begin_layout Example
We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:minimal-sufficient-exp-rv"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a minimal sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 We can express 
\begin_inset Formula $f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)$
\end_inset

 in exponential family form as
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\underbrace{\theta^{n}}_{c\left(\theta\right)}\exp\left\{ \underbrace{-\theta}_{\omega_{1}\left(\theta\right)}\underbrace{\sum_{i=1}^{n}x_{i}}_{t_{1}\left(\mathbf{x}\right)}\right\} 
\end{flalign*}

\end_inset

with 
\begin_inset Formula $h\left(\mathbf{x}\right)=1$
\end_inset

.
 The parameter space for this distribution is given by 
\begin_inset Formula $\left\{ \omega_{1}\left(\theta\right)=-\theta:\theta>0\right\} $
\end_inset

, i.e., 
\begin_inset Formula $k=1$
\end_inset

.
 The representation of this parameter space in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

 is the interval 
\begin_inset Formula $\left(-\infty,0\right)$
\end_inset

, which has positive length, i.e., it is an open set, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is complete.
\end_layout

\begin_layout Example
The cdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y\leq y\right\} \right) & =P\left(\left\{ \frac{X_{n}}{\sum_{i=1}^{n}X_{i}}\leq y\right\} \right).
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 be iid observations from 
\begin_inset Formula $F_{Y}\left(y\right)$
\end_inset

 (corresponding to 
\begin_inset Formula $\theta=1$
\end_inset

) with 
\begin_inset Formula $X_{i}=\theta Z_{i}$
\end_inset

, so that we have
\begin_inset Formula 
\[
P\left(\left\{ Y\leq y\right\} \right)=P\left(\left\{ \frac{\theta Z_{n}}{\sum_{i=1}^{n}\theta Z_{i}}\leq y\right\} \right)=P\left(\left\{ \frac{Z_{n}}{\sum_{i=1}^{n}Z_{i}}\leq y\right\} \right).
\]

\end_inset

This expression does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $Y$
\end_inset

 is an ancillary statistic.
 Because 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete and minimal sufficient statistic, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:basu"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mathcal{U}\left(\theta,\theta+1\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid.
 Is 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(X_{\left(1\right)},X_{\left(n\right)}\right)$
\end_inset

 a complete statistic?
\end_layout

\begin_layout Example
Any one-to-one function of a minimal sufficient statistic is also minimal
 sufficient, so it follows that 
\begin_inset Formula $\left(X_{\left(1\right)},X_{\left(n\right)}-X_{\left(1\right)}\right)$
\end_inset

 is also minimal sufficient.
 We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:ancillary-stat-uniform-range"

\end_inset

 that the range statistic 
\begin_inset Formula $R=X_{\left(n\right)}-X_{\left(1\right)}$
\end_inset

 is an ancillary statistic for a uniform random variable.
 (More generally, we have 
\begin_inset Formula $X\sim\mathcal{U}\left(\theta,\theta+1\right)$
\end_inset

.
 Suppose that 
\begin_inset Formula $Z=\mathcal{U}\left(0,1\right)$
\end_inset

, so that 
\begin_inset Formula $X=Z+\theta$
\end_inset

.
 It follows that 
\begin_inset Formula $X\sim\mathcal{U}\left(\theta,\theta+1\right)$
\end_inset

 is a location family, and range is ancillary for a location family.) Thus,
 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 depends on an ancillary statistic, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:basu"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is not complete.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
We showed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-normal"

\end_inset

 that the 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 family is an exponential family, with 
\begin_inset Formula $\omega_{1}\left(\boldsymbol{\theta}\right)=1/\sigma^{2}$
\end_inset

 and 
\begin_inset Formula $\omega_{2}\left(\boldsymbol{\theta}\right)=\mu/\sigma^{2}$
\end_inset

.
 The parameter space for this family is given by 
\begin_inset Formula 
\[
\Theta=\left\{ \left(\mu,\sigma^{2}\right):-\infty<\mu<\infty,\sigma>0\right\} ,
\]

\end_inset

i.e., 
\begin_inset Formula $k=2$
\end_inset

.
 The representation of this parameter space in 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset

 is the set 
\begin_inset Formula 
\[
\left\{ \left(1/\sigma^{2},\mu/\sigma^{2}\right):\left(\mu,\sigma\right)\in\Theta\right\} =\left\{ \left(x,y\right):x>0,-\infty<y<\infty\right\} ,
\]

\end_inset

which clearly contains an open set, e.g., the rectangle 
\begin_inset Formula $\left\{ \left(x,y\right):1<x<2,1<y<2\right\} $
\end_inset

.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-normal"

\end_inset

, we found that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\left(\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}X_{i}^{2}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:complete-stat-exp-family"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\begin_layout Chapter
Point estimation
\end_layout

\begin_layout Standard
Many inferential problems fall into one of three types: point estimation,
 confidence estimation, or hypothesis testing.
 Point estimation refers to providing a single 
\begin_inset Quotes eld
\end_inset

best guess
\begin_inset Quotes erd
\end_inset

 of some quantity of interest, e.g., a model parameter, a cdf 
\begin_inset Formula $F$
\end_inset

, a pdf 
\begin_inset Formula $f$
\end_inset

, a regression function, a prediction for a future value 
\begin_inset Formula $Y$
\end_inset

 of a random variable.
\end_layout

\begin_layout Definition
A 
\shape italic
point estimator
\shape default
 is any function 
\begin_inset Formula $W\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 of a sample; that is, any statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a point estimator.
 An 
\shape italic
estimate
\shape default
 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is the observed value of an estimator 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 (that is, a number) that is obtained when a sample is actually taken.
\end_layout

\begin_layout Standard
Often, we are interested in estimating some function 
\begin_inset Formula $T\left(\theta\right)$
\end_inset

, e.g., if 
\begin_inset Formula $X\sim N\left(\mu,\sigma^{2}\right)$
\end_inset

, then the parameter is 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu,\sigma^{2}\right)$
\end_inset

.
 If our goal is to estimate 
\begin_inset Formula $\mu$
\end_inset

, then 
\begin_inset Formula $\mu=T\left(\boldsymbol{\theta}\right)$
\end_inset

 is called the 
\shape italic
parameter of interest
\shape default
 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is called a 
\shape italic
nuisance parameter
\shape default
.
\end_layout

\begin_layout Section
Methods of finding estimators
\end_layout

\begin_layout Subsection
Method of moments estimators
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X\sim f\left(x|\theta\right)$
\end_inset

.
 The 
\begin_inset Formula $r\mbox{th}$
\end_inset

 moment of 
\begin_inset Formula $X$
\end_inset

 is denoted 
\begin_inset Formula $\mu_{r}=\E\left[X^{r}\right]$
\end_inset

.
 Note that 
\begin_inset Formula $\mu_{r}$
\end_inset

 is a function of 
\begin_inset Formula $\theta$
\end_inset

, e.g., 
\begin_inset Formula $\E\left[X\right]=\mu_{1}$
\end_inset

, 
\begin_inset Formula $\E\left[X^{2}\right]=\mu_{2}$
\end_inset

, 
\begin_inset Formula $\Var\left(X\right)=\mu_{2}-\mu_{1}^{2}$
\end_inset

.
 A method of moments estimator for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 is obtained by solving
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{r} & =\sum_{i=1}^{n}\frac{X_{i}^{r}}{n}.
\end{flalign*}

\end_inset

Moment estimators are not invariant to transformation of the data (a function
 of 
\begin_inset Formula $x$
\end_inset

) or to reparameterization of 
\begin_inset Formula $\theta$
\end_inset

 (a function of 
\begin_inset Formula $\theta$
\end_inset

).
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Exponential MOM estimator]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:mom-exp-rv"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Exp}\left(\theta\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid, i.e., each has the pdf of 
\begin_inset Formula $X$
\end_inset

, which is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\theta\mathrm{e}^{-\theta x}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

.
 Find a moment estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
We will find 
\begin_inset Formula $\hat{\mu}_{1}=\E\left[X\right]$
\end_inset

.
\begin_inset Formula 
\[
\hat{\mu}_{1}=\E\left[X\right]=\int_{0}^{\infty}x\cdot\theta\mathrm{e}^{-\theta x}\mbox{d}x=\lim_{t\rightarrow\infty}\int_{0}^{t}x\cdot\theta\mathrm{e}^{-\theta x}\mbox{d}x
\]

\end_inset

Let 
\begin_inset Formula $u=x$
\end_inset

 and 
\begin_inset Formula $v'=\theta\mathrm{e}^{-\theta x}$
\end_inset

, so that 
\begin_inset Formula $u'=1$
\end_inset

 and 
\begin_inset Formula $v=-\mathrm{e}^{-\theta x}$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\int_{0}^{t}uv'\dif x & =\left[uv\right|_{0}^{t}-\int_{0}^{t}vu'\dif x\\
 & =-x\mathrm{e}^{-\theta x}\Big\rvert_{0}^{t}-\int_{0}^{t}-\mathrm{e}^{-\theta x}\dif x\\
 & =\left(-t\mathrm{e}^{-\theta t}-0\right)-\left(\frac{1}{\theta}\mathrm{e}^{-\theta x}\right|_{0}^{t}\\
 & =-t\mathrm{e}^{-\theta t}-\left(\frac{1}{\theta}\mathrm{e}^{-\theta t}-\frac{1}{\theta}\mathrm{e}^{0}\right)\\
 & =-t\mathrm{e}^{-\theta t}-\frac{1}{\theta}\left(\mathrm{e}^{-\theta t}-1\right)
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\lim_{t\rightarrow\infty}\left[-t\mathrm{e}^{-\theta t}-\frac{1}{\theta}\left(\mathrm{e}^{-\theta t}-1\right)\right]\\
 & =0-\lim_{t\rightarrow\infty}\frac{1}{\theta}\left(\mathrm{e}^{-\theta t}-1\right)\\
 & =-\frac{1}{\theta}\lim_{t\rightarrow\infty}\left(\mathrm{e}^{-\theta t}-1\right)\\
 & =-\frac{1}{\theta}\left(0-1\right)\\
 & =\frac{1}{\theta}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\[
\hat{\mu}_{1}=\frac{\sum_{i=1}^{n}X_{i}}{n}\implies\frac{1}{\theta}=\bar{x}\implies\hat{\theta}=\frac{1}{\bar{x}}.
\]

\end_inset


\end_layout

\begin_layout Remark
Had we used the parameterization 
\begin_inset Formula $f_{X}\left(x|\theta\right)=\left(1/\theta\right)\mathrm{e}^{-x/\theta}$
\end_inset

, we would instead have found 
\begin_inset Formula $\hat{\theta}=\bar{x}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:mom-sampling-replacement"

\end_inset

Suppose we have a population with 
\begin_inset Formula $\theta$
\end_inset

 members labeled 
\begin_inset Formula $1,\ldots,\theta$
\end_inset

 from which we sample 
\begin_inset Formula $n$
\end_inset

 observations with replacement and record their labels 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

.
 Find a moment estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
We are (randomly) sampling with replacement, so an equal-likelihood model
 is appropriate.
 Then, it follows that the pmf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{X_{i}}\left(x_{i}|\theta\right)=P\left(\left\{ X_{i}=x_{i}\right\} \right)=\frac{1}{\theta}I_{\left\{ x_{i}\in\left\{ 1,\ldots,\theta\right\} \right\} },
\]

\end_inset

so that the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}\left[\frac{1}{\theta}I_{\left\{ x_{i}\in\left\{ 1,\ldots,\theta\right\} \right\} }\right]=\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }I_{\left\{ x_{\left(n\right)}\leq\theta\right\} }.
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\E\left[X\right]\\
 & =\sum_{x=1}^{\theta}x\cdot p_{X}\left(x|\theta\right)\\
 & =\sum_{x=1}^{\theta}x\cdot\frac{1}{\theta}\\
 & =\frac{1}{\theta}\sum_{x=1}^{\theta}x\\
 & =\frac{1}{\theta}\left[1+2+\cdots+\left(\theta-1\right)+\theta\right]\\
 & =\frac{1}{\theta}\left(\frac{\theta\left(\theta+1\right)}{2}\right)\\
 & =\frac{\theta+1}{2}.
\end{flalign*}

\end_inset

We must solve 
\begin_inset Formula 
\[
\hat{\mu}_{1}=\frac{\sum_{i=1}^{n}X_{i}}{n}\implies\frac{\hat{\theta}+1}{2}=\bar{x}\implies\hat{\theta}+1=2\bar{x}\implies\hat{\theta}=2\bar{x}-1.
\]

\end_inset


\end_layout

\begin_layout Remark
The technique used above to find the sum of the first 
\begin_inset Formula $n$
\end_inset

 numbers involves taking the sums of the respective extremes, e.g., 
\begin_inset Formula $1+n$
\end_inset

, 
\begin_inset Formula $2+\left(n-1\right)=1+n$
\end_inset

, 
\begin_inset Formula $3+\left(n-2\right)=1+n$
\end_inset

, and so on.
 There are 
\begin_inset Formula $n/2$
\end_inset

 such pairs, so it follows that the sum of the first 
\begin_inset Formula $n$
\end_inset

 numbers is given by
\begin_inset Formula 
\begin{flalign}
\sum_{x=1}^{n}x & =\frac{n\left(n+1\right)}{2}.\label{eq:sum-of-first-n-numbers}
\end{flalign}

\end_inset


\end_layout

\begin_layout Example
Suppose
\begin_inset Formula 
\[
X_{1},\ldots,X_{n}\sim F\left(x|\alpha,\beta\right)=\begin{cases}
0, & x<0\\
\left(x/\beta\right)^{\alpha}, & 0\leq x\leq\beta\\
1 & x>\beta
\end{cases}.
\]

\end_inset

Find moment estimators for 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 
\end_layout

\begin_layout Example
We must find estimators for two parameters, so we will need two equations,
 so we will take the first and second moments.
 The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{X}\left(x\right)=\frac{\dif}{\dif x}F_{X}=\frac{\dif}{\dif x}\left(\frac{x}{\beta}\right)^{\alpha}=\alpha\left(\frac{x}{\beta}\right)^{\alpha-1}\left(\frac{1}{\beta}\right)=\frac{\alpha}{\beta}\left(\frac{x^{\alpha-1}}{\beta^{\alpha-1}}\right)=\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}},
\]

\end_inset

where 
\begin_inset Formula $x\in\left[0,\beta\right]$
\end_inset

 and 
\begin_inset Formula $f_{X}\left(x\right)=0$
\end_inset

 otherwise.
 Then, the first moment of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[X\right] & =\int_{0}^{\beta}x\cdot f_{X}\left(x\right)\dif x\\
 & =\int_{0}^{\beta}x\cdot\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}\dif x\\
 & =\frac{\alpha}{\beta^{\alpha}}\int_{0}^{\beta}x^{\alpha}\dif x\\
 & =\frac{\alpha}{\beta^{\alpha}}\left[\frac{1}{\alpha+1}x^{\alpha+1}\right|_{0}^{\beta}\\
 & =\frac{\alpha}{\beta^{\alpha}}\left(\frac{\beta^{\alpha+1}}{\alpha+1}-\frac{0}{\alpha+1}\right)\\
 & =\frac{\alpha\beta}{\alpha+1}
\end{flalign*}

\end_inset

and the second moment is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[X^{2}\right] & =\int_{0}^{\beta}x^{2}\cdot\frac{\alpha x^{\alpha-1}}{\beta^{\alpha}}\dif x\\
 & =\frac{\alpha}{\beta^{\alpha}}\int_{0}^{\beta}x^{\alpha+1}\dif x\\
 & =\frac{\alpha}{\beta^{\alpha}}\left[\frac{1}{\alpha+2}x^{\alpha+2}\right|_{0}^{\beta}\\
 & =\frac{\alpha}{\beta^{\alpha}}\left(\frac{\beta^{\alpha+2}}{\alpha+2}-\frac{0}{\alpha+2}\right)\\
 & =\frac{\alpha\beta^{2}}{\alpha+2}.
\end{flalign*}

\end_inset

We will solve the first equation for 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 
\begin_inset Formula 
\[
\frac{\hat{\alpha}\hat{\beta}}{\hat{\alpha}+1}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\bar{x}\implies\hat{\alpha}\hat{\beta}=\bar{x}\left(\hat{a}+1\right)\implies\hat{\beta}=\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}
\]

\end_inset

We will solve the section equation for 
\begin_inset Formula $\hat{\alpha}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\hat{\alpha}\hat{\beta}^{2}}{\hat{\alpha}+2} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\\
\implies\hat{\alpha}\hat{\beta}^{2} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\implies\hat{\alpha}\left(\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}\right)^{2} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\implies\frac{\bar{x}^{2}\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}} & =\left(\hat{\alpha}+2\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)\\
\implies\frac{\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}\left(\hat{\alpha}+2\right)} & =\frac{1}{n\bar{x}^{2}}\sum_{i=1}^{n}x_{i}^{2}
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $c=\left(1/n\bar{x}^{2}\right)\sum_{i=1}^{n}x_{i}^{2}$
\end_inset

, so that
\begin_inset Formula 
\[
\frac{\left(\hat{\alpha}+1\right)^{2}}{\hat{\alpha}\left(\hat{\alpha}+2\right)}=c\implies\hat{\alpha}^{2}+2\hat{\alpha}+1=c\hat{\alpha}\left(\hat{\alpha}+2\right)=c\hat{\alpha}^{2}+2c\hat{\alpha}\implies\hat{\alpha}^{2}\left(1-c\right)+\hat{\alpha}\left(2-2c\right)+1=0.
\]

\end_inset

Then, the quadratic formula gives
\begin_inset Formula 
\begin{flalign*}
x & =\frac{-b\pm\sqrt{b^{2}-4ac}}{2a}\\
\hat{\alpha} & =\frac{-\left(2-2c\right)\pm\sqrt{\left(2-2c\right)^{2}-4\left(1-c\right)\cdot1}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{\left(2\left(1-c\right)\right)^{2}-4\left(1-c\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{4\left(1-c\right)^{2}-4\left(1-c\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm\sqrt{4\left(\left(1-c\right)^{2}-\left(1-c\right)\right)}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm2\sqrt{1-2c+c^{2}-1+c}}{2\left(1-c\right)}\\
 & =\frac{2\left(c-1\right)\pm2\sqrt{c^{2}-c}}{2\left(1-c\right)}\\
 & =\frac{c-1\pm\sqrt{c^{2}-c}}{1-c},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\[
\hat{\beta}=\frac{\bar{x}\left(\hat{\alpha}+1\right)}{\hat{\alpha}}=\frac{\bar{x}\hat{\alpha}+\bar{x}}{\hat{\alpha}}=\bar{x}+\frac{\bar{x}}{\frac{c-1\pm\sqrt{c^{2}-c}}{1-c}}=\bar{x}\left(1+\frac{1-c}{c-1\pm\sqrt{c^{2}-c}}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
Method of moments estimators may not be uniquely defined, as can be seen
 in the following example.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson MOM estimator]
\end_layout

\end_inset

Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\text{Poisson}\left(\lambda\right)$
\end_inset

.
 Recall that 
\begin_inset Formula $\E\left[X\right]=\Var\left(X\right)=\lambda$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
\E\left[X\right]=\hat{\mu}_{1}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\implies\hat{\lambda}=\bar{x}
\]

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
\Var\left(X\right) & =\E\left[X^{2}\right]-\E\left[X\right]^{2}\\
\Leftrightarrow\hat{\lambda} & =\hat{\mu}_{2}-\hat{\mu}_{1}^{2}\\
 & =\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}\right)-\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\bar{X}^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\frac{n\bar{X}^{2}}{n}\\
 & =\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}^{2}-n\bar{X}^{2}\right)\\
 & =\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2},
\end{flalign*}

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:computing-sums-rand-samples"

\end_inset

.
 
\end_layout

\begin_layout Standard
Standard method of moments may not work.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim f\left(x|\theta\right)=\theta x^{-2}$
\end_inset

, 
\begin_inset Formula $0<\theta\leq x<\infty$
\end_inset

.
 Then,
\begin_inset Formula 
\begin{flalign*}
\E_{\theta}\left[X\right] & =\int_{\theta}^{\infty}x\frac{\theta}{x^{2}}\dif x\\
 & =\theta\int_{\theta}^{\infty}\frac{1}{x}\dif x\\
 & =\theta\lim_{c\rightarrow\infty}\int_{\theta}^{c}\frac{1}{x}\dif x\\
 & =\theta\lim_{c\rightarrow\infty}\left(\log x\Big\rvert_{\theta}^{c}\right)\\
 & =\theta\lim_{c\rightarrow\infty}\left(\log c-\log\theta\right)\\
 & =\infty,
\end{flalign*}

\end_inset

so that 
\begin_inset Formula $\mu_{1}\left(\theta\right)=\hat{\mu}_{1}$
\end_inset

 has no solution.
 If we consider
\begin_inset Formula 
\begin{flalign*}
\E_{\theta}\left[\frac{1}{X}\right] & =\int_{\theta}^{\infty}\frac{\theta}{x^{3}}\dif x\\
 & =\lim_{c\rightarrow\infty}\int_{\theta}^{c}\frac{\theta}{x^{3}}\dif x\\
 & =\lim_{c\rightarrow\infty}\left(-\theta\frac{1}{2x^{2}}\Bigr\rvert_{\theta}^{c}\right)\\
 & =\lim_{c\rightarrow\infty}\left(-\theta\frac{1}{2c^{2}}-\left(-\theta\frac{1}{2\theta^{2}}\right)\right)\\
 & =0+\frac{1}{2\theta}\\
 & =\frac{1}{2\theta},
\end{flalign*}

\end_inset

then setting 
\begin_inset Formula $u_{-1}\left(\theta\right)=\hat{\mu}_{-1}=\left(1/n\right)\sum_{i=1}^{n}1/X_{i}$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{1}{2\hat{\theta}}=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{X_{i}}\implies\hat{\theta}=\frac{n}{2\sum_{i=1}^{n}\frac{1}{X_{i}}}.
\]

\end_inset

If we consider instead
\begin_inset Formula 
\begin{flalign*}
\E_{\theta}\left[X^{1/2}\right] & =\int_{\theta}^{\infty}x^{1/2}\frac{\theta}{x^{2}}\dif x\\
 & =\lim_{c\rightarrow\infty}\left[\theta\int_{\theta}^{c}x^{-3/2}\dif x\right]\\
 & =\lim_{c\rightarrow\infty}\left[\theta\left(-2x^{-1/2}\Bigr\rvert_{\theta}^{c}\right)\right]\\
 & =\lim_{c\rightarrow\infty}\left[\theta\left(-2c^{-1/2}-\left(-2\theta^{-1/2}\right)\right)\right]\\
 & =\theta\left(0+2\theta^{-1/2}\right)\\
 & =2\sqrt{\theta},
\end{flalign*}

\end_inset

then setting 
\begin_inset Formula $\mu_{1/2}\left(\theta\right)=\hat{\mu}_{1/2}$
\end_inset

 gives
\begin_inset Formula 
\[
2\sqrt{\theta}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{1/2}\implies\hat{\theta}=\frac{\left(\sum_{i=1}^{n}X_{i}^{1/2}\right)^{2}}{4n^{2}}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood estimators
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are independent random variables from 
\begin_inset Formula $f_{i}\left(x|\theta_{1},\ldots,\theta_{k}\right)$
\end_inset

, then the likelihood function is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=\mathcal{L}\left(\theta_{1},\ldots,\theta_{k}|x_{1},\ldots,x_{n}\right)=\prod_{i=1}^{n}f_{i}\left(x_{i}|\theta_{1},\ldots,\theta_{k}\right)=f\left(\mathbf{x}|\theta\right),
\]

\end_inset

i.e., the likelihood function is just the joint density of the data, except
 that we treat it as a function of the parameter 
\begin_inset Formula $\theta$
\end_inset

.
 The likelihood function is not a density function, i.e., it does not integrate
 to 1.
 If 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a discrete random vector, then 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)=P_{\theta}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)$
\end_inset

.
 If we compare the likelihood function at two parameter points and find
 that
\begin_inset Formula 
\[
P_{\theta_{1}}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right)=\mathcal{L}\left(\theta_{1}|\mathbf{x}\right)>\mathcal{L}\left(\theta_{2}|\mathbf{x}\right)=P_{\theta_{2}}\left(\left\{ \mathbf{X}=\mathbf{x}\right\} \right),
\]

\end_inset

then the sample we actually observed is more likely to have occurred if
 
\begin_inset Formula $\theta=\theta_{1}$
\end_inset

 than if 
\begin_inset Formula $\theta=\theta_{2}$
\end_inset

, which can be interpreted as saying that 
\begin_inset Formula $\theta_{1}$
\end_inset

 is a more plausible value for the true value of 
\begin_inset Formula $\theta$
\end_inset

 than is 
\begin_inset Formula $\theta_{2}$
\end_inset

.
\end_layout

\begin_layout Definition
For each sample point 
\begin_inset Formula $\mathbf{x}$
\end_inset

, let 
\begin_inset Formula $\hat{\theta}\left(\mathbf{x}\right)$
\end_inset

 be a parameter value at which 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 attains its maximum as a function of 
\begin_inset Formula $\theta$
\end_inset

, with 
\begin_inset Formula $\mathbf{x}$
\end_inset

 held fixed.
 A 
\shape italic
maximum likelihood estimator
\shape default
 (MLE) of the parameter 
\begin_inset Formula $\theta$
\end_inset

 based on a sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is 
\begin_inset Formula $\hat{\theta}\left(\mathbf{X}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The MLE is the parameter point for which the observed sample is most likely.
 Let 
\begin_inset Formula $\mathcal{L}\left(\hat{\theta}|X\right)$
\end_inset

 be the maximum of all likelihood functions evaluated at 
\begin_inset Formula $\theta$
\end_inset

, i.e.,
\begin_inset Formula 
\[
\mathcal{L}\left(\hat{\theta}|X\right)=\max\left\{ \mathcal{L}\left(\theta|X\right):\theta\in\Theta\right\} \Leftrightarrow\hat{\theta}\text{ is an MLE}.
\]

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-generic, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h', fig.cap='maximizing the likelihood function'>>=
\end_layout

\begin_layout Plain Layout

x <- seq(0, 2, length = 1000)
\end_layout

\begin_layout Plain Layout

par(mgp = c(0.2,1,0), mar = c(1,2,0.1,2))
\end_layout

\begin_layout Plain Layout

plot(x, (-x^2)+2*x, type = "l", ylab = expression(paste("L(", lambda, ")")),
 xlab = expression(lambda), xaxt = "n", yaxt = "n", cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

points(x = 1, y = 1, pch = 19)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
If the likelihood is differentiable in 
\begin_inset Formula $\theta$
\end_inset

, possible candidates for 
\begin_inset Formula $\hat{\theta}$
\end_inset

 are those that solve
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log\mathcal{L}\left(\theta|X\right) & =0,
\end{flalign*}

\end_inset

which gives solutions that find interior extrema, some of which may be minima.
 To check whether a solution is a maximum, verify that 
\begin_inset Formula 
\begin{flalign*}
\dfrac{\partial^{2}}{\partial\theta^{2}}\log\mathcal{L}\left(\theta|X\right) & <0,
\end{flalign*}

\end_inset

i.e., concave.
 It is often easier to work with 
\begin_inset Formula $\log\mathcal{L}\left(\theta|X\right)$
\end_inset

 than with 
\begin_inset Formula $\mathcal{L}\left(\theta|X\right)$
\end_inset

, and whatever maximizes the likelihood will also maximize log-likelihood.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Exponential MLE]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:mle-exp-rv"

\end_inset

Let 
\begin_inset Formula $X\sim\text{Exp}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 be a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, so that the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are iid.
 Find the MLE of 
\begin_inset Formula $\lambda$
\end_inset

 and compare it to the MOM estimator.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\lambda\right) & =\frac{1}{\lambda}\mathrm{e}^{-x/\lambda}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $x\geq0$
\end_inset

, so that the pdf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)=\prod_{i=1}^{n}f_{X}\left(x_{i}|\lambda\right)=\prod_{i=1}^{n}\frac{1}{\lambda}\mathrm{e}^{-x_{i}/\lambda}=\frac{1}{\lambda^{n}}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\lambda}.
\]

\end_inset

Then, we have 
\begin_inset Formula $\mathcal{L}\left(\lambda|\mathbf{x}\right)=f_{\mathbf{X}}\left(\mathbf{x}|\lambda\right)$
\end_inset

, so that 
\begin_inset Formula 
\begin{flalign*}
\log\mathcal{L}\left(\lambda|\mathbf{x}\right) & =\log\left(\frac{1}{\lambda^{n}}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\lambda}\right)\\
 & =\log\left(\frac{1}{\lambda^{n}}\right)+\log\left(\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\lambda}\right)\\
 & =-n\log\lambda-\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

We will take the derivative of 
\begin_inset Formula $\log\mathcal{L}\left(\lambda|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\[
\frac{\partial}{\partial\lambda}\log\mathcal{L}\left(\lambda|\mathbf{x}\right)=\frac{\partial}{\partial\lambda}\left[-n\log\lambda-\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}\right]=-\frac{n}{\lambda}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{n}{\lambda}=\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\implies n\lambda^{2}=\lambda\sum_{i=1}^{n}x_{i}\implies\hat{\lambda}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\bar{x}.
\]

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $\log\mathcal{L}\left(\lambda|\mathbf{x}\right)$
\end_inset

 at 
\begin_inset Formula $\lambda=\hat{\lambda}$
\end_inset

 to verify that this is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\lambda^{2}}\log\mathcal{L}\left(\lambda|\mathbf{x}\right)\Bigr\rvert_{\lambda=\hat{\lambda}} & =\frac{\partial^{2}}{\partial\lambda^{2}}\left[-\frac{n}{\lambda}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\right|_{\lambda=\hat{\lambda}}\\
 & =\frac{n}{\lambda^{2}}-\frac{2}{\lambda^{3}}\sum_{i=1}^{n}x_{i}\Bigr\rvert_{\lambda=\hat{\lambda}}\\
 & =\frac{n}{\bar{x}^{2}}-\frac{2}{\bar{x}^{3}}\cdot n\bar{x}\\
 & =\frac{n}{\bar{x}^{2}}-\frac{2n}{\bar{x}^{2}}\\
 & =-\frac{n}{\bar{x}^{2}}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\bar{x}^{2}>0$
\end_inset

, so it follows that 
\begin_inset Formula $-n/\bar{x}^{2}<0$
\end_inset

, therefore 
\begin_inset Formula $\hat{\lambda}=\bar{X}$
\end_inset

 is the MLE.
 In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mom-exp-rv"

\end_inset

, we found that 
\begin_inset Formula $\hat{\lambda}_{MOM}=\bar{x}$
\end_inset

, so the two estimators agree.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Let's reconsider 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mom-sampling-replacement"

\end_inset

, where we found that the pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 was given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }I_{\left\{ x_{\left(n\right)}\leq\theta\right\} }.
\]

\end_inset

Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
We have
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=\begin{cases}
\frac{1}{\theta^{n}}I_{\left\{ x_{\left(1\right)\geq1}\right\} }, & \theta\geq x_{\left(n\right)}\\
0, & \theta<x_{\left(n\right)}
\end{cases},
\]

\end_inset

whose graph is shown below.
 Clearly, the maximum value of 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 occurs at 
\begin_inset Formula $x_{\left(n\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}=X_{\left(n\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-sampling, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(2, 4, length=2000) 
\end_layout

\begin_layout Plain Layout

plot(z, 1/(z^10), type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(1, 4), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=c(1,2), labels=c(expression(x[(1)]), expression(x[(n)])), cex.axis
 = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Remark*
When the support of the likelihood function depends on the parameter, it
 may not be necessary to take the derivative of 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

, e.g., the MLE may be found graphically.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Multinomial MLE]
\end_layout

\end_inset

Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are sampled from a multinomial distribution with 3 categories and probabilities
, e.g., the genotypes 
\begin_inset Formula $AA$
\end_inset

, 
\begin_inset Formula $Aa$
\end_inset

, and 
\begin_inset Formula $aa$
\end_inset

, so that 
\begin_inset Formula $X_{i}$
\end_inset

 represents the category of the 
\begin_inset Formula $i\mbox{th}$
\end_inset

 observation.
 Suppose that the pmf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X_{i}=1|\theta\right\} \right) & =\theta^{2}\\
P\left(\left\{ X_{i}=2|\theta\right\} \right) & =2\theta\left(1-\theta\right)\\
P\left(\left\{ X_{i}=3|\theta\right\} \right) & =\left(1-\theta\right)^{2}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\theta\in\left(0,1\right)$
\end_inset

.
 We observe 
\begin_inset Formula $n_{k}=\sum_{i=1}^{n}I_{\left\{ X_{i}=k\right\} }$
\end_inset

 individuals of type 
\begin_inset Formula $k\in\left\{ 1,2,3\right\} $
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
Note that the pmf specified for 
\begin_inset Formula $X_{i}$
\end_inset

 is legitimate, i.e., is it non-negative and
\begin_inset Formula 
\[
\theta^{2}+2\theta\left(1-\theta\right)+\left(1-\theta\right)^{2}=\theta^{2}+2\theta-2\theta^{2}+1-2\theta+\theta^{2}=1.
\]

\end_inset

 We can write the pmf as
\begin_inset Formula 
\begin{flalign*}
p_{X_{i}}\left(x_{i}|\theta\right) & =\left(\theta^{2}\right)^{I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{I_{\left\{ x_{i}=3\right\} }}
\end{flalign*}

\end_inset

where each 
\begin_inset Formula $I_{\left\{ x_{i}=k\right\} }$
\end_inset

 is equal to 0 or 1.
 Then, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{i},\ldots,X_{n}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}p_{X_{i}}\left(x_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\left(\theta^{2}\right)^{I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{I_{\left\{ x_{i}=3\right\} }}\\
 & =\left(\theta^{2}\right)^{\sum_{i=1}^{n}I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=3\right\} }}
\end{flalign*}

\end_inset

so that the likelihood function is given by 
\begin_inset Formula 
\begin{flalign*}
\mathcal{L}\left(\theta|\mathbf{x}\right) & =p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\\
 & =\left(\theta^{2}\right)^{\sum_{i=1}^{n}I_{\left\{ x_{i}=1\right\} }}\left[2\theta\left(1-\theta\right)\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=2\right\} }}\left[\left(1-\theta\right)^{2}\right]^{\sum_{i=1}^{n}I_{\left\{ x_{i}=3\right\} }}\\
 & =\left(\theta^{2}\right)^{n_{1}}\left[2\theta\left(1-\theta\right)\right]^{n_{2}}\left[\left(1-\theta\right)^{2}\right]^{n_{3}}\\
 & =\theta^{2n_{1}}\left[2\theta\left(1-\theta\right)\right]^{n_{2}}\left(1-\theta\right)^{2n_{3}}\\
 & =2^{n_{2}}\theta^{2n_{1}+n_{2}}\left(1-\theta\right)^{n_{2}+2n_{3}}\\
\Leftrightarrow\log\mathcal{L}\left(\theta|\mathbf{x}\right) & =\log\left[2^{n_{2}}\theta^{2n_{1}+n_{2}}\left(1-\theta\right)^{n_{2}+2n_{3}}\right]\\
 & =\log2^{n_{2}}+\log\theta^{2n_{1}+n_{2}}+\log\left(1-\theta\right)^{n_{2}+2n_{3}}\\
 & =n_{2}\log2+\left(2n_{1}+n_{2}\right)\log\theta+\left(n_{2}+2n_{3}\right)\log\left(1-\theta\right).
\end{flalign*}

\end_inset

We will take the derivative of the log-likelihood with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log\mathcal{L}\left(\theta|\mathbf{x}\right) & =\frac{\partial}{\partial\theta}\left[n_{2}\log2+\left(2n_{1}+n_{2}\right)\log\theta+\left(n_{2}+2n_{3}\right)\log\left(1-\theta\right)\right]\\
 & =0+\frac{2n_{1}+n_{2}}{\theta}+\frac{n_{2}+2n_{3}}{1-\theta}\cdot-1\\
 & =\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}+2n_{3}}{1-\theta}
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}+2n_{3}}{1-\theta} & =0\\
\Leftrightarrow\frac{n_{2}+2n_{3}}{1-\theta} & =\frac{2n_{1}+n_{2}}{\theta}\\
\Leftrightarrow\theta\left(n_{2}+2n_{3}\right) & =\left(1-\theta\right)\left(2n_{1}+n_{2}\right)\\
\Leftrightarrow\frac{1-\theta}{\theta} & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}\\
\Leftrightarrow\frac{1}{\theta}-1 & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}\\
\Leftrightarrow\frac{1}{\theta} & =\frac{n_{2}+2n_{3}}{2n_{1}+n_{2}}+1\\
 & =\frac{n_{2}+2n_{3}+2n_{1}+n_{2}}{2n_{1}+n_{2}}\\
 & =\frac{2\left(n_{1}+n_{2}+n_{3}\right)}{2n_{1}+n_{2}}\\
 & =\frac{2n}{2n_{1}+n_{2}}\\
\Leftrightarrow\hat{\theta} & =\frac{2n_{1}+n_{2}}{2n}.
\end{flalign*}

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\theta^{2}}\log\mathcal{L}\left(\theta|\mathbf{x}\right)\Bigr\rvert_{\theta=\hat{\theta}} & =\frac{\partial}{\partial\theta}\left[\frac{2n_{1}+n_{2}}{\theta}-\frac{n_{2}-2n_{3}}{1-\theta}\right|_{\theta=\hat{\theta}}\\
 & =-\frac{2n_{1}+n_{2}}{\theta^{2}}-\left(\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\cdot-1\cdot-1\right)\\
 & =-\frac{2n_{1}+n_{2}}{\theta^{2}}-\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\\
 & =-\left[\frac{2n_{1}+n_{2}}{\theta^{2}}+\frac{n_{2}-2n_{3}}{\left(1-\theta\right)^{2}}\right]
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n_{k}\geq0$
\end_inset

 and 
\begin_inset Formula $n>0$
\end_inset

, i.e., at least one of 
\begin_inset Formula $n_{k}$
\end_inset

 is positive, and we have 
\begin_inset Formula $\theta^{2}>0$
\end_inset

 and 
\begin_inset Formula $\left(1-\theta\right)^{2}>0$
\end_inset

, so that the sum above is positive, so that the expression above is negative.
 It follows that 
\begin_inset Formula $\hat{\theta}=\left(2n_{1}+n_{2}\right)/2n$
\end_inset

 is the MLE.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson MLE]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:mle-poisson"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\text{iid}}{\sim}\text{Poisson}\left(\lambda\right)$
\end_inset

, where 
\begin_inset Formula $\lambda>0$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-poisson"

\end_inset

, we found that the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 was given by
\begin_inset Formula 
\begin{flalign*}
p_{\mathbf{X}}\left(\mathbf{x}|\lambda\right) & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}},
\end{flalign*}

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log\mathcal{L}\left(\lambda|\mathbf{x}\right) & =\log\left[\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-n\lambda}\lambda^{\sum_{i=1}^{n}x_{i}}\right]\\
 & =\log\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)+\log\mathrm{e}^{-n\lambda}+\log\lambda^{\sum_{i=1}^{n}x_{i}}\\
 & =\left(\log\frac{1}{x_{1}!}+\cdots+\log\frac{1}{x_{n}!}\right)-n\lambda+\left(\sum_{i=1}^{n}x_{i}\right)\log\lambda\\
 & =\left[\left(\log1-\log x_{1}!\right)+\cdots+\left(\log1-\log x_{n}!\right)\right]-n\lambda+\left(\sum_{i=1}^{n}x_{i}\right)\log\lambda\\
 & =\left[\left(0-\log x_{1}!\right)+\cdots+\left(0-\log x_{n}!\right)\right]-n\lambda+\log\lambda\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =\sum_{i=1}^{n}-\log x_{i}!-n\lambda+\log\lambda\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =-n\lambda+\log\lambda\left(\sum_{i=1}^{n}x_{i}\right)-\sum_{i=1}^{n}\log x_{i}!.
\end{flalign*}

\end_inset

We will take the derivative of the log-likelihood with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\[
\frac{\partial}{\partial\lambda}\log\mathcal{L}\left(\lambda|\mathbf{x}\right)=\frac{\partial}{\partial\lambda}\left[-n\lambda+\log\lambda\left(\sum_{i=1}^{n}x_{i}\right)-\sum_{i=1}^{n}\log x_{i}!\right]=-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}=0\implies\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}=n\implies\hat{\lambda}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\bar{x}.
\]

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\lambda=\hat{\lambda}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\lambda}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\lambda^{2}}\log\mathcal{L}\left(\lambda|\mathbf{x}\right)\Bigr\rvert_{\lambda=\hat{\lambda}} & =\frac{\partial}{\partial\lambda}\left[-n+\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}\right|_{\lambda=\hat{\lambda}}\\
 & =-\frac{1}{\lambda^{2}}\sum_{i=1}^{n}x_{i}\Bigr\rvert_{\lambda=\hat{\lambda}}\\
 & =-\frac{1}{\bar{x}^{2}}\sum_{i=1}^{n}x_{i}\\
 & =-\frac{1}{\bar{x}^{2}}n\bar{x}\\
 & =-\frac{n}{\bar{x}}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $\bar{x}\geq0$
\end_inset

 (because the 
\begin_inset Formula $x_{i}\mbox{'s}$
\end_inset

 are each non-negative), so that 
\begin_inset Formula $-n/\bar{x}<0$
\end_inset

 (assuming that 
\begin_inset Formula $\bar{x}>0$
\end_inset

).
 It follows that 
\begin_inset Formula $\hat{\lambda}=\bar{x}$
\end_inset

 is the MLE.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform MLE, single-parameter case]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\overset{\text{iid}}{\sim}\mathcal{U}\left(\theta-\frac{1}{2},\theta+\frac{1}{2}\right)$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x_{i}\right) & =\frac{1}{\left(\theta+\frac{1}{2}\right)-\left(\theta-\frac{1}{2}\right)}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =\frac{1}{1}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} },
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\prod_{i=1}^{n}I_{\left\{ \theta-\frac{1}{2}<x_{i}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ \theta-\frac{1}{2}<x_{1}<\theta+\frac{1}{2}\right\} }\cdot\ldots\cdot I_{\left\{ \theta-\frac{1}{2}<x_{n}<\theta+\frac{1}{2}\right\} }\\
 & =I_{\left\{ x_{\left(1\right)}>\theta-\frac{1}{2}\right\} }I_{\left\{ x_{\left(n\right)}<\theta+\frac{1}{2}\right\} }.
\end{flalign*}

\end_inset

Then, the likelihood function is given by 
\begin_inset Formula 
\begin{flalign*}
\mathcal{L}\left(\theta|\mathbf{x}\right) & =\begin{cases}
1, & x_{\left(n\right)}-\frac{1}{2}<\theta<x_{\left(1\right)}+\frac{1}{2}\\
0, & \mbox{otherwise}
\end{cases},
\end{flalign*}

\end_inset

and its graph is shown below.
 Clearly, 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 is constant between 
\begin_inset Formula $x_{\left(n\right)}-\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula $x_{\left(1\right)}+\frac{1}{2}$
\end_inset

 and 0 elsewhere, so that the MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is given by any point in the interval 
\begin_inset Formula $\left(x_{\left(n\right)}-\frac{1}{2},x_{\left(1\right)}+\frac{1}{2}\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<likelihood-uniform, echo=FALSE, fig.height=2, fig.width=3, fig.align='center',
 fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,1,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(1, 2, length=1000) 
\end_layout

\begin_layout Plain Layout

plot(z, rep(1,1000), type="l", xlab = expression(theta), ylab=expression(paste(L
, "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(0, 3), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=c(1,2), labels=c(expression(x[(n)]-frac(1,2)), expression(x[(1)]+frac
(1,2))), cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

axis(2, at=1, labels=1, cex.axis=0.75, las=2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots X_{n}\stackrel{\text{iid}}{\sim}f\left(x|\theta\right)=\theta/x^{2}$
\end_inset

, 
\begin_inset Formula $0<\theta\leq x<\infty$
\end_inset

.
 Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
The support of 
\begin_inset Formula $X$
\end_inset

 depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, so we will rewrite the pdf of 
\begin_inset Formula $X$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f_{X}\left(x|\theta\right) & =\frac{\theta}{x^{2}}I_{\left\{ x\geq\theta\right\} }.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\frac{\theta}{x_{i}^{2}}I_{\left\{ x_{i}\geq\theta\right\} }=\frac{\theta^{n}}{\prod_{i=1}^{n}x_{i}^{2}}I_{\left\{ x_{\left(1\right)}\geq\theta\right\} },
\]

\end_inset

so that the likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
\mathcal{L}\left(\theta|\mathbf{x}\right) & =\begin{cases}
\theta^{n}/\prod_{i=1}^{n}x_{i}^{2}, & \theta\leq x_{\left(1\right)}\\
0, & \theta>x_{\left(1\right)}
\end{cases}.
\end{flalign*}

\end_inset

The graph of the likelihood function is shown below.
 Clearly, the maximum value of 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 occurs at 
\begin_inset Formula $x_{\left(1\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}=X_{\left(1\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(0, 1, length=1000) 
\end_layout

\begin_layout Plain Layout

plot(z, z^3, type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(x),")")), yaxt="n", xaxt="n", xlim = c(0, 2), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at = 1, labels = expression(x[(1)]), cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Uniform MLE, two-parameter case]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\text{iid}}{\sim}\mathcal{U}\left(\mu-\sqrt{3}\sigma,\mu+\sqrt{3}\sigma\right)$
\end_inset

.
 Find the MLEs of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

.
\end_layout

\begin_layout Example
The pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{i}}\left(x_{i}|\mu,\sigma\right) & =\frac{1}{\left(\mu+\sqrt{3}\sigma\right)-\left(\mu-\sqrt{3}\sigma\right)}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} }\\
 & =\frac{1}{2\sqrt{3}\sigma}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} },
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\mu,\sigma\right) & =\prod_{i=1}^{n}f_{X_{i}}\left(x_{i}|\mu,\sigma\right)\\
 & =\prod_{i=1}^{n}\frac{1}{2\sqrt{3}\sigma}I_{\left\{ \mu-\sqrt{3}\sigma<x_{i}<\mu+\sqrt{3}\sigma\right\} }\\
 & =\left(2\sqrt{3}\sigma\right)^{-n}I_{\left\{ x_{\left(1\right)}>\mu-\sqrt{3}\sigma\right\} }I_{\left\{ x_{\left(n\right)}<\mu+\sqrt{3}\sigma\right\} }.
\end{flalign*}

\end_inset

To maximize 
\begin_inset Formula $\mathcal{L}$
\end_inset

, we note that 
\begin_inset Formula $\left(2\sqrt{3}\sigma\right)^{-n}$
\end_inset

 increases as 
\begin_inset Formula $\sigma$
\end_inset

 decreases.
 Thus, we must find the minimum value of 
\begin_inset Formula $\sigma$
\end_inset

 such that 
\begin_inset Formula $\mathcal{L}$
\end_inset

 is positive, which is true when 
\begin_inset Formula $\sigma>\left(\mu-x_{\left(1\right)}\right)/\sqrt{3}$
\end_inset

 and 
\begin_inset Formula $\sigma>\left(x_{\left(n\right)}-\mu\right)/\sqrt{3}$
\end_inset

.
 In the graph below, the region of positivity of 
\begin_inset Formula $\mathcal{L}$
\end_inset

 is shaded.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3.5, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,4,0.1,4))
\end_layout

\begin_layout Plain Layout

z <- seq(-1, 3, length=2000) 
\end_layout

\begin_layout Plain Layout

# set yaxs="i" to remove the default 4% padding beyond the ylim
\end_layout

\begin_layout Plain Layout

plot(z, (2-z)/sqrt(3), type = "l", xlab = expression(mu), ylab = expression(sigm
a), yaxt = "n", xaxt = "n", yaxs = "i", xlim = c(0, 3), ylim = c(0,1.5),
 cex.lab = 0.75)
\end_layout

\begin_layout Plain Layout

lines(z, (z-1)/sqrt(3), type="l")
\end_layout

\begin_layout Plain Layout

polygon(c(-1, 1.5, 4), c(3/sqrt(3), 1/(2*sqrt(3)), 3/sqrt(3)), density=20,
 angle=0)
\end_layout

\begin_layout Plain Layout

axis(1, at = c(1,2), labels = c(expression(x[(1)]), expression(x[(n)])),
 cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

# use las to rotate the strings and padj to move them along the axis
\end_layout

\begin_layout Plain Layout

mtext(expression(sigma == frac(mu-x[(1)], sqrt(3))), side=4, line=0.5, las=2,
 padj=-1, cex=0.75)
\end_layout

\begin_layout Plain Layout

mtext(expression(sigma == frac(x[(n)]-mu, sqrt(3))), side=2, line=0.5, las=2,
 padj=-1, cex=0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
The minimum value of 
\begin_inset Formula $\sigma$
\end_inset

 such that 
\begin_inset Formula $\mathcal{L}$
\end_inset

 is positive occurs precisely when the two lines intersect, i.e., when
\begin_inset Formula 
\[
\frac{\mu-x_{\left(1\right)}}{\sqrt{3}}=\frac{x_{\left(n\right)}-\mu}{\sqrt{3}}\implies\mu-x_{\left(1\right)}=x_{\left(n\right)}-\mu\implies2\mu=x_{\left(n\right)}+x_{\left(1\right)}\implies\hat{\mu}=\frac{x_{\left(n\right)}+x_{\left(1\right)}}{2}.
\]

\end_inset

We substitute into one of the line equations to find 
\begin_inset Formula $\hat{\sigma}$
\end_inset

.
\begin_inset Formula 
\[
\hat{\sigma}=\frac{\hat{\mu}-x_{\left(1\right)}}{\sqrt{3}}=\frac{\frac{1}{2}\left(x_{\left(n\right)}+x_{\left(1\right)}\right)-x_{\left(1\right)}}{\sqrt{3}}=\frac{x_{\left(n\right)}+x_{\left(1\right)}-2x_{\left(1\right)}}{2\sqrt{3}}=\frac{x_{\left(n\right)}-x_{\left(1\right)}}{2\sqrt{3}}
\]

\end_inset

Thus, the MLEs for 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 are given by 
\begin_inset Formula $\hat{\mu}=\left(x_{\left(n\right)}+x_{\left(1\right)}\right)/2$
\end_inset

 and 
\begin_inset Formula $\hat{\sigma}=\left(x_{\left(n\right)}-x_{\left(1\right)}\right)/2\sqrt{3}$
\end_inset

, respectively.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $X_{i}$
\end_inset

 and 
\begin_inset Formula $Y_{i}$
\end_inset

 are independent for 
\begin_inset Formula $i=1,\ldots,n$
\end_inset

, where 
\begin_inset Formula $X_{i}\sim f\left(x|\lambda\right)=\left(1/\lambda\right)e^{-x/\lambda}$
\end_inset

 and 
\begin_inset Formula $Y_{i}\sim f\left(y|\mu\right)=\left(1/\mu\right)e^{-y/\mu}$
\end_inset

.
 We observe 
\begin_inset Formula 
\[
Z_{i}=\min\left(X_{i},Y_{i}\right)\quad\mbox{and}\quad W_{i}=\begin{cases}
1, & \mbox{if }Z_{i}=X_{i}\\
0, & \mbox{if }Z_{i}=Y_{i}
\end{cases}.
\]

\end_inset

Find the MLEs of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
In the case where 
\begin_inset Formula $W_{i}=1$
\end_inset

, we have 
\begin_inset Formula $Z_{i}=\min\left(X_{i},Y_{i}\right)=X_{i}$
\end_inset

, so that our information about 
\begin_inset Formula $Y_{i}$
\end_inset

 is limited to 
\begin_inset Formula $Y_{i}>Z_{i}$
\end_inset

, and we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=1\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ X_{i}=z_{i}\right\} \cap\left\{ Y_{i}>z_{i}\right\} \right)\\
 & =P\left(\left\{ X_{i}=z_{i}\right\} \right)\cdot P\left(\left\{ Y_{i}>z_{i}\right\} \right)\\
 & =f_{X_{i}}\left(z_{i}|\lambda\right)\cdot\left(1-F_{Y_{i}}\left(z_{i}|\mu\right)\right).
\end{flalign*}

\end_inset

In the case where 
\begin_inset Formula $W_{i}=0$
\end_inset

, we have 
\begin_inset Formula $Z_{i}=\min\left(X_{i},Y_{i}\right)=Y_{i}$
\end_inset

, so that our information about 
\begin_inset Formula $X_{i}$
\end_inset

 is limited to 
\begin_inset Formula $X_{i}>Z_{i}$
\end_inset

, and we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=0\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ Y_{i}=z_{i}\right\} \cap\left\{ X_{i}>z_{i}\right\} \right)\\
 & =P\left(\left\{ Y_{i}=z_{i}\right\} \right)\cdot P\left(\left\{ X_{i}>z_{i}\right\} \right)\\
 & =f_{Y_{i}}\left(z_{i}|\mu\right)\cdot\left(1-F_{X_{i}}\left(z_{i}|\lambda\right)\right).
\end{flalign*}

\end_inset

The cdf of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{Y_{i}}\left(y|\mu\right)=\int_{0}^{y}f_{Y_{i}}\left(t|\mu\right)\mbox{d}t=\int_{0}^{y}\frac{1}{\mu}e^{-t/\mu}\mbox{d}t=\left[-e^{-t/\mu}\right|_{0}^{y}=-e^{-y/\mu}-\left(-e^{0}\right)=1-e^{-y/\mu}.
\]

\end_inset

By symmetry, the cdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula $F_{X_{i}}\left(x|\lambda\right)=1-e^{-x/\lambda}$
\end_inset

.
 So, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=1\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =\frac{1}{\lambda}e^{-z_{i}/\lambda}\cdot e^{-z_{i}/\mu}
\end{flalign*}

\end_inset

and 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ W_{i}=0\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =\frac{1}{\mu}e^{-z_{i}/\mu}\cdot e^{-z_{i}/\lambda}.
\end{flalign*}

\end_inset

Then, the joint pdf of 
\begin_inset Formula $W_{i}$
\end_inset

 and 
\begin_inset Formula $Z_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{W_{i},Z_{i}}\left(w_{i},z_{i}|\lambda,\mu\right) & =P\left(\left\{ Z_{i}=z_{i}\right\} \cap\left\{ W_{i}=w_{i}\right\} \right)\\
 & =\left[\frac{1}{\lambda}e^{-z_{i}/\lambda}e^{-z_{i}/\mu}\right]^{w_{i}}\left[\frac{1}{\mu}e^{-z_{i}/\mu}e^{-z_{i}/\lambda}\right]^{1-w_{i}}\\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}e^{\left(-z_{i}/\lambda\right)w_{i}}e^{\left(-z_{i}/\mu\right)w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{\left(-z_{i}/\mu\right)\left(1-w_{i}\right)}e^{\left(-z_{i}/\lambda\right)\left(1-w_{i}\right)}\\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -w_{i}\frac{z_{i}}{\lambda}-w_{i}\frac{z_{i}}{\mu}-\frac{z_{i}}{\mu}+w_{i}\frac{z_{i}}{\mu}-\frac{z_{i}}{\lambda}+w_{i}\frac{z_{i}}{\lambda}\right\} \\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -\frac{z_{i}}{\mu}-\frac{z_{i}}{\lambda}\right\} \\
 & =\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{-z_{i}/\mu}e^{-z_{i}/\lambda},
\end{flalign*}

\end_inset

so that the joint pdf of 
\begin_inset Formula $\mathbf{W}=W_{1},\ldots,W_{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Z}=Z_{1},\ldots,Z_{n}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{W},\mathbf{Z}}\left(\mathbf{w},\mathbf{z}|\lambda,\mu\right) & =\prod_{i=1}^{n}f_{W_{i},Z_{i}}\left(w_{i},z_{i}|\lambda,\mu\right)\\
 & =\prod_{i=1}^{n}\left[\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}e^{-z_{i}/\mu}e^{-z_{i}/\lambda}\right]\\
 & =\prod_{i=1}^{n}\left[\left(\frac{1}{\lambda}\right)^{w_{i}}\left(\frac{1}{\mu}\right)^{1-w_{i}}\exp\left\{ -z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} \right]\\
 & =\lambda^{-\sum_{i=1}^{n}w_{i}}\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}\exp\left\{ -\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} .
\end{flalign*}

\end_inset

Then, the log-likelihood is given by 
\begin_inset Formula 
\begin{flalign*}
\log L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\log\left[\lambda^{-\sum_{i=1}^{n}w_{i}}\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}\exp\left\{ -\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\right\} \right]\\
 & =\log\lambda^{-\sum_{i=1}^{n}w_{i}}+\log\mu^{-\sum_{i=1}^{n}\left(1-w_{i}\right)}-\sum_{i=1}^{n}z_{i}\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\\
 & =-\log\lambda\sum_{i=1}^{n}w_{i}-\log\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}.
\end{flalign*}

\end_inset

We will take the derivative of 
\begin_inset Formula $\log L$
\end_inset

 with respect to 
\begin_inset Formula $\lambda$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\lambda}\log L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\frac{\partial}{\partial\lambda}\left[-\log\lambda\sum_{i=1}^{n}w_{i}-\log\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}\right]\\
 & =-\frac{1}{\lambda}\sum_{i=1}^{n}w_{i}-0+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}z_{i}\\
 & =-\frac{1}{\lambda}\sum_{i=1}^{n}w_{i}+\frac{1}{\lambda^{2}}\sum_{i=1}^{n}z_{i}
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\hat{\lambda}^{2}}\sum_{i=1}^{n}z_{i} & =\frac{1}{\hat{\lambda}}\sum_{i=1}^{n}w_{i}\\
\Leftrightarrow\hat{\lambda}^{2}\sum_{i=1}^{n}w_{i} & =\hat{\lambda}\sum_{i=1}^{n}z_{i}\\
\Leftrightarrow\hat{\lambda} & =\frac{\sum_{i=1}^{n}z_{i}}{\sum_{i=1}^{n}w_{i}}.
\end{flalign*}

\end_inset

We will now take the derivative of 
\begin_inset Formula $\log L$
\end_inset

 with respect to 
\begin_inset Formula $\mu$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\mu}\log L\left(\lambda,\mu|\mathbf{w},\mathbf{z}\right) & =\frac{\partial}{\partial\mu}\left[-\log\lambda\sum_{i=1}^{n}w_{i}-\log\mu\sum_{i=1}^{n}\left(1-w_{i}\right)-\left(\frac{1}{\mu}+\frac{1}{\lambda}\right)\sum_{i=1}^{n}z_{i}\right]\\
 & =0-\frac{1}{\mu}\sum_{i=1}^{n}\left(1-w_{i}\right)+\frac{1}{\mu^{2}}\sum_{i=1}^{n}z_{i}\\
 & =-\frac{1}{\mu}\sum_{i=1}^{n}\left(1-w_{i}\right)+\frac{1}{\mu^{2}}\sum_{i=1}^{n}z_{i}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\hat{\mu}^{2}}\sum_{i=1}^{n}z_{i} & =\frac{1}{\hat{\mu}}\sum_{i=1}^{n}\left(1-w_{i}\right)\\
\Leftrightarrow\hat{\mu}^{2}\sum_{i=1}^{n}\left(1-w_{i}\right) & =\hat{\mu}\sum_{i=1}^{n}z_{i}\\
\Leftrightarrow\hat{\mu} & =\frac{\sum_{i=1}^{n}z_{i}}{\sum_{i=1}^{n}\left(1-w_{i}\right)}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Remark*
The above example is a common setting for survival (or failure) analysis.
 For example, 
\begin_inset Formula $X_{i}$
\end_inset

 might be the time of cure for the 
\begin_inset Formula $i\mbox{th}$
\end_inset

 subject, and 
\begin_inset Formula $Y_{i}$
\end_inset

 the follow-up time.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Invariance property of MLEs]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:invariance-mle"

\end_inset

If 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE of 
\begin_inset Formula $\theta$
\end_inset

, then for any function 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, the MLE of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 is 
\begin_inset Formula $\tau\left(\hat{\theta}\right)$
\end_inset

.
 (This is Theorem 7.2.10 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\hat{\eta}$
\end_inset

 denote the value the maximizes 
\begin_inset Formula $L^{*}\left(\eta|\mathbf{x}\right)$
\end_inset

.
 We must show that 
\begin_inset Formula $L^{*}\left(\hat{\eta}|\mathbf{x}\right)=L^{*}\left[\tau\left(\hat{\theta}\right)|\mathbf{x}\right]$
\end_inset

.
 Now, the maxima of 
\begin_inset Formula $L$
\end_inset

 and 
\begin_inset Formula $L^{*}$
\end_inset

 coincide, so we have
\begin_inset Formula 
\[
L^{*}\left(\hat{\eta}|\mathbf{x}\right)=\sup_{\eta}\sup_{\left\{ \theta:\tau\left(\theta\right)=\eta\right\} }L\left(\theta|\mathbf{x}\right)=\sup_{\theta}L\left(\theta|\mathbf{x}\right)=L\left(\hat{\theta}|\mathbf{x}\right),
\]

\end_inset

where the second equality follows because the iterated maximization is equal
 to the unconditional maximization over 
\begin_inset Formula $\theta$
\end_inset

, which is attained at 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 Furthermore
\begin_inset Formula 
\[
L\left(\hat{\theta}|\mathbf{x}\right)=\sup_{\left\{ \theta:\tau\left(\theta\right)=\tau\left(\hat{\theta}\right)\right\} }L\left(\theta|\mathbf{x}\right)=L^{*}\left[\tau\left(\hat{\theta}\right)|\mathbf{x}\right].
\]

\end_inset

Hence the string of equalities shows that 
\begin_inset Formula $L^{*}\left(\hat{\eta}|\mathbf{x}\right)=L^{*}\left(\tau\left(\hat{\theta}\right)|\mathbf{x}\right)$
\end_inset

 and that 
\begin_inset Formula $\tau\left(\hat{\theta}\right)$
\end_inset

 is the MLE of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $X_{1},\ldots,X_{n}\stackrel{\text{iid}}{\sim}\mbox{Exp}\left(\lambda\right)$
\end_inset

, so that the pdf of 
\begin_inset Formula $X_{i}$
\end_inset

 is given by 
\begin_inset Formula $f_{X_{i}}\left(x|\lambda\right)=\lambda\mathrm{e}^{-\lambda x}$
\end_inset

, and the MLE of 
\begin_inset Formula $\lambda$
\end_inset

 is 
\begin_inset Formula $\hat{\lambda}=1/\bar{X}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $P\left(\left\{ X>a\right\} \right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Let 
\begin_inset Formula $\eta=P\left(\left\{ X>a\right\} \right)$
\end_inset

, so that we have 
\begin_inset Formula 
\begin{flalign*}
\eta & =P\left(\left\{ X>a\right\} \right)\\
 & =\int_{a}^{\infty}f_{X}\left(x|\lambda\right)\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\int_{a}^{c}\lambda\mathrm{e}^{-\lambda x}\mbox{d}x\\
 & =\lim_{c\rightarrow\infty}\left[-\mathrm{e}^{-\lambda x}\right|_{a}^{c}\\
 & =\lim_{c\rightarrow\infty}\left(-\mathrm{e}^{-\lambda c}-\left(-\mathrm{e}^{-\lambda a}\right)\right)\\
 & =\lim_{c\rightarrow\infty}\left(-\mathrm{e}^{-\lambda c}+\mathrm{e}^{-\lambda a}\right)\\
 & =0+\mathrm{e}^{-\lambda a}\\
 & =\mathrm{e}^{-\lambda a}.
\end{flalign*}

\end_inset

Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:invariance-mle"

\end_inset

, we have 
\begin_inset Formula $\hat{\eta}=\mathrm{e}^{-\left(1/\bar{x}\right)a}=\mathrm{e}^{-a/\bar{x}}$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
\log\eta=\log\mathrm{e}^{-\lambda a}\implies-\lambda a=\log\eta\implies\lambda=-\frac{\log\eta}{a},
\]

\end_inset

so that
\begin_inset Formula 
\[
f_{X_{i}}\left(x|\eta\right)=\left(-\frac{\log\eta}{a}\right)\mathrm{e}^{-\left(-\log\eta/a\right)x}=\left(-\frac{\log\eta}{a}\right)\mathrm{e}^{\left(x\log\eta\right)/a}.
\]

\end_inset

Then, the joint pdf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\eta\right)=\prod_{i=1}^{n}f_{X_{i}}\left(x|\eta\right)=\prod_{i=1}^{n}\left(-\frac{\log\eta}{a}\right)\mathrm{e}^{\left(x_{i}\log\eta\right)/a}=\left(-\frac{\log\eta}{a}\right)^{n}\mathrm{e}^{\sum_{i=1}^{n}\left(x_{i}\log\eta\right)/a},
\]

\end_inset

so that the log-likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
\log\mathcal{L}\left(\eta|\mathbf{x}\right) & =\log\left[\left(-\frac{\log\eta}{a}\right)^{n}\mathrm{e}^{\sum_{i=1}^{n}\left(x_{i}\log\eta\right)/a}\right]\\
 & =\log\left[\left(-\frac{\log\eta}{a}\right)^{n}\right]+\log\left[\mathrm{e}^{\sum_{i=1}^{n}\left(x_{i}\log\eta\right)/a}\right]\\
 & =n\log\left(-\frac{\log\eta}{a}\right)+\sum_{i=1}^{n}\left(\frac{x_{i}\log\eta}{a}\right)\\
 & =n\log\left(-\frac{\log\eta}{a}\right)+\frac{\log\eta}{a}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\eta$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\eta}\log\mathcal{L}\left(\eta|\mathbf{x}\right) & =\frac{\partial}{\partial\eta}\left[n\log\left(-\frac{\log\eta}{a}\right)+\log\eta\left(a\sum_{i=1}^{n}x_{i}\right)\right]\\
 & =\frac{n}{-\frac{\log\eta}{a}}\left(-\frac{1}{\eta a}\right)+\frac{1}{\eta a}\sum_{i=1}^{n}x_{i}\\
 & =\frac{n}{\eta\log\eta}+\frac{1}{\eta a}\sum_{i=1}^{n}x_{i}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{n}{\hat{\eta}\log\hat{\eta}}+\frac{1}{\hat{\eta}a}\sum_{i=1}^{n}x_{i} & =0\\
\implies\frac{n}{\hat{\eta}\log\hat{\eta}} & =-\frac{1}{\hat{\eta}a}\sum_{i=1}^{n}x_{i}\\
\implies\hat{\eta}\log\hat{\eta}\sum_{i=1}^{n}x_{i} & =-n\hat{\eta}a\\
\implies\log\hat{\eta} & =-\frac{na}{\sum_{i=1}^{n}x_{i}}\\
 & =-\frac{a}{\bar{x}}\\
\implies\hat{\eta} & =\mathrm{e}^{-a/\bar{x}},
\end{flalign*}

\end_inset

which agrees with the result obtained from the theorem.
\end_layout

\end_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $\mbox{median}\left(X_{1},\ldots,X_{n}\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The median of a distribution is a value 
\begin_inset Formula $m$
\end_inset

 such that 
\begin_inset Formula $P\left(\left\{ Y\leq m\right\} \right)\geq\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula $P\left(\left\{ Y\geq m\right\} \right)\geq\frac{1}{2}.$
\end_inset

 If 
\begin_inset Formula $Y$
\end_inset

 is continuous, 
\begin_inset Formula $m$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\int_{-\infty}^{m}f\left(y\right)\dif y=\int_{m}^{\infty}f\left(y\right)\dif y=\frac{1}{2}.
\]

\end_inset


\begin_inset Formula $X$
\end_inset

 is continuous, so it follows that 
\begin_inset Formula 
\[
\frac{1}{2}=P\left(\left\{ X\leq m\right\} \right)=F_{X}\left(m\right).
\]

\end_inset

The cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{X}\left(x\right)=\int_{0}^{x}f_{X}\left(x\right)\dif x=\int_{0}^{x}\lambda\mathrm{e}^{-\lambda x}\dif x=-\mathrm{e}^{-\lambda x}\Big\rvert_{0}^{x}=-\mathrm{e}^{-\lambda x}-\left(-\mathrm{e}^{0}\right)=1-\mathrm{e}^{-\lambda x},
\]

\end_inset

so it follows that 
\begin_inset Formula 
\[
F_{X}\left(m\right)=1-\mathrm{e}^{-\lambda m}=\frac{1}{2}\implies\mathrm{e}^{-\lambda m}=\frac{1}{2}\implies-\lambda m=\log\left(2^{-1}\right)=-\log2\implies m=\frac{\log2}{\lambda}.
\]

\end_inset

Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:invariance-mle"

\end_inset

, we have 
\begin_inset Formula 
\[
\hat{m}=\frac{\log2}{\hat{\lambda}}=\frac{\log2}{1/\bar{x}}=\bar{x}\log2.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Theorem
If an MLE is unique, then it is a function of the sufficient statistics.
\end_layout

\begin_layout Proof
Suppose that 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 is a random sample from the distribution of 
\begin_inset Formula $X$
\end_inset

, whose pdf is given by 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

.
 Suppose also that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE of 
\begin_inset Formula $\theta$
\end_inset

, and that the associated likelihood function is given by 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=f\left(\mathbf{x}|\theta\right)=h\left(\mathbf{x}\right)g\left(T\left(\mathbf{x}\right)|\theta\right),
\]

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

.
 Then, maximizing 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 is equivalent to maximizing 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

, which is a function of 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

, which is a sufficient statistic.
\end_layout

\begin_layout Standard
Often, it is not possible to find the MLE analytically and we need to use
 numerical methods.
 Two commonly used methods are the Newton-Raphson algorithm and the Expectation-
Maximization (EM) algorithm.
 Both are iterative methods that produce a sequence of values 
\begin_inset Formula $\theta^{\left(0\right)},\theta^{\left(1\right)},\ldots$
\end_inset

 that, under ideal conditions, converge to the MLE 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 It is helpful to use a good starting value 
\begin_inset Formula $\theta^{\left(0\right)}$
\end_inset

.
 Often, the method of moments estimator is a good starting value.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:newton-raphson"

\end_inset

Newton-Raphson
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\ell\left(\theta|\mathbf{x}\right)=\log\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

.
 When we maximize the log-likelihood by setting its derivative equal to
 zero, we are solving the equation
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\ell\left(\theta|\mathbf{x}\right) & =0.
\end{flalign*}

\end_inset

To motivate Newton-Raphson, we will expand the derivative of 
\begin_inset Formula $\ell\left(\theta|\mathbf{x}\right)$
\end_inset

 around 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

.
 Recall from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:taylor-polynomial"

\end_inset

 that the Taylor series expansion of 
\begin_inset Formula $f$
\end_inset

 around 
\begin_inset Formula $a$
\end_inset

 is given by 
\begin_inset Formula 
\[
f\left(x\right)=\sum_{n=0}^{\infty}\frac{f^{\left(n\right)}\left(a\right)}{n!}\left(x-a\right)^{n}=f\left(a\right)+f'\left(a\right)\left(x-a\right)+\frac{f''\left(a\right)}{2!}\left(x-a\right)^{2}+\frac{f'''\left(a\right)}{3!}\left(x-a\right)^{3}+\cdots,
\]

\end_inset

so that we have 
\begin_inset Formula 
\begin{flalign*}
\ell'\left(\hat{\theta}\right) & \approx\ell'\left(\theta^{\left(j\right)}\right)+\ell''\left(\theta^{\left(j\right)}\right)\left(\hat{\theta}-\theta^{\left(j\right)}\right).
\end{flalign*}

\end_inset

Setting 
\begin_inset Formula $\ell'$
\end_inset

 equal to zero and solving for 
\begin_inset Formula $\hat{\theta}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0 & =\ell'\left(\hat{\theta}\right)\\
 & \approx\ell'\left(\theta^{\left(j\right)}\right)+\ell''\left(\theta^{\left(j\right)}\right)\left(\hat{\theta}-\theta^{\left(j\right)}\right)\\
 & \approx\ell'\left(\theta^{\left(j\right)}\right)+\hat{\theta}\ell''\left(\theta^{\left(j\right)}\right)-\theta^{\left(j\right)}\ell''\left(\theta^{\left(j\right)}\right)\\
\implies\hat{\theta}\ell''\left(\theta^{\left(j\right)}\right) & \approx\theta^{\left(j\right)}\ell''\left(\theta^{\left(j\right)}\right)-\ell'\left(\theta^{\left(j\right)}\right)\\
\implies\hat{\theta} & \approx\frac{\theta^{\left(j\right)}\ell''\left(\theta^{\left(j\right)}\right)-\ell'\left(\theta^{\left(j\right)}\right)}{\ell''\left(\theta^{\left(j\right)}\right)}\\
 & \approx\theta^{\left(j\right)}-\frac{\ell'\left(\theta^{\left(j\right)}\right)}{\ell''\left(\theta^{\left(j\right)}\right)}.
\end{flalign*}

\end_inset

This suggests the following iterative scheme:
\begin_inset Formula 
\begin{equation}
\hat{\theta}^{\left(j+1\right)}=\theta^{\left(j\right)}-\frac{\ell'\left(\theta^{\left(j\right)}\right)}{\ell''\left(\theta^{\left(j\right)}\right)}.\label{eq:newton-raphson-update}
\end{equation}

\end_inset

We will continue to improve the estimator in this way until convergence.
\end_layout

\begin_layout Standard
In the multiparameter case, the MLE 
\begin_inset Formula $\hat{\boldsymbol{\theta}}=\left(\hat{\theta}_{1},\ldots,\hat{\theta}_{k}\right)$
\end_inset

 is a vector and the method becomes
\begin_inset Formula 
\[
\hat{\boldsymbol{\theta}}^{\left(j+1\right)}=\boldsymbol{\theta}^{\left(j\right)}-\mathbf{H}_{\ell}^{-1}\ell'\left(\boldsymbol{\theta}^{\left(j\right)}\right),
\]

\end_inset

where 
\begin_inset Formula $\ell'\left(\boldsymbol{\theta}^{\left(j\right)}\right)$
\end_inset

 is a vector of first derivatives, i.e., 
\begin_inset Formula 
\[
\ell'\left(\boldsymbol{\theta}^{\left(j\right)}\right)=\left(\frac{\partial}{\partial\theta_{1}}\ell\left(\boldsymbol{\theta}\right),\ldots,\frac{\partial}{\partial\theta_{k}}\ell\left(\boldsymbol{\theta}\right)\right),
\]

\end_inset

and 
\begin_inset Formula $\mathbf{H}_{\ell}\left(\boldsymbol{\theta}\right)$
\end_inset

 is the (Hessian) matrix of second partial derivatives of the log-likelihood,
 i.e.,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{2.5}
\end_layout

\end_inset

 
\begin_inset Formula 
\[
\mathbf{H}_{\ell}\left(\boldsymbol{\theta}\right)=l''\left(\boldsymbol{\theta}\right)=\begin{vmatrix}\dfrac{\partial^{2}}{\partial\theta_{1}^{2}}\ell\left(\theta\right) & \cdots & \dfrac{\partial^{2}}{\partial\theta_{1}\theta_{k}}\ell\left(\theta\right)\\
\vdots & \ddots & \vdots\\
\dfrac{\partial^{2}}{\partial\theta_{k}\theta_{1}}\ell\left(\theta\right) & \cdots & \dfrac{\partial^{2}}{\partial\theta_{k}^{2}}\ell\left(\theta\right)
\end{vmatrix}.
\]

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

Suppose 
\begin_inset Formula $\varepsilon$
\end_inset

 is sufficiently close to zero.
 Then, the convergence criterion may be 
\begin_inset Formula $\left|\theta^{\left(j+1\right)}-\theta^{\left(j\right)}\right|<\varepsilon$
\end_inset

, 
\begin_inset Formula $\ell'\left(\theta^{\left(j+1\right)}\right)<\varepsilon$
\end_inset

, or 
\begin_inset Formula $\mathcal{L}\left(\theta^{\left(j+1\right)}\right)-\mathcal{L}\left(\theta^{\left(j\right)}\right)<\varepsilon$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 with
\begin_inset Formula 
\[
Y_{i}=\begin{cases}
0, & \text{if subject \ensuremath{i} did not experience the event}\\
1, & \text{if subject \ensuremath{i} experienced the event}
\end{cases}.
\]

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be the fixed covariate for each subject and 
\begin_inset Formula 
\[
P\left(\left\{ Y_{i}=1\right\} \right)=\frac{\mathrm{e}^{\theta x_{i}}}{1+\mathrm{e}^{\theta x_{i}}}.
\]

\end_inset

Suppose 
\begin_inset Formula $n=5$
\end_inset

 and 
\begin_inset Formula $\left(x_{i},y_{i}\right)$
\end_inset

 are as shown below.
 Solve for the MLE of 
\begin_inset Formula $\theta$
\end_inset

 using the Newton-Raphson algorithm.
\end_layout

\begin_layout Example
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x_{i}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $y_{i}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
We have 
\begin_inset Formula $Y_{i}\sim\text{Bernoulli}\left(p_{i}\right)$
\end_inset

, so that the pmf of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
p_{Y_{i}}\left(y_{i}|p_{i}\right) & =p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}}.
\end{flalign*}

\end_inset

We are given that 
\begin_inset Formula $P\left(\left\{ Y_{i}=1\right\} \right)=\mathrm{e}^{\theta x_{i}}/\left(1+\mathrm{e}^{\theta x_{i}}\right)$
\end_inset

, so we can write
\begin_inset Formula 
\[
P\left(\left\{ Y_{i}=1\right\} \right)=p_{i}^{1}\left(1-p_{i}\right)^{1-1}=p_{i}=\frac{\mathrm{e}^{\theta x_{i}}}{1+\mathrm{e}^{\theta x_{i}}}.
\]

\end_inset

Then, we can reparameterize 
\begin_inset Formula $p_{Y_{i}}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
p_{Y_{i}}\left(y_{i}|\theta\right) & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(1-\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1+e^{\theta x_{i}}-e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\theta x_{i}}}\right)^{1-y_{i}}\\
 & =\left(\frac{e^{\theta x_{i}}}{1+e^{\theta x_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\theta x_{i}}}\right)\left(\frac{1}{1+e^{\theta x_{i}}}\right)^{-y_{i}}\\
 & =\frac{\left(e^{\theta x_{i}}\right)^{y_{i}}}{\left(1+e^{\theta x_{i}}\right)^{y_{i}}}\left(1+e^{\theta x_{i}}\right)^{-1}\left(1+e^{\theta x_{i}}\right)^{y_{i}}\\
 & =e^{\theta x_{i}y_{i}}\left(1+e^{\theta x_{i}}\right)^{-1},
\end{flalign*}

\end_inset

so that the joint pmf of 
\begin_inset Formula $\mathbf{Y}=Y_{1},\ldots,Y_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{\mathbf{Y}}\left(\mathbf{y}|\theta\right)=\prod_{i=1}^{n}p_{Y_{i}}\left(y_{i}|\theta\right)=\prod_{i=1}^{n}\mathrm{e}^{\theta x_{i}y_{i}}\left(1+\mathrm{e}^{\theta x_{i}}\right)^{-1}=\mathrm{e}^{\sum_{i=1}^{n}\theta x_{i}y_{i}}\prod_{i=1}^{n}\left(1+\mathrm{e}^{\theta x_{i}}\right)^{-1}.
\]

\end_inset

Then, the log-likelihood is given by 
\begin_inset Formula 
\begin{flalign*}
\ell\left(\theta\right) & =\log\mathcal{L}\left(\theta|\mathbf{y}\right)\\
 & =\log\left[\mathrm{e}^{\sum_{i=1}^{n}\theta x_{i}y_{i}}\prod_{i=1}^{n}\left(1+\mathrm{e}^{\theta x_{i}}\right)^{-1}\right]\\
 & =\log\mathrm{e}^{\theta\sum_{i=1}^{n}x_{i}y_{i}}+\log\prod_{i=1}^{n}\left(1+\mathrm{e}^{\theta x_{i}}\right)^{-1}\\
 & =\theta\sum_{i=1}^{n}x_{i}y_{i}+\sum_{i=1}^{n}\log\left(1+\mathrm{e}^{\theta x_{i}}\right)^{-1}\\
 & =\theta\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\log\left(1+\mathrm{e}^{\theta x_{i}}\right).
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\ell\left(\theta\right) & =\frac{\partial}{\partial\theta}\left[\theta\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\log\left(1+\mathrm{e}^{\theta x_{i}}\right)\right]\\
 & =\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+\mathrm{e}^{\theta x_{i}}}\cdot x_{i}\mathrm{e}^{\theta x_{i}}
\end{flalign*}

\end_inset

To implement Newton-Raphson algorithm, we will also need the second derivative
 with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta^{2}}\ell\left(\theta\right) & =\frac{\partial}{\partial\theta}\left[\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+e^{\theta x_{i}}}\cdot x_{i}\mathrm{e}^{\theta x_{i}}\right]\\
 & =0-\sum_{i=1}^{n}\frac{\left(x_{i}^{2}\mathrm{e}^{\theta x_{i}}\right)\left(1+\mathrm{e}^{\theta x_{i}}\right)-\left(x_{i}\mathrm{e}^{\theta x_{i}}\right)\left(x_{i}\mathrm{e}^{\theta x_{i}}\right)}{\left(1+\mathrm{e}^{\theta x_{i}}\right)^{2}}\\
 & =-\sum_{i=1}^{n}\frac{x_{i}^{2}\mathrm{e}^{\theta x_{i}}+x_{i}^{2}\mathrm{e}^{2\theta x_{i}}-x_{i}^{2}\mathrm{e}^{2\theta x_{i}}}{\left(1+\mathrm{e}^{\theta x_{i}}\right)^{2}}\\
 & =-\sum_{i=1}^{n}\frac{x_{i}^{2}\mathrm{e}^{\theta x_{i}}}{\left(1+\mathrm{e}^{\theta x_{i}}\right)^{2}}
\end{flalign*}

\end_inset

Thus, we must solve
\begin_inset Formula 
\begin{flalign*}
\hat{\theta} & =\theta^{\left(j\right)}-\frac{\ell'\left(\theta^{\left(j\right)}\right)}{\ell''\left(\theta^{\left(j\right)}\right)}\\
 & =\theta^{\left(j\right)}-\frac{\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+\mathrm{e}^{\theta x_{i}}}\cdot x_{i}\mathrm{e}^{\theta x_{i}}}{-\sum_{i=1}^{n}\frac{x_{i}^{2}\mathrm{e}^{\theta x_{i}}}{\left(1+\mathrm{e}^{\theta x_{i}}\right)^{2}}}\\
 & =\theta^{\left(j\right)}+\frac{\sum_{i=1}^{n}x_{i}y_{i}-\sum_{i=1}^{n}\frac{1}{1+\mathrm{e}^{\theta x_{i}}}\cdot x_{i}\mathrm{e}^{\theta x_{i}}}{\sum_{i=1}^{n}\frac{x_{i}^{2}\mathrm{e}^{\theta x_{i}}}{\left(1+\mathrm{e}^{\theta x_{i}}\right)^{2}}}.
\end{flalign*}

\end_inset

We can implement the algorithm using the following 
\family sans
R
\family default
 code.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

newtraph <- function(x, y, theta, eps = 1e-6) {
\end_layout

\begin_layout Plain Layout

	diff <- 1
\end_layout

\begin_layout Plain Layout

	theta.hat <- theta
\end_layout

\begin_layout Plain Layout

	while (diff > eps) {
\end_layout

\begin_layout Plain Layout

		eta <- theta * x
\end_layout

\begin_layout Plain Layout

		U <- sum(x * y) - sum((x * exp(eta)) / (1 + exp(eta)))
\end_layout

\begin_layout Plain Layout

		I <- sum((x^2 * exp(eta)) / (1 + exp(eta)^2))
\end_layout

\begin_layout Plain Layout

		theta.new <- theta + (U / I)
\end_layout

\begin_layout Plain Layout

		diff <- abs(theta.new - theta)
\end_layout

\begin_layout Plain Layout

		theta <- theta.new
\end_layout

\begin_layout Plain Layout

		theta.hat <- c(theta.hat, theta)
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

	return(theta.hat)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x <- c(4.1,2.2,3.9,7.1,6.2)
\end_layout

\begin_layout Plain Layout

y <- c(0,1,0,1,1)
\end_layout

\begin_layout Plain Layout

newtraph(x, y, 0.3)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
So, we see that our estimates for 
\begin_inset Formula $\hat{\theta}$
\end_inset

 converge and 
\begin_inset Formula $\hat{\theta}\approx0.125$
\end_inset

.
\end_layout

\begin_layout Subsubsection
EM algorithm
\end_layout

\begin_layout Standard
The idea behind the EM algorithm is to iterate between taking an expectation
 and maximizing.
 Consider data 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 whose density 
\begin_inset Formula $f\left(\mathbf{y}|\theta\right)$
\end_inset

 leads to a log-likelihood function that is hard to maximize.
 Suppose we can find another random variable 
\begin_inset Formula $Z$
\end_inset

 such that 
\begin_inset Formula $f\left(\mathbf{y}|\theta\right)=\int f\left(\mathbf{y},\mathbf{z}|\theta\right)\mbox{d}z$
\end_inset

 and such that the likelihood based on 
\begin_inset Formula $f\left(\mathbf{y},\mathbf{z}|\theta\right)$
\end_inset

 is easy to maximize.
 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 is the observed data, and 
\begin_inset Formula $Z$
\end_inset

 is called the augmented (or latent, or missing) data, and 
\begin_inset Formula $L\left(\theta|\mathbf{y},\mathbf{z}\right)=f\left(\mathbf{y},\mathbf{z}|\theta\right)$
\end_inset

 is the complete-data likelihood.
 Conceptually, the EM algorithm works by filling in the missing data, maximizing
 the expectation of the complete log-likelihood, and iterating until convergence.
\end_layout

\begin_layout Standard
The EM algorithm proceeds as follows:
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(0)]
\backslash
setcounter{enumi}{0}
\end_layout

\end_inset

Pick a starting value 
\begin_inset Formula $\theta^{\left(0\right)}$
\end_inset

.
 For 
\begin_inset Formula $j=1,2,\ldots$
\end_inset

, repeat steps 1 and 2 below.
\end_layout

\begin_layout Enumerate

\series bold
E-step:
\series default
 Calculate
\begin_inset Formula 
\begin{flalign*}
J\left(\theta|\theta^{\left(j\right)}\right) & =\E\left[\log L\left(\theta|\mathbf{y},\mathbf{z}\right)|\theta^{\left(j\right)},\mathbf{y}\right]\\
 & =\E\left[\log f\left(y_{1},\ldots,y_{n},z_{1},\ldots,z_{n}|\theta\right)|\theta^{\left(j\right)},Y_{1}=y_{1},\ldots,Y_{n}=y_{n}\right]
\end{flalign*}

\end_inset

The expectation is over the missing data 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 treating 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

 and the observed data 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 as fixed.
\end_layout

\begin_layout Enumerate

\series bold
M-step:
\series default
 Find 
\begin_inset Formula $\theta^{\left(j+1\right)}$
\end_inset

 to maximize 
\begin_inset Formula $J\left(\theta|\theta^{\left(j\right)}\right)$
\end_inset

, i.e., take the derivative, set it equal to zero, and solve for 
\begin_inset Formula $\theta\rightarrow\theta^{\left(j+1\right)}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Mixture of normals]
\end_layout

\end_inset

Often the data distribution may arise as a mixture of normal densities.
 Consider, for instance, heights of people arising as a mixture of men's
 and women's heights.
 The density of a mixture of two Normals is
\begin_inset Formula 
\[
f\left(y|\theta\right)=\left(1-p\right)\phi\left(y|\mu_{0},\sigma_{0}\right)+p\phi\left(y|\mu_{1},\sigma_{1}\right)
\]

\end_inset

where 
\begin_inset Formula $\phi\left(y|\mu,\sigma\right)$
\end_inset

 denotes a normal density with mean 
\begin_inset Formula $\mu$
\end_inset

 and standard deviation 
\begin_inset Formula $\sigma$
\end_inset

.
 The idea is that an observation is drawn from the first normal with probability
 
\begin_inset Formula $p$
\end_inset

 and the second with probability 
\begin_inset Formula $1-p$
\end_inset

.
 However, we don't know from which Normal it was drawn.
 The parameters are 
\begin_inset Formula $\boldsymbol{\theta}=\left(\mu_{0},\sigma_{0},\mu_{1},\sigma_{1},p\right)$
\end_inset

.
\end_layout

\begin_layout Example
The incomplete-data likelihood is given by
\begin_inset Formula 
\[
L\left(\boldsymbol{\theta}|\mathbf{y}\right)=\prod_{i=1}^{n}\left[\left(1-p\right)\phi\left(y_{i}|\mu_{0},\sigma_{0}\right)+p\phi\left(y_{i}|\mu_{1},\sigma_{1}\right)\right],
\]

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\[
\log L\left(\boldsymbol{\theta}|\mathbf{y}\right)=\sum_{i=1}^{n}\log\left[\left(1-p\right)\phi\left(y_{i}|\mu_{0},\sigma_{0}\right)+p\phi\left(y_{i}|\mu_{1},\sigma_{1}\right)\right].
\]

\end_inset

The sum inside the logarithm makes this difficult to maximize.
 Instead, we will define an indicator variable 
\begin_inset Formula $Z_{i}$
\end_inset

 as
\begin_inset Formula 
\[
Z_{i}=\begin{cases}
0, & \text{if }Y_{i}\sim N\left(\mu_{0},\sigma_{0}^{2}\right)\\
1, & \text{if }Y_{i}\sim N\left(\mu_{1},\sigma_{1}^{2}\right)
\end{cases}.
\]

\end_inset

Then, we can write the joint density of 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 conditioned on 
\begin_inset Formula $\mathbf{Z}$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{Y}}\left(\mathbf{y}|\theta,z_{i}=j\right) & =\phi\left(y_{i}|\mu_{j},\sigma_{j}\right)\\
 & =\frac{1}{\sqrt{2\pi\sigma_{j}^{2}}}e^{-\left(y_{i}-\mu_{j}\right)^{2}/\left(2\sigma_{j}^{2}\right)}.
\end{flalign*}

\end_inset

Recall that the probability of the intersection of two events 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 is equal to the probability of 
\begin_inset Formula $A$
\end_inset

 given 
\begin_inset Formula $B$
\end_inset

 multiplied by the probability of 
\begin_inset Formula $B$
\end_inset

, i.e., 
\begin_inset Formula 
\[
P\left(\left\{ A\cap B\right\} \right)=P\left(\left\{ A|B\right\} \right)P\left(\left\{ B\right\} \right).
\]

\end_inset

Then, abusing notation slightly, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ Y_{i}=y_{i}\right\} \cap\left\{ Z_{i}=z_{i}\right\} \right) & =P\left(\left\{ Y_{i}=y_{i}|Z_{i}=z_{i}\right\} \right)P\left(\left\{ Z_{i}=z_{i}\right\} \right),
\end{flalign*}

\end_inset

i.e., the joint density of 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Z}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{Y},\mathbf{Z}}\left(y_{i},z_{i}=j\right) & =f_{\mathbf{Y}}\left(y_{i}|z_{i}=j,\theta\right)P\left(\left\{ Z_{i}=j\right\} \right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $P\left(\left\{ Z_{i}=1\right\} \right)=p$
\end_inset

 and 
\begin_inset Formula $P\left(\left\{ Z_{i}=0\right\} \right)=1-p$
\end_inset

.
 Then, the complete-data likelihood is given by
\begin_inset Formula 
\begin{flalign*}
L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right) & =\prod_{i=1}^{n}f_{\mathbf{Y},\mathbf{Z}}\left(y_{i},z_{i}|\boldsymbol{\theta}\right)\\
 & =\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2\pi\sigma_{j}^{2}}}e^{-\left(y_{i}-\mu_{j}\right)^{2}/\left(2\sigma_{j}^{2}\right)}\cdot P\left(\left\{ Z_{i}=j\right\} \right)\right]\\
 & =\prod_{i=1}^{n}\left[\left[\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\cdot\left(1-p\right)\right]^{1-z_{i}}\cdot\left[\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\cdot p\right]^{z_{i}}\right],
\end{flalign*}

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right) & =\log\prod_{i=1}^{n}\left[\left[\frac{1}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\cdot\left(1-p\right)\right]^{1-z_{i}}\cdot\left[\frac{1}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\cdot p\right]^{z_{i}}\right]\\
 & =\sum_{i=1}^{n}\left\{ \left(1-z_{i}\right)\log\left[\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right]+z_{i}\log\left[\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right]\right\} .
\end{flalign*}

\end_inset

We will now find the expected value of the complete-data log likelihood
 (the E-step).
\begin_inset Formula 
\begin{flalign*}
J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\E\left[\log L\left(\boldsymbol{\theta}|\mathbf{y},\mathbf{z}\right)|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\\
 & =\sum_{i=1}^{n}\left\{ \E\left[1-z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\log\left[\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right]+\E\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]\log\left[\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right]\right\} 
\end{flalign*}

\end_inset

We will evaluate 
\begin_inset Formula $\E\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right]=P\left(\left\{ z_{i}=1|y_{i},\boldsymbol{\theta}^{\left(j\right)}\right\} \right)$
\end_inset

 using Bayes' Rule.
\begin_inset Formula 
\begin{flalign*}
\E\left[z_{i}|\mathbf{y},\boldsymbol{\theta}^{\left(j\right)}\right] & =P\left(\left\{ z_{i}=1|y_{i},\boldsymbol{\theta}^{\left(j\right)}\right\} \right)\\
 & =\frac{f\left(y_{i}|z_{i}=1,\boldsymbol{\theta}^{\left(j\right)}\right)P\left(\left\{ Z_{i}=1\right\} \right)}{\sum_{k=0}^{1}f\left(y_{i}|Z_{i}=k,\boldsymbol{\theta}^{\left(j\right)}\right)P\left(\left\{ Z_{i}=k\right\} \right)}\\
 & =\frac{\phi\left(y_{i}|\mu_{1}^{\left(j\right)},\sigma_{1}^{\left(j\right)}\right)\cdot p^{\left(j\right)}}{p^{\left(j\right)}\phi\left(y_{i}|\mu_{1}^{\left(j\right)},\sigma_{1}^{\left(j\right)}\right)+\left(1-p^{\left(j\right)}\right)\phi\left(y_{i}|\mu_{0}^{\left(j\right)},\sigma_{0}^{\left(j\right)}\right)}\\
 & =\tau_{i}^{\left(j\right)}
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\log\left(\frac{1-p}{\sqrt{2\pi\sigma_{0}^{2}}}\right)+\log\left(e^{-\left(y_{i}-\mu_{0}\right)^{2}/\left(2\sigma_{0}^{2}\right)}\right)\right]\right.\\
 & \qquad\left.+\tau_{i}^{\left(j\right)}\left[\log\frac{p}{\sqrt{2\pi\sigma_{1}^{2}}}+\log\left(e^{-\left(y_{i}-\mu_{1}\right)^{2}/\left(2\sigma_{1}^{2}\right)}\right)\right]\right\} \\
 & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\log\left(1-p\right)-\log\left(2\pi\sigma_{0}^{2}\right)^{1/2}-\frac{\left(y_{i}-\mu_{0}\right)^{2}}{2\sigma_{0}^{2}}\right]+\tau_{i}^{\left(j\right)}\left[\log p-\log\left(2\pi\sigma_{1}^{2}\right)^{1/2}-\frac{\left(y_{i}-\mu_{1}\right)^{2}}{2\sigma_{1}^{2}}\right]\right\} \\
 & =\sum_{i=1}^{n}\left\{ \left(1-\tau_{i}^{\left(j\right)}\right)\left[\log\left(1-p\right)-\frac{1}{2}\log2\pi\sigma_{0}^{2}-\frac{\left(y_{i}-\mu_{0}\right)^{2}}{2\sigma_{0}^{2}}\right]+\tau_{i}^{\left(j\right)}\left[\log p-\frac{1}{2}\log2\pi\sigma_{1}^{2}-\frac{\left(y_{i}-\mu_{1}\right)^{2}}{2\sigma_{1}^{2}}\right]\right\} .
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $p$
\end_inset

 (the M-step).
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial p}J\left(\boldsymbol{\theta}|\boldsymbol{\theta}^{\left(j\right)}\right) & =\sum_{i=1}^{n}\left\{ -\left(1-\tau_{i}^{\left(j\right)}\right)\frac{1}{1-p}+\tau_{i}^{\left(j\right)}\frac{1}{p}\right\} 
\end{flalign*}

\end_inset

(Although 
\begin_inset Formula $p$
\end_inset

 is incorporated in 
\begin_inset Formula $\tau_{i}^{\left(j\right)}$
\end_inset

, it is considered to have been evaluated there, so that 
\begin_inset Formula $\tau_{i}^{\left(j\right)}$
\end_inset

 is treated as constant when taking the deriative of 
\begin_inset Formula $J$
\end_inset

 with respect to 
\begin_inset Formula $p$
\end_inset

.) Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n}\frac{\left(1-\tau_{i}^{\left(j\right)}\right)}{1-p} & =\sum_{i=1}^{n}\frac{\tau_{i}^{\left(j\right)}}{p}\\
\Leftrightarrow\frac{1}{1-p}\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right) & =\frac{1}{p}\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}\\
\Leftrightarrow\left(1-p\right)\sum_{i=1}^{n}\tau_{i}^{\left(j\right)} & =p\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right)\\
\Leftrightarrow\frac{1-p}{p} & =\frac{\sum_{i=1}^{n}\left(1-\tau_{i}^{\left(j\right)}\right)}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow\frac{1}{p}-1 & =\frac{n-\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow\frac{1}{p}-1 & =\frac{n}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}-1\\
\Leftrightarrow\frac{1}{p} & =\frac{n}{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}\\
\Leftrightarrow p^{\left(j+1\right)} & =\frac{\sum_{i=1}^{n}\tau_{i}^{\left(j\right)}}{n}.
\end{flalign*}

\end_inset

We can proceed similarly to find the other parameter estimates, i.e., by taking
 the derivative of 
\begin_inset Formula $J$
\end_inset

 with respect to each parameter, setting the result equal to zero, and solving
 for the parameter.
\end_layout

\begin_layout Standard
We have introduced four methods for finding MLEs.
 In order of approximate difficulty, the methods are
\end_layout

\begin_layout Enumerate
Set the derivative of the log-likelihood equal to zero and solve or 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 This is the simplest method when a closed-form solution is possible.
\end_layout

\begin_layout Enumerate
When the support depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, plot the likelihood function and determine where it attains its maximum.
\end_layout

\begin_layout Enumerate
Use the Newton-Raphson method to estimate the MLE.
\end_layout

\begin_layout Enumerate
Use the EM method to find the expectation of the complete log-likelihood
 using the observed 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and an initial estimate for 
\begin_inset Formula $\theta^{\left(j\right)}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Introduce a latent variable 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Enumerate
Find the complete log-likelihood 
\begin_inset Formula $L\left(\theta|\mathbf{y},\mathbf{z}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Use the E-step to estimate 
\begin_inset Formula $z^{\left(j\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Use the M-step to maximize 
\begin_inset Formula $J\left(\theta|\theta^{\left(j\right)}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection
Bayes estimators
\end_layout

\begin_layout Standard
In the classical (frequentist) approach, 
\begin_inset Formula $\theta$
\end_inset

 is considered to be unknown, but fixed.
 in the Bayesian approach, 
\begin_inset Formula $\theta$
\end_inset

 is treated as a random variable.
 A prior distribution for 
\begin_inset Formula $\theta$
\end_inset

, 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

, is specified.
 This may be non-informative or may describe expert opinion (or subjective
 belief).
 Inference is based on the posterior distribution, 
\begin_inset Formula 
\[
\pi\left(\theta|\mathbf{x}\right)=\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{\int f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)\dif\theta},
\]

\end_inset

which is obtained from Bayes' rule and updates the prior, 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

, given the observed data.
 The expectation of 
\begin_inset Formula $\pi\left(\theta|\mathbf{x}\right)$
\end_inset

, which is given by
\begin_inset Formula 
\[
\E\left[\pi\left(\theta|\mathbf{x}\right)\right]=\int_{\Theta}\theta\cdot\pi\left(\theta|\mathbf{x}\right)\dif\theta,
\]

\end_inset

where 
\begin_inset Formula $\Theta$
\end_inset

 is the support of 
\begin_inset Formula $\theta$
\end_inset

, i.e., the parameter space, is called the posterior mean or the 
\shape italic
Bayes estimator
\shape default
.
 One could also use the most probable value of 
\begin_inset Formula $\theta$
\end_inset

, the 
\shape italic
posterior mode
\shape default
, as a point estimator.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\text{Bernoulli}\left(\theta\right)$
\end_inset

 and consider 
\begin_inset Formula $\theta\sim\mathcal{U}\left(0,1\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the Bayes estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The prior distribution is uniform on the interval 
\begin_inset Formula $\left(0,1\right)$
\end_inset

, so its density is given by
\begin_inset Formula 
\[
\pi\left(\theta\right)=\frac{1}{1-0}=1.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}},
\]

\end_inset

so that the posterior distribution is given by
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{\int p_{\mathbf{X}}\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)\mbox{d}\theta}\\
 & =\frac{\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\cdot1}{\int_{0}^{1}\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\cdot1\mbox{d}\theta}\\
 & =\frac{\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}}{\int_{0}^{1}\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\mbox{d}\theta}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $\alpha=\sum_{i=1}^{n}x_{i}+1$
\end_inset

 and let 
\begin_inset Formula $\beta=n-\sum_{i=1}^{n}x_{i}+1$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{\int_{0}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\mbox{d}\theta}.
\end{flalign*}

\end_inset

We recognize the integrand as the kernel of a 
\begin_inset Formula $\text{Beta}\left(\alpha,\beta\right)$
\end_inset

 distribution, so we write
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{B\left(\alpha,\beta\right)\int_{0}^{1}\frac{1}{B\left(\alpha,\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\mbox{d}\theta},
\end{flalign*}

\end_inset

where 
\begin_inset Formula $B\left(\alpha,\beta\right)=\Gamma\left(\alpha\right)\Gamma\left(\beta\right)/\Gamma\left(\alpha+\beta\right)$
\end_inset

.
 Then, the integrand is the pdf of a 
\begin_inset Formula $\text{Beta}\left(\alpha,\beta\right)$
\end_inset

 distribution, which integrates to 1, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{B\left(\alpha,\beta\right)\cdot1}\\
 & =\frac{1}{B\left(\alpha,\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1},
\end{flalign*}

\end_inset

which is the pdf of a 
\begin_inset Formula $\text{Beta}\left(\alpha,\beta\right)$
\end_inset

 random variable.
 It follows that 
\begin_inset Formula 
\[
\pi\left(\theta|\mathbf{x}\right)\sim\text{Beta}\left(\sum_{i=1}^{n}x_{i}+1,n-\sum_{i=1}^{n}x_{i}+1\right).
\]

\end_inset

The expected value (mean) of a 
\begin_inset Formula $\text{Beta}\left(\gamma,\psi\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\gamma/\left(\gamma+\psi\right)$
\end_inset

, so it follows that 
\begin_inset Formula 
\begin{flalign*}
\E\left[\pi\left(\theta|\mathbf{x}\right)\right] & =\frac{\alpha}{\alpha+\beta}\\
 & =\frac{\sum_{i=1}^{n}x_{i}+1}{\left(\sum_{i=1}^{n}x_{i}+1\right)+\left(n-\sum_{i=1}^{n}x_{i}+1\right)}\\
 & =\frac{\sum_{i=1}^{n}x_{i}+1}{n+2}
\end{flalign*}

\end_inset

is the Bayes estimator (posterior mean) for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Show that the Bayes estimator is a weighted average of the MLE and the prior
 mean.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log L\left(\theta|\mathbf{x}\right) & =\log\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\\
 & =\log\theta^{\sum_{i=1}^{n}x_{i}}+\log\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\\
 & =\left(\sum_{i=1}^{n}x_{i}\right)\log\theta+\left(n-\sum_{i=1}^{n}x_{i}\right)\log\left(1-\theta\right).
\end{flalign*}

\end_inset

We will take the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right) & =\frac{\partial}{\partial\theta}\left[\left(\sum_{i=1}^{n}x_{i}\right)\log\theta+\left(n-\sum_{i=1}^{n}x_{i}\right)\log\left(1-\theta\right)\right]\\
 & =\frac{1}{\theta}\sum_{i=1}^{n}x_{i}+\left(n-\sum_{i=1}^{n}x_{i}\right)\frac{1}{1-\theta}\cdot-1\\
 & =\frac{1}{\theta}\sum_{i=1}^{n}x_{i}-\frac{1}{1-\theta}\left(n-\sum_{i=1}^{n}x_{i}\right)
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\begin{flalign*}
\frac{1}{\theta}\sum_{i=1}^{n}x_{i} & =\frac{1}{1-\theta}\left(n-\sum_{i=1}^{n}x_{i}\right)\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right) & =\left(1-\theta\right)\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right) & =\sum_{i=1}^{n}x_{i}-\theta\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left(n-\sum_{i=1}^{n}x_{i}\right)+\theta\sum_{i=1}^{n}x_{i} & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta\left[\left(n-\sum_{i=1}^{n}x_{i}\right)+\sum_{i=1}^{n}x_{i}\right] & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\theta n & =\sum_{i=1}^{n}x_{i}\\
\Leftrightarrow\hat{\theta} & =\bar{x}.
\end{flalign*}

\end_inset

The prior mean is given by 
\begin_inset Formula 
\[
\E\left[\theta\right]=\frac{0+1}{2}=\frac{1}{2}.
\]

\end_inset

Suppose that 
\begin_inset Formula $a$
\end_inset

 is the weight of the MLE and 
\begin_inset Formula $1-a$
\end_inset

 is the weight of the prior mean, so that we have
\begin_inset Formula 
\begin{flalign*}
a\bar{x}+\left(1-a\right)\frac{1}{2} & =\frac{n\bar{x}+1}{n+2}\\
 & =\frac{n}{n+2}\bar{x}+\frac{1}{n+2}\\
 & =\frac{n}{n+2}\bar{x}+\left(\frac{1}{n+2}\right)\left(\frac{2}{2}\right)\\
 & =\frac{n}{n+2}\bar{x}+\left(\frac{2+n-n}{n+2}\right)\frac{1}{2}\\
 & =\frac{n}{n+2}\bar{x}+\left(1-\frac{n}{n+2}\right)\frac{1}{2}
\end{flalign*}

\end_inset

Then, it follows that 
\begin_inset Formula 
\begin{flalign*}
a & =\frac{n}{n+2}\quad\mbox{and}\quad1-a=\frac{2}{n+2}.
\end{flalign*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson Bayes estimator]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:bayes-estimator-poisson"

\end_inset

Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 are iid 
\begin_inset Formula $\text{Poisson}\left(\lambda\right)$
\end_inset

, i.e.,
\begin_inset Formula 
\[
f\left(\mathbf{x}|\lambda\right)=\frac{\lambda^{\sum_{i=1}^{n}x_{i}}\mathrm{e}^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}.
\]

\end_inset

The investigator decides to use a prior distribution that captures his prior
 opinion using a gamma density (conjugate prior for Poisson) with mean of
 15 and standard deviation of 5.
 Find the posterior mean (Bayes estimator) of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Example
The posterior distribution is given by
\begin_inset Formula 
\begin{flalign*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\lambda\right)\pi\left(\lambda\right)}{\int f\left(\mathbf{x}|\lambda\right)\pi\left(\lambda\right)\dif\lambda}\\
 & =\frac{\left[\frac{\lambda^{\sum_{i=1}^{n}x_{i}}\mathrm{e}^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}\right]\left[\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}\mathrm{e}^{-\lambda/\beta}\right]}{\int_{0}^{\infty}\left[\frac{\lambda^{\sum_{i=1}^{n}x_{i}}\mathrm{e}^{-n\lambda}}{\prod_{i=1}^{n}x_{i}!}\right]\left[\frac{1}{\Gamma\left(\alpha\right)\beta^{\alpha}}\lambda^{\alpha-1}\mathrm{e}^{-\lambda/\beta}\right]\dif\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-n\lambda-\lambda/\beta}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-n\lambda-\lambda/\beta}\dif\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-\lambda\left(n+1/\beta\right)}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-\lambda\left(n+1/\beta\right)}\dif\lambda}\\
 & =\frac{\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-\lambda/\left(n+1/\beta\right)^{-1}}}{\int_{0}^{\infty}\lambda^{\sum_{i=1}^{n}x_{i}+\alpha-1}\mathrm{e}^{-\lambda/\left(n+1/\beta\right)^{-1}}\dif\lambda}
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $\alpha'=\sum_{i=1}^{n}x_{i}+\alpha$
\end_inset

 and let 
\begin_inset Formula $\beta'=1/\left(n+1/\beta\right)$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\lambda|\mathbf{x}\right) & =\frac{\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'}}{\int_{0}^{\infty}\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'}\dif\lambda}.
\end{flalign*}

\end_inset

We recognize the integrand as the kernel of a 
\begin_inset Formula $\Gamma\left(\alpha',\beta'\right)$
\end_inset

 distribution, so we write
\begin_inset Formula 
\[
\pi\left(\lambda|\mathbf{x}\right)=\frac{\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'}}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}\int_{0}^{\infty}\frac{1}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}}\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'}\dif\lambda}.
\]

\end_inset

Then, the integrand is the pdf of a 
\begin_inset Formula $\Gamma\left(\alpha',\beta'\right)$
\end_inset

 distribution, which integrates to 1, so that we have
\begin_inset Formula 
\[
\pi\left(\lambda|\mathbf{x}\right)=\frac{\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'}}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}\cdot1}=\frac{1}{\Gamma\left(\alpha'\right)\beta'^{\alpha'}}\lambda^{\alpha'-1}\mathrm{e}^{-\lambda/\beta'},
\]

\end_inset

which is the pdf of a 
\begin_inset Formula $\text{Gamma}\left(\alpha',\beta'\right)$
\end_inset

 random variable, i.e., 
\begin_inset Formula $\pi\left(\lambda|\mathbf{x}\right)\sim\text{Gamma}\left(\sum_{i=1}^{n}x_{i}+\alpha,1/\left(n+1/\beta\right)\right)$
\end_inset

.
 The expected value (mean) of a 
\begin_inset Formula $\text{Gamma}\left(\gamma,\psi\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\gamma\psi$
\end_inset

, so it follows that
\begin_inset Formula 
\[
\E\left[\pi\left(\lambda|\mathbf{x}\right)\right]=\left(\sum_{i=1}^{n}x_{i}+\alpha\right)\left(\frac{1}{n+1/\beta}\right)=\frac{\sum_{i=1}^{n}x_{i}+\alpha}{n+1/\beta}
\]

\end_inset

is the Bayes estimator of 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mathcal{F}$
\end_inset

 denote the class of pdfs or pmfs 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 (indexed by 
\begin_inset Formula $\theta$
\end_inset

).
 A class 
\begin_inset Formula $\Pi$
\end_inset

 of prior distributions is a 
\shape italic
conjugate family
\shape default
 for 
\begin_inset Formula $\mathcal{F}$
\end_inset

 if the posterior distribution is in the class 
\begin_inset Formula $\Pi$
\end_inset

 for all 
\begin_inset Formula $f\in\mathcal{F}$
\end_inset

, all priors in 
\begin_inset Formula $\Pi$
\end_inset

, and all 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

, i.e., if 
\begin_inset Formula 
\[
\pi\left(\theta\right)\in\Pi\implies f\left(\theta|x\right)\in\Pi.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
Sampling models from exponential families all have conjugate priors, including
 the following examples.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sampling model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Prior & Posterior
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Binomial}\left(n,\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Beta
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Poisson}\left(\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gamma
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Exponential}\left(\theta\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gamma
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Gamma}\left(\alpha,\theta\right)$
\end_inset

 (
\begin_inset Formula $\alpha$
\end_inset

 known)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gamma
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Normal}\left(\theta,\sigma^{2}\right)$
\end_inset

 (
\begin_inset Formula $\sigma^{2}$
\end_inset

 known)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Normal
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Without a conjugate prior, it is not usually possible to obtain a closed-form
 solution for a posterior distribution.
\end_layout

\begin_layout Standard
Contemporary computational resources have had an enormous impact on Bayesian
 inference.
 The computationally difficult part of Bayesian inference is the calculation
 of the normalizing constant that makes the posterior density integrate
 to 1.
 Traditionally, such calculations were performed analytically, often using
 conjugate priors so that the integrations could be done explicitly.
 For more complex problems, Markov chain Monte Carlo methods are used to
 sample from the posterior distribution.
\end_layout

\begin_layout Section
Methods of evaluating estimators
\end_layout

\begin_layout Standard
In the previous section, we discussed techniques for finding point estimators
 of parameters.
 Since we can usually apply more than one of these methods, we are often
 faced with the task of choosing between estimators.
 What qualities should a 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 estimator have? Is it possible to find a 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\hat{\theta}$
\end_inset

?
\end_layout

\begin_layout Definition
The 
\shape italic
bias
\shape default
 of a point estimator 
\begin_inset Formula $W$
\end_inset

 of a parameter 
\begin_inset Formula $\theta$
\end_inset

 is the difference between the expected value of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

; that is, 
\begin_inset Formula $\text{Bias}_{\theta}W=\E_{\theta}\left[W\right]-\theta$
\end_inset

.
 An estimator whose bias is identically (in 
\begin_inset Formula $\theta$
\end_inset

) equal to zero is called 
\shape italic
unbiased
\shape default
 and satisfied 
\begin_inset Formula $\E_{\theta}\left[W\right]=\theta$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 be a random sample from the pdf 
\begin_inset Formula 
\[
f\left(y|\theta\right)=\frac{2y}{\theta^{2}},\quad0\leq y\leq\theta.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the moment estimator and the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The first moment of 
\begin_inset Formula $Y_{i}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[Y_{i}\right] & =\int_{0}^{\theta}y\cdot f_{Y_{i}}\left(y|\theta\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y\cdot\frac{2y}{\theta^{2}}\mbox{d}y\\
 & =\frac{2}{3\theta^{2}}y^{3}\Big\rvert_{0}^{\theta}\\
 & =\frac{2\theta^{3}}{3\theta^{2}}-0\\
 & =\frac{2}{3}\theta,
\end{flalign*}

\end_inset

so that we have 
\begin_inset Formula 
\begin{flalign*}
\hat{\mu}_{1} & =\frac{\sum_{i=1}^{n}y_{i}}{n}\\
\Leftrightarrow\frac{2}{3}\hat{\theta} & =\bar{y}\\
\Leftrightarrow\hat{\theta}_{MOM} & =\frac{3}{2}\bar{y}.
\end{flalign*}

\end_inset

The support depends on the parameter 
\begin_inset Formula $\theta$
\end_inset

, so we can write the pdf as 
\begin_inset Formula 
\begin{flalign*}
f_{Y}\left(y|\theta\right) & =\frac{2y}{\theta^{2}}I_{\left\{ 0\leq y\leq\theta\right\} }.
\end{flalign*}

\end_inset

Then, the likelihood function is given by
\begin_inset Formula 
\begin{flalign*}
L\left(\theta|\mathbf{y}\right) & =\prod_{i=1}^{n}f_{Y_{i}}\left(y_{i}|\theta\right)\\
 & =\prod_{i=1}^{n}\frac{2y_{i}}{\theta^{2}}I_{\left\{ 0\leq y_{i}\leq\theta\right\} }\\
 & =\frac{2^{n}\prod_{i=1}^{n}y_{i}}{\theta^{2n}}I_{\left\{ 0\leq y_{\left(n\right)}\leq\theta\right\} },
\end{flalign*}

\end_inset

which we can write as 
\begin_inset Formula 
\[
L\left(\theta|\mathbf{y}\right)=\begin{cases}
\dfrac{2^{n}\prod_{i=1}^{n}y_{i}}{\theta^{2n}}, & \theta\geq y_{\left(n\right)}\\
0, & \theta<y_{\left(n\right)}
\end{cases},
\]

\end_inset

and whose graph is shown below.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo=FALSE, fig.height=2, fig.width=3, fig.align='center', fig.pos='h'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(3,3,0.1,2))
\end_layout

\begin_layout Plain Layout

z <- seq(2, 4, length=2000) 
\end_layout

\begin_layout Plain Layout

plot(z, 2^4/(z^4), type="l", xlab = expression(theta), ylab=expression(paste(L,
 "(",theta,"|",bold(y),")")), yaxt="n", xaxt="n", xlim = c(1, 4), cex.lab
 = 0.75)
\end_layout

\begin_layout Plain Layout

axis(1, at=2, labels=expression(y[(n)]), cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Clearly, the likelihood is maximized at 
\begin_inset Formula $Y_{\left(n\right)}$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Is either estimator unbiased?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The expected value of 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
\E\left[\hat{\theta}_{MOM}\right] & =\E\left[\frac{3}{2}\bar{Y}\right]\\
 & =\frac{3}{2}\E\left[\bar{Y}\right]\\
 & =\frac{3}{2}\E\left[\frac{1}{n}\sum_{i=1}^{n}Y_{i}\right]\\
 & =\frac{3}{2n}\sum_{i=1}^{n}\E\left[Y_{i}\right]\\
 & =\frac{3}{2n}\sum_{i=1}^{n}\frac{2}{3}\theta\\
 & =\frac{3}{2n}\left(n\cdot\frac{2}{3}\theta\right)\\
 & =\theta,
\end{flalign*}

\end_inset

so it follows that 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta$
\end_inset

.
 The cdf of 
\begin_inset Formula $Y$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{Y}\left(y\right)=\int_{0}^{y}f_{Y}\left(y|\theta\right)\dif t=\int_{0}^{y}\frac{2t}{\theta^{2}}\dif t=\frac{t^{2}}{\theta^{2}}\Bigr\rvert_{0}^{y}=\frac{y^{2}}{\theta^{2}}-0=\frac{y^{2}}{\theta^{2}},
\]

\end_inset

so that the pdf of 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

 is given by 
\begin_inset Formula 
\[
f_{Y_{\left(n\right)}}\left(y|\theta\right)=n\left[F_{y}\left(y\right)\right]^{n-1}f_{Y}\left(y\right)=n\left(\frac{y^{2}}{\theta^{2}}\right)^{n-1}\left(\frac{2y}{\theta^{2}}\right)=n\frac{y^{2\left(n-1\right)}}{\theta^{2\left(n-1\right)}}\left(\frac{2y}{\theta^{2}}\right)=\frac{2ny^{2n-1}}{\theta^{2n}}.
\]

\end_inset

Then, the expected value of 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[\hat{\theta}_{MLE}\right] & =\E\left[Y_{\left(n\right)}\right]\\
 & =\int_{0}^{\theta}y\cdot f_{Y_{\left(n\right)}}\left(y\right)\mbox{d}y\\
 & =\int_{0}^{\theta}y\cdot\frac{2ny^{2n-1}}{\theta^{2n}}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}y^{2n}\mbox{d}y\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+1}y^{2n+1}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left(\frac{\theta^{2n+1}}{2n+1}-0\right)\\
 & =\frac{2n}{2n+1}\theta.
\end{flalign*}

\end_inset

We see that 
\begin_inset Formula $\E\left[\hat{\theta}_{MLE}\right]$
\end_inset

 is not equal to 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is a biased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Construct an unbiased estimator based on the MLE.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
An unbiased estimator 
\begin_inset Formula $\tilde{\lambda}$
\end_inset

 for 
\begin_inset Formula $\lambda$
\end_inset

 satisfies the condition that 
\begin_inset Formula $\E\left[\tilde{\lambda}\right]=\lambda$
\end_inset

 for all 
\begin_inset Formula $\lambda$
\end_inset

.
 So, to construct an unbiased estimator 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 based on the MLE 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

, we need to find a constant such that 
\begin_inset Formula $\E\left[\hat{\theta}_{MLE}\right]=\theta$
\end_inset

.
 We see that
\begin_inset Formula 
\[
\tilde{\theta}=\frac{2n+1}{2n}\hat{\theta}_{MLE}=\frac{2n+1}{2n}Y_{\left(n\right)}
\]

\end_inset

satisfies the condition that 
\begin_inset Formula 
\[
\E\left[\tilde{\theta}\right]=\E\left[\frac{2n+1}{2n}Y_{\left(n\right)}\right]=\frac{2n+1}{2n}\E\left[Y_{\left(n\right)}\right]=\frac{2n+1}{2n}\left(\frac{2n}{2n+1}\theta\right)=\theta,
\]

\end_inset

so it follows that 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is an unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

 based on the MLE.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
The precision of an estimator is another important property beside unbiasedness.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 be two unbiased estimators for a parameter 
\begin_inset Formula $\theta$
\end_inset

.
 If 
\begin_inset Formula $\Var\left(\hat{\theta}_{1}\right)<\Var\left(\hat{\theta}_{2}\right)$
\end_inset

, we say that 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 is 
\shape italic
more efficient
\shape default
 than 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

.
 The relative efficiency of 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 with respect to 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 is the ratio 
\begin_inset Formula $\Var\left(\hat{\theta}_{2}\right)/\Var\left(\hat{\theta}_{1}\right)$
\end_inset

.
\end_layout

\begin_layout Example
Consider the two unbiased estimators from the example above.
 Which estimator is more efficient?
\end_layout

\begin_layout Example
The variance of 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(\hat{\theta}_{MOM}\right)=\Var\left(\frac{3}{2}\bar{Y}\right)=\Var\left(\frac{3}{2n}\sum_{i=1}^{n}Y_{i}\right)=\frac{9}{4n^{2}}\Var\left(\sum_{i=1}^{n}Y_{i}\right)=\frac{9}{4n^{2}}\sum_{i=1}^{n}\Var\left(Y_{i}\right),
\]

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:variance-of-ind-rvs"

\end_inset

, i.e., the variance of the sum of independent random variables is equal to
 the sum of their variances.
 To find the variance of 
\begin_inset Formula $Y_{i}$
\end_inset

, we will need to find its second moment.
\begin_inset Formula 
\[
\E\left[Y^{2}\right]=\int_{0}^{\theta}y^{2}\cdot f_{Y}\left(y\right)\dif y=\int_{0}^{\theta}y^{2}\frac{2y}{\theta^{2}}\dif y=\frac{1}{2\theta^{2}}y^{4}\Big\rvert_{0}^{\theta}=\frac{\theta^{4}}{2\theta^{2}}-0=\frac{\theta^{2}}{2}
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\Var\left(\hat{\theta}_{MOM}\right) & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\Var\left(Y_{i}\right)\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\E\left[Y^{2}\right]-\left(\E\left[Y\right]\right)^{2}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{\theta^{2}}{2}-\left(\frac{2}{3}\theta\right)^{2}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{\theta^{2}}{2}-\frac{4\theta^{2}}{9}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\left[\frac{9\theta^{2}}{18}-\frac{8\theta^{2}}{18}\right]\\
 & =\frac{9}{4n^{2}}\sum_{i=1}^{n}\frac{\theta^{2}}{18}\\
 & =\frac{9}{4n^{2}}\cdot n\frac{\theta^{2}}{18}\\
 & =\frac{\theta^{2}}{8n}.
\end{flalign*}

\end_inset

The variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(\tilde{\theta}\right)=\Var\left(\frac{2n+1}{2n}\hat{\theta}_{MLE}\right)=\left(\frac{2n+1}{2n}\right)^{2}\Var\left(\hat{\theta}_{MLE}\right)=\frac{\left(2n+1\right)^{2}}{4n^{2}}\Var\left(\hat{\theta}_{MLE}\right).
\]

\end_inset

We will now find the second moment of 
\begin_inset Formula $\hat{\theta}_{MLE}=Y_{\left(n\right)}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\E\left[Y_{\left(n\right)}^{2}\right] & =\int_{0}^{\theta}y^{2}\cdot f_{Y_{\left(n\right)}}\left(y\right)\dif y\\
 & =\int_{0}^{\theta}y^{2}\cdot\frac{2ny^{2n-1}}{\theta^{2n}}\dif y\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}y^{2n+1}\dif y\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+2}y^{2n+2}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left(\frac{\theta^{2n+2}}{2n+2}-0\right)\\
 & =\frac{2n\theta^{2}}{2n+2}\\
 & =\frac{n\theta^{2}}{n+1}
\end{flalign*}

\end_inset

Then, the variance of 
\begin_inset Formula $\hat{\theta}_{MLE}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(\hat{\theta}_{MLE}\right) & =\Var\left(Y_{\left(n\right)}\right)\\
 & =\E\left[Y_{\left(n\right)}^{2}\right]-\left(\E\left[Y_{\left(n\right)}\right]\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\left(\frac{2n\theta}{2n+1}\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\frac{4n^{2}\theta^{2}}{\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[n\left(2n+1\right)^{2}-4n^{2}\left(n+1\right)\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[n\left(4n^{2}+4n+1\right)-4n^{3}-4n^{2}\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{\theta^{2}\left[4n^{3}+4n^{2}+n-4n^{3}-4n^{2}\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}},
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
\Var\left(\tilde{\theta}\right) & =\frac{\left(2n+1\right)^{2}}{4n^{2}}\Var\left(\hat{\theta}_{MLE}\right)\\
 & =\frac{\left(2n+1\right)^{2}}{4n^{2}}\left(\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}}\right)\\
 & =\frac{\theta^{2}}{4n\left(n+1\right)}.
\end{flalign*}

\end_inset

If 
\begin_inset Formula $\Var\left(\hat{\theta}_{MOM}\right)<\Var\left(\tilde{\theta}\right)$
\end_inset

, then 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is more efficient than 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 
\begin_inset Formula 
\begin{flalign*}
\Var\left(\hat{\theta}_{MOM}\right) & <\Var\left(\tilde{\theta}\right)\\
\Leftrightarrow\frac{\theta^{2}}{8n} & <\frac{\theta^{2}}{4n\left(n+1\right)}\\
\Leftrightarrow8n & >4n\left(n+1\right)\\
\Leftrightarrow8n-4n\left(n+1\right) & >0\\
\Leftrightarrow4n\left(2-\left(n+1\right)\right) & >0\\
\Leftrightarrow4n\left(2-n-1\right) & >0\\
\Leftrightarrow4n\left(1-n\right) & >0\\
\Leftrightarrow4n-4n^{2} & >0
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

, so it follows that 
\begin_inset Formula $4n-4n^{2}<0$
\end_inset

, so it cannot be the case that 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is more efficient than 
\begin_inset Formula $\tilde{\theta}$
\end_inset

.
 Therefore, 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is more efficient than 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

.
 The relative efficiency of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 with respect to 
\begin_inset Formula $\hat{\theta}_{MOM}$
\end_inset

 is given by 
\begin_inset Formula 
\[
\frac{\Var\left(\hat{\theta}_{MOM}\right)}{\Var\left(\tilde{\theta}\right)}=\frac{\theta^{2}/\left(8n\right)}{\theta^{2}/\left(4n\left(n+1\right)\right)}=\frac{4n\left(n+1\right)}{8n}=\frac{n+1}{2}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Mean squared error
\end_layout

\begin_layout Definition
The 
\shape italic
mean squared error
\shape default
 (MSE) of an estimator 
\begin_inset Formula $W$
\end_inset

 of a parameter 
\begin_inset Formula $\theta$
\end_inset

 is the function of 
\begin_inset Formula $\theta$
\end_inset

 defined by 
\begin_inset Formula $\E_{\theta}\left[\left(W-\theta\right)^{2}\right]$
\end_inset

.
\end_layout

\begin_layout Standard
Any increasing function of 
\begin_inset Formula $\left|W-\theta\right|$
\end_inset

 would serve to measure the goodness of an estimator, but MSE has at least
 two advantages over other distance measures.
 First, it is analytically tractable (absolute value would lead to discontinuiti
es).
 Second, it has the interpretation
\begin_inset Formula 
\begin{flalign*}
\E\left[\left(W-\theta\right)^{2}\right] & =\E\left[\left(W-\E\left[W\right]+\E\left[W\right]-\theta\right)^{2}\right]\\
 & =\E\left[\left(W-\E\left[W\right]\right)^{2}+2\left(W-\E\left[W\right]\right)\left(\E\left[W\right]-\theta\right)+\left(\E\left[W\right]-\theta\right)^{2}\right]\\
 & =\E\left[\left(W-\E\left[W\right]\right)^{2}\right]+2\E\left[\left(W-\E\left[W\right]\right)\left(\E\left[W\right]-\theta\right)\right]+\E\left[\left(\E\left[W\right]-\theta\right)^{2}\right]\\
 & =\Var\left(W\right)+2\E\left[W\E\left[W\right]-\theta W-\left(\E\left[W\right]\right)^{2}+\theta\E\left[W\right]\right]+\E\left[\left(\mbox{Bias}\left(W\right)\right)^{2}\right]\\
 & =\Var\left(W\right)+2\left[\E\left[W\E\left[W\right]\right]-\E\left[\theta W\right]-\E\left[\left(\E\left[W\right]\right)^{2}\right]+\E\left[\theta\E\left[W\right]\right]\right]+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\Var\left(W\right)+2\left[\left(\E\left[W\right]\right)^{2}-\theta\E\left[W\right]-\left(\E\left[W\right]\right)^{2}+\theta\E\left[W\right]\right]+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\Var\left(W\right)+2\left(0\right)+\left[\mbox{Bias}\left(W\right)\right]^{2}\\
 & =\Var\left(W\right)+\left[\mbox{Bias}\left(W\right)\right]^{2}.
\end{flalign*}

\end_inset

If 
\begin_inset Formula $W$
\end_inset

 is unbiased, 
\begin_inset Formula $\text{MSE}\left[W\right]=\Var\left(W\right)$
\end_inset

.
 An estimator that has good MSE properties has small combined variance and
 bias.
 Controlling bias does not guarantee that MSE is controlled.
 There is often a bias-variance trade-off, such that a small increase in
 bias can be traded for a larger decrease in variance, resulting in an improveme
nt in MSE.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid 
\begin_inset Formula $\text{Bernoulli}\left(\theta\right)$
\end_inset

.
 Consider the statistics
\begin_inset Formula 
\[
T_{1}=\frac{\sum_{i}X_{i}+1}{n+2}\quad\mbox{and}\quad T_{2}=\frac{\sum_{i}X_{i}}{n}.
\]

\end_inset

Find the MSE for 
\begin_inset Formula $T_{1}$
\end_inset

 and 
\begin_inset Formula $T_{2}$
\end_inset

.
\end_layout

\begin_layout Example
Noting that the expected value of a 
\begin_inset Formula $\text{Bernoulli}\left(p\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $p$
\end_inset

, we will begin by finding the expected value of 
\begin_inset Formula $T_{1}$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\E\left[T_{1}\right] & =\E\left[\frac{\sum_{i}X_{i}+1}{n+2}\right]\\
 & =\frac{1}{n+2}\E\left[\sum_{i=1}^{n}X_{i}+1\right]\\
 & =\frac{1}{n+2}\left(\sum_{i=1}^{n}\E\left[X_{i}\right]+1\right)\\
 & =\frac{1}{n+2}\left(\sum_{i=1}^{n}\theta+1\right)\\
 & =\frac{n\theta+1}{n+2}.
\end{flalign*}

\end_inset

Then, the bias of 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\[
\text{Bias}\left(T_{1}\right)=\E\left[T_{1}\right]-\theta=\frac{n\theta+1}{n+2}-\frac{\theta\left(n+2\right)}{n+2}=\frac{n\theta+1-n\theta-2\theta}{n+2}=\frac{1-2\theta}{n+2}.
\]

\end_inset

Noting that the variance of a 
\begin_inset Formula $\text{Bernoulli}\left(p\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $p\left(1-p\right)$
\end_inset

, it follows that the variance of 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(T_{1}\right) & =\Var\left(\frac{\sum_{i}X_{i}+1}{n+2}\right)\\
 & =\left(\frac{1}{n+2}\right)^{2}\Var\left(\sum_{i=1}^{n}X_{i}+1\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\Var\left(\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\sum_{i=1}^{n}\Var\left(X_{i}\right)\\
 & =\frac{1}{\left(n+2\right)^{2}}\sum_{i=1}^{n}\theta\left(1-\theta\right)\\
 & =\frac{n\theta\left(1-\theta\right)}{\left(n+2\right)^{2}},
\end{flalign*}

\end_inset

where the fourth equality follows from the independence of the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

.
 Then, the MSE for 
\begin_inset Formula $T_{1}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\text{MSE}\left(T_{1}\right) & =\Var\left(T_{1}\right)+\left[\text{Bias}\left(T_{1}\right)\right]^{2}\\
 & =\frac{n\theta\left(1-\theta\right)}{\left(n+2\right)^{2}}+\left[\frac{1-2\theta}{n+2}\right]^{2}\\
 & =\frac{n\theta\left(1-\theta\right)+\left(1-2\theta\right)^{2}}{\left(n+2\right)^{2}}\\
 & =\frac{n\theta-n\theta^{2}+\left(1-4\theta+4\theta^{2}\right)}{\left(n+2\right)^{2}}\\
 & =\frac{\theta^{2}\left(4-n\right)-\theta\left(4-n\right)+1}{\left(n+2\right)^{2}}.
\end{flalign*}

\end_inset

The expected value of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\[
\E\left[T_{2}\right]=\E\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}\right]=\frac{1}{n}\sum_{i=1}^{n}\E\left[X_{i}\right]=\frac{1}{n}\sum_{i=1}^{n}\theta=\frac{n\theta}{n}=\theta,
\]

\end_inset

so that we have 
\begin_inset Formula 
\[
\text{Bias}\left(T_{2}\right)=\E\left[T_{2}\right]-\theta=\theta-\theta=0.
\]

\end_inset

The variance of 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(T_{2}\right) & =\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\Var\left(X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\theta\left(1-\theta\right)\\
 & =\frac{n\theta\left(1-\theta\right)}{n^{2}}\\
 & =\frac{\theta\left(1-\theta\right)}{n},
\end{flalign*}

\end_inset

so that the MSE for 
\begin_inset Formula $T_{2}$
\end_inset

 is given by
\begin_inset Formula 
\[
\text{MSE}\left(T_{2}\right)=\Var\left(T_{2}\right)+\left[\text{Bias}\left(T_{2}\right)^{2}\right]=\frac{\theta\left(1-\theta\right)}{n}+0=\frac{\theta\left(1-\theta\right)}{n}.
\]

\end_inset


\end_layout

\begin_layout Remark
The MSEs may differ based on how large 
\begin_inset Formula $n$
\end_inset

 is, i.e., one may be superior for certain ranges of 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Subsection
Best unbiased estimators
\end_layout

\begin_layout Standard
A common way to make the problem of finding a 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 estimator tractable is to limit the class of estimators to unbiased estimators.
\end_layout

\begin_layout Definition
An estimator 
\begin_inset Formula $W^{*}$
\end_inset

 is a 
\shape italic
best unbiased estimator
\shape default
 of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 if it satisfies 
\begin_inset Formula $\E_{\theta}\left[W^{*}\right]=\tau\left(\theta\right)$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

 and, for any other estimator 
\begin_inset Formula $W$
\end_inset

 with 
\begin_inset Formula $\E_{\theta}\left[W\right]=\tau\left(\theta\right)$
\end_inset

, we have 
\begin_inset Formula $\mbox{Var}_{\theta}\left(W^{*}\right)\leq\mbox{Var}_{\theta}\left(W\right)$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

.
 
\begin_inset Formula $W^{*}$
\end_inset

 is also called a 
\shape italic
uniform minimum variance unbiased estimator
\shape default
 (UMVUE) of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Cramér-Rao Inequality]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:cramér-rao"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be a sample with pdf 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

, and let 
\begin_inset Formula $W\left(\mathbf{X}\right)=W\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 be any estimator satisfying
\begin_inset Formula 
\[
\frac{\dif}{\dif\theta}\E_{\theta}\left[W\left(\mathbf{X}\right)\right]=\int_{\mathcal{X}}\frac{\partial}{\partial\theta}\left[W\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta\right)\right]\dif\mathbf{x}\quad\mbox{and}\quad\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)<\infty.
\]

\end_inset

Then
\begin_inset Formula 
\[
\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\frac{\dif}{\dif\theta}\E_{\theta}\left[W\left(\mathbf{X}\right)\right]\right]^{2}}{\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}=\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}
\]

\end_inset

with Fisher information
\begin_inset Formula 
\[
I=\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]=-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right].
\]

\end_inset

(This is Theorem 7.3.9 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula 
\[
U=W\left(\mathbf{X}\right)\quad\text{and}\quad V=\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right).
\]

\end_inset

Then, from the definition of correlation, we have
\begin_inset Formula 
\[
\Corr\left(U,V\right)=\frac{\Cov\left(U,V\right)}{\sqrt{\Var\left(U\right)}\sqrt{\Var\left(V\right)}}.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:correlation-values"

\end_inset

, it follows that
\begin_inset Formula 
\[
0\leq\left[\Corr\left(U,V\right)\right]^{2}=\frac{\left[\Cov\left(U,V\right)\right]^{2}}{\Var\left(U\right)\Var\left(V\right)}\leq1,
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\left[\Cov\left(U,V\right)\right]^{2}\leq\Var\left(U\right)\Var\left(V\right)\implies\Var\left(U\right)\geq\frac{\left[\Cov\left(U,V\right)\right]^{2}}{\Var\left(V\right)}.
\]

\end_inset

Then,
\begin_inset Formula 
\begin{flalign*}
\Cov\left(U,V\right) & =\E\left[\left(U-\mu_{U}\right)\left(V-\mu_{V}\right)\right]\\
 & =\E\left[UV-\mu_{V}U-\mu_{U}V+\mu_{U}\mu_{V}\right]\\
 & =\E\left[UV\right]-\mu_{V}\E\left[U\right]-\mu_{U}\E\left[V\right]+\mu_{U}\mu_{V}\\
 & =\E\left[UV\right]-\E\left[V\right]\E\left[U\right]-\E\left[U\right]\E\left[V\right]+\E\left[U\right]\E\left[V\right]\\
 & =\E\left[UV\right]-\E\left[V\right]\E\left[U\right]\\
 & =\E\left[W\left(\mathbf{X}\right)\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right]-\E\left[W\left(\mathbf{X}\right)\right]\E\left[\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right]\\
 & =\int_{\mathcal{X}}W\left(\mathbf{X}\right)\left[\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right]f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}-\E\left[W\left(\mathbf{X}\right)\right]\int_{\mathcal{X}}\left[\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right]f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}\\
 & =\int_{\mathcal{X}}W\left(\mathbf{X}\right)\left[\frac{1}{f\left(\mathbf{x}|\theta\right)}\frac{\partial}{\partial\theta}f\left(\mathbf{x}|\theta\right)\right]f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}-\E\left[W\left(\mathbf{X}\right)\right]\int_{\mathcal{X}}\left[\frac{1}{f\left(\mathbf{x}|\theta\right)}\frac{\partial}{\partial\theta}f\left(\mathbf{x}|\theta\right)\right]f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}\\
 & =\int_{\mathcal{X}}W\left(\mathbf{X}\right)\left[\frac{\partial}{\partial\theta}f\left(\mathbf{x}|\theta\right)\right]\text{d}\mathbf{x}-\E\left[W\left(\mathbf{X}\right)\right]\int_{\mathcal{X}}\left[\frac{\partial}{\partial\theta}f\left(\mathbf{x}|\theta\right)\right]\text{d}\mathbf{x}\\
 & =\frac{\partial}{\partial\theta}\int_{\mathcal{X}}W\left(\mathbf{X}\right)f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}-\E\left[W\left(\mathbf{X}\right)\right]\frac{\partial}{\partial\theta}\int_{\mathcal{X}}f\left(\mathbf{x}|\theta\right)\text{d}\mathbf{x}\\
 & =\frac{\partial}{\partial\theta}\E\left[W\left(\mathbf{X}\right)\right]-\E\left[W\left(\mathbf{X}\right)\right]\frac{\partial}{\partial\theta}1\\
 & =\frac{\partial}{\partial\theta}\E\left[W\left(\mathbf{X}\right)\right]-\E\left[W\left(\mathbf{X}\right)\right]\cdot0\\
 & =\frac{\partial}{\partial\theta}\E\left[W\left(\mathbf{X}\right)\right]\\
 & =\tau'\left(\theta\right).
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\Var\left(V\right) & =\Var\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]-\left(\E\left[\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right]\right)^{2}\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]-0\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right],
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
The Cramér-Rao lower bound gives the smallest variance that can be attained
 for an unbiased estimator provided the conditions of the theorem are met.
 Under independence, 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f_{i}\left(x_{i}|\theta\right)$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right] & =\E\left[\left(\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f_{i}\left(x_{i}|\theta\right)\right)^{2}\right]\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\sum_{i=1}^{n}\log f_{i}\left(x_{i}|\theta\right)\right)^{2}\right]\\
 & =\E\left[\left(\sum_{i=1}^{n}\frac{\partial}{\partial\theta}\log f_{i}\left(x_{i}|\theta\right)\right)^{2}\right]\\
 & =\sum_{i=1}^{n}\E\left[\left(\frac{\partial}{\partial\theta}\log f_{i}\left(x_{i}|\theta\right)\right)^{2}\right],
\end{flalign*}

\end_inset

where the third quality follows from the linearity of differentiation and
 the final equality from the linearity of expected value.
 If the 
\begin_inset Formula $X_{i}\mbox{'s}$
\end_inset

 are also identically distributed, i.e., they are iid, then we have
\begin_inset Formula 
\[
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]=\sum_{i=1}^{n}\E\left[\left(\frac{\partial}{\partial\theta}\log f_{i}\left(x_{i}|\theta\right)\right)^{2}\right]=n\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(x|\theta\right)\right)^{2}\right],
\]

\end_inset

and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cramér-rao"

\end_inset

 reduces to
\begin_inset Formula 
\[
\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{n\E_{\theta}\left[\left(\frac{\partial}{\partial\theta}\log f\left(x|\theta\right)\right)^{2}\right]}
\]

\end_inset

for all estimators 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 that are unbiased for 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Exponential CRLB]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\text{Exp}\left(\theta\right)$
\end_inset

, i.e.,
\begin_inset Formula 
\[
f\left(x|\theta\right)=\theta e^{-\theta x},\quad x>0,\quad\theta>0.
\]

\end_inset

Give the Cramér-Rao lower bound for any unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, where
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:minimal-sufficient-exp-rv"

\end_inset

, the joint density of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f\left(\mathbf{x}|\theta\right)=\theta^{n}e^{-\theta\sum_{i=1}^{n}x_{i}},
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\log f\left(\mathbf{x}|\theta\right)=\log\theta^{n}e^{-\theta\sum_{i=1}^{n}x_{i}}=n\log\theta-\theta\sum_{i=1}^{n}x_{i}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right) & =\frac{n}{\theta}-\sum_{i=1}^{n}x_{i},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right] & =\E\left[\frac{n^{2}}{\theta^{2}}-2\frac{n}{\theta}\sum_{i=1}^{n}x_{i}+\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right]\\
 & =\E\left[\frac{n^{2}}{\theta^{2}}\right]-\E\left[2\frac{n}{\theta}\sum_{i=1}^{n}x_{i}\right]+\E\left[\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right]\\
 & =\frac{n^{2}}{\theta^{2}}-\frac{2n}{\theta}\E\left[\sum_{i=1}^{n}x_{i}\right]+\E\left[\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right].
\end{flalign*}

\end_inset

An exponential random variable with parameter 
\begin_inset Formula $\lambda$
\end_inset

 is a gamma random variable with parameters 
\begin_inset Formula $\left(1,\lambda\right)$
\end_inset

.
 The sum of 
\begin_inset Formula $n$
\end_inset

 independent gamma random variables 
\begin_inset Formula $W_{i}\sim\Gamma\left(k_{i},\lambda\right)$
\end_inset

 has the distribution of a 
\begin_inset Formula $\Gamma\left(\sum_{i=1}^{n}k_{i},\lambda\right)$
\end_inset

 random variable.
 We have 
\begin_inset Formula $X_{i}\sim\text{Exp}\left(\theta\right)$
\end_inset

, so it follows that 
\begin_inset Formula $X_{i}\sim\Gamma\left(1,\theta\right)$
\end_inset

 and that 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\sim\Gamma\left(n,\theta\right)$
\end_inset

.
 The expectation of a 
\begin_inset Formula $\Gamma\left(\alpha,\beta\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\alpha/\beta$
\end_inset

 , so it follows that 
\begin_inset Formula 
\[
\E\left[\sum_{i=1}^{n}x_{i}\right]=\frac{n}{\theta}.
\]

\end_inset

The variance of a 
\begin_inset Formula $\Gamma\left(\alpha,\beta\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\alpha/\beta^{2}$
\end_inset

, so it follows that
\begin_inset Formula 
\begin{flalign*}
\frac{n}{\theta^{2}} & =\Var\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =\E\left[\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right]-\E\left[\sum_{i=1}^{n}x_{i}\right]^{2}\\
 & =\E\left[\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right]-\left(\frac{n}{\theta}\right)^{2}\\
\implies\E\left[\left(\sum_{i=1}^{n}x_{i}\right)^{2}\right] & =\frac{n}{\theta^{2}}+\frac{n^{2}}{\theta^{2}}\\
 & =\frac{n+n^{2}}{\theta^{2}}.
\end{flalign*}

\end_inset

Then, we have
\begin_inset Formula 
\[
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]=\frac{n^{2}}{\theta^{2}}-\frac{2n}{\theta}\cdot\frac{n}{\theta}+\frac{n+n^{2}}{\theta^{2}}=\frac{n}{\theta^{2}}.
\]

\end_inset

We could also have taken the second derivative of 
\begin_inset Formula $\ln f\left(x|\theta\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 to obtain
\begin_inset Formula 
\[
\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(x|\theta\right)\right]=\E\left[\frac{\partial}{\partial\theta}\left(\frac{n}{\theta}-\sum_{i=1}^{n}x_{i}\right)\right]=\E\left[-\frac{n}{\theta^{2}}\right]=-\frac{n}{\theta^{2}}.
\]

\end_inset

Then, using the fact that 
\begin_inset Formula 
\[
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(x|\theta\right)\right)^{2}\right]=-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(x|\theta\right)\right],
\]

\end_inset

we have
\begin_inset Formula 
\[
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(x|\theta\right)\right)^{2}\right]=-\left(-\frac{n}{\theta^{2}}\right)=\frac{n}{\theta^{2}}.
\]

\end_inset

We have 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

, so that 
\begin_inset Formula $\tau'\left(\theta\right)=1$
\end_inset

.
 Then, the Cramér-Rao lower bound is given by
\begin_inset Formula 
\[
\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]}=\frac{1^{2}}{\dfrac{n}{\theta^{2}}}=\frac{\theta^{2}}{n}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\tau\left(\theta\right)=\frac{1}{\theta}$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We have 
\begin_inset Formula $\tau'\left(\theta\right)=-\theta^{-2}$
\end_inset

, so the Cramér-Rao lower bound is given by
\begin_inset Formula 
\[
\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)\right)^{2}\right]}=\frac{\left(-\theta^{-2}\right)^{2}}{n\theta^{-2}}=\frac{\theta^{-4}}{n\theta^{-2}}=\frac{1}{n\theta^{2}}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mle-exp-rv"

\end_inset

, we found that the MLE of 
\begin_inset Formula $\theta$
\end_inset

 was given by 
\begin_inset Formula $\hat{\theta}=\bar{X}$
\end_inset

.
 Using the parameterization for 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 given here, we have 
\begin_inset Formula $\hat{\theta}=1/\bar{X}$
\end_inset

.
 Is 
\begin_inset Formula $\hat{\theta}$
\end_inset

 a best unbiased estimator for 
\begin_inset Formula $\tau\left(\theta\right)=1/\theta$
\end_inset

?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We must first show that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is unbiased.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:invariance-mle"

\end_inset

, we have 
\begin_inset Formula 
\[
\widehat{\tau\left(\theta\right)}=\frac{1}{\hat{\theta}}=\frac{1}{\frac{1}{\bar{X}}}=\bar{X},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\E\left[\bar{X}\right]=\E\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}\right]=\frac{1}{n}\E\left[\sum_{i=1}^{n}X_{i}\right]=\frac{1}{n}\left(\frac{n}{\theta}\right)=\frac{1}{\theta}=\tau\left(\theta\right).
\]

\end_inset

It follows that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is an unbiased estimator for 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
 Then, the variance of 
\begin_inset Formula $\widehat{\tau\left(\theta\right)}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(\bar{X}\right)=\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var\left(X_{i}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{1}{\theta^{2}}=\frac{1}{n^{2}}\left(\frac{n}{\theta^{2}}\right)=\frac{1}{n\theta^{2}},
\]

\end_inset

where the third equality follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:variance-of-ind-rvs"

\end_inset

.
 We see that the variance of 
\begin_inset Formula $\widehat{\tau\left(\theta\right)}$
\end_inset

 is equal to the Cramér-Rao lower bound found above, so it follows that
 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a best unbiased estimator (UMVUE) for 
\begin_inset Formula $\tau\left(\theta\right)=1/\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Is the MLE of 
\begin_inset Formula $\theta$
\end_inset

 a best unbiased estimator for 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We have 
\begin_inset Formula $\widehat{\tau\left(\theta\right)}=\hat{\theta}=1/\bar{X}$
\end_inset

, so that the variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(\hat{\theta}\right) & =\E\left[\hat{\theta}^{2}\right]-\left(\E\left[\hat{\theta}\right]\right)^{2}\\
 & =\E\left[\left(\frac{1}{\bar{X}}\right)^{2}\right]-\left(\E\left[\frac{1}{\bar{X}}\right]\right)^{2}\\
 & =\E\left[\frac{1}{\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)^{2}}\right]-\left(\E\left[\frac{1}{\frac{1}{n}\sum_{i=1}^{n}X_{i}}\right]\right)^{2}\\
 & =\E\left[\frac{n^{2}}{\left(\sum_{i=1}^{n}X_{i}\right)^{2}}\right]-\left(\E\left[\frac{n}{\sum_{i=1}^{n}X_{i}}\right]\right)^{2}\\
 & =n^{2}\E\left[\frac{1}{\left(\sum_{i=1}^{n}X_{i}\right)^{2}}\right]-\left(n\E\left[\frac{1}{\sum_{i=1}^{n}X_{i}}\right]\right)^{2}\\
 & =n^{2}\E\left[\frac{1}{\left(\sum_{i=1}^{n}X_{i}\right)^{2}}\right]-n^{2}\left(\E\left[\frac{1}{\sum_{i=1}^{n}X_{i}}\right]\right)^{2}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $U=\sum_{i=1}^{n}X_{i}\sim\Gamma\left(n,\theta\right)$
\end_inset

, so that the pdf of 
\begin_inset Formula $U$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{U}\left(u|n,\theta\right)=\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\qquad u>0,
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\E\left[\frac{1}{\bar{X}}\right]=n\E\left[\frac{1}{U}\right]\quad\text{and}\quad\E\left[\left(\frac{1}{\bar{X}^{2}}\right)\right]=n^{2}\E\left[\frac{1}{U^{2}}\right].
\]

\end_inset

Noting that 
\begin_inset Formula $\Gamma\left(n\right)=\left(n-1\right)\Gamma\left(n-1\right)$
\end_inset

, the expected value of 
\begin_inset Formula $1/U$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
\E\left[\frac{1}{U}\right] & =\int_{0}^{\infty}\frac{1}{u}\cdot f_{U}\left(u|n,\theta\right)\dif u\\
 & =\int_{0}^{\infty}\frac{1}{u}\cdot\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\int_{0}^{\infty}\frac{\theta^{n}}{\Gamma\left(n\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\int_{0}^{\infty}\frac{\theta\cdot\theta^{n-1}}{\left(n-1\right)\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\frac{\theta}{n-1}\int_{0}^{\infty}\frac{\theta^{n-1}}{\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\Gamma\left(n-1,\theta\right)$
\end_inset

 random variable, so that we have
\begin_inset Formula 
\[
\E\left[\frac{1}{U}\right]=\frac{\theta}{n-1}\int_{0}^{\infty}\frac{\theta^{n-1}}{\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u=\frac{\theta}{n-1}\cdot1=\frac{\theta}{n-1}.
\]

\end_inset

Then, the expected value of 
\begin_inset Formula $1/U^{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[\frac{1}{U^{2}}\right] & =\int_{0}^{\infty}\frac{1}{u^{2}}\cdot f_{U}\left(u|n\theta\right)\dif u\\
 & =\int_{0}^{\infty}\frac{1}{u^{2}}\cdot\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\int_{0}^{\infty}\frac{\theta^{2}\cdot\theta^{n-2}}{\left(n-1\right)\Gamma\left(n-1\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\int_{0}^{\infty}\frac{\theta^{2}\cdot\theta^{n-2}}{\left(n-1\right)\left(n-2\right)\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\frac{\theta^{2}}{\left(n-1\right)\left(n-2\right)}\int_{0}^{\infty}\frac{\theta^{n-2}}{\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\Gamma\left(n-2,\theta\right)$
\end_inset

 random variable, so that we have
\begin_inset Formula 
\begin{flalign*}
\E\left[\frac{1}{U^{2}}\right] & =\frac{\theta^{2}}{\left(n-1\right)\left(n-2\right)}\int_{0}^{\infty}\frac{\theta^{n-2}}{\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\frac{\theta^{2}}{\left(n-1\right)\left(n-2\right)}\cdot1\\
 & =\frac{\theta^{2}}{\left(n-1\right)\left(n-2\right)}.
\end{flalign*}

\end_inset

Then, the variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(\hat{\theta}\right) & =n^{2}\E\left(\frac{1}{U^{2}}\right)-n^{2}\left(\E\left[\frac{1}{U}\right]\right)^{2}\\
 & =n^{2}\left(\frac{\theta^{2}}{\left(n-1\right)\left(n-2\right)}\right)-n^{2}\left(\frac{\theta}{n-1}\right)^{2}\\
 & =\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}-\frac{n^{2}\theta^{2}}{\left(n-1\right)^{2}}\\
 & =\frac{n^{2}\theta^{2}\left(n-1\right)-n^{2}\theta^{2}\left(n-2\right)}{\left(n-1\right)^{2}\left(n-2\right)}\\
 & =\frac{n^{2}\theta^{2}\left[\left(n-1\right)-\left(n-2\right)\right]}{\left(n-1\right)^{2}\left(n-2\right)}\\
 & =\frac{n^{2}\theta^{2}\left(n-1-n+2\right)}{\left(n-1\right)^{2}\left(n-2\right)}\\
 & =\frac{n^{2}\theta^{2}}{\left(n-1\right)^{2}\left(n-2\right)}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
We will check whether the variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 attains the Cramér-Rao Lower Bound for the variance of an estimator of
 
\begin_inset Formula $\theta$
\end_inset

, which we found above to be 
\begin_inset Formula $\theta^{2}/n$
\end_inset

.
\begin_inset Formula 
\[
\Var\left(\hat{\theta}\right)>\frac{\theta^{2}}{n}\implies\frac{n^{2}\theta^{2}}{\left(n-1\right)^{2}\left(n-2\right)}-\frac{\theta^{2}}{n}>0\implies\frac{n^{2}}{\left(n-1\right)^{2}\left(n-2\right)}-\frac{1}{n}=\frac{n^{3}-\left(n-1\right)^{2}\left(n-2\right)}{n\left(n-1\right)^{2}\left(n-2\right)}>0
\]

\end_inset

We will expand the expression 
\begin_inset Formula $\left(n-1\right)^{2}\left(n-2\right)$
\end_inset

 as
\begin_inset Formula 
\begin{flalign*}
\left(n-1\right)^{2}\left(n-2\right) & =\left(n^{2}-2n+1\right)\left(n-2\right)\\
 & =\left(n^{2}-\left(2n-1\right)\right)\left(n-2\right)\\
 & =n^{3}-2n^{2}-n\left(2n-1\right)+2\left(2n-1\right)\\
 & =n^{3}-2n^{2}-2n^{2}+n+4n-2\\
 & =n^{3}-4n^{2}+5n-2,
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
\frac{n^{3}-\left(n^{3}-4n^{2}+5n-2\right)}{n\left(n^{3}-4n^{2}+5n-2\right)} & >0\\
\frac{4n^{2}-5n+2}{n\left(n-1\right)^{2}\left(n-2\right)} & >0.
\end{flalign*}

\end_inset

This expression will be defined if and only if the denominator is non-zero.
 By definition, we have 
\begin_inset Formula $n\geq1$
\end_inset

.
 In the case that 
\begin_inset Formula $n=1$
\end_inset

 or 
\begin_inset Formula $n=2$
\end_inset

, the denominator will be equal to zero, so the expression will be defined
 if and only if 
\begin_inset Formula $n>2\Leftrightarrow n\geq3$
\end_inset

.
 In the case that 
\begin_inset Formula $n=3$
\end_inset

, the numerator is equal to 
\begin_inset Formula $36-15+2=23>0$
\end_inset

, and as 
\begin_inset Formula $n$
\end_inset

 increases, we see that 
\begin_inset Formula $4n^{2}-5n+2>0$
\end_inset

, so that the numerator is positive.
 Thus, we conclude that the variance of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is greater than the Cramér-Rao Lower Bound, so 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is not a best unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Beta CRLB]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 denote a random sample of size 
\begin_inset Formula $n>2$
\end_inset

 from a 
\begin_inset Formula $\text{Beta}\left(\theta,1\right)$
\end_inset

 distribution, i.e., 
\begin_inset Formula 
\[
f\left(x|\theta\right)=\theta x^{\theta-1},\quad0<x<1,\quad\theta>0.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The joint density of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\theta x_{i}^{\theta-1}=\theta^{n}\prod_{i=1}^{n}x_{i}^{\theta-1},
\]

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log L\left(\theta|\mathbf{x}\right) & =\log\theta^{n}\prod_{i=1}^{n}x_{i}^{\theta-1}\\
 & =n\log\theta+\log\prod_{i=1}^{n}x_{i}^{\theta-1}\\
 & =n\log\theta+\sum_{i=1}^{n}\log x_{i}^{\theta-1}\\
 & =n\log\theta+\left(\theta-1\right)\sum_{i=1}^{n}\log x_{i}.
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)=\frac{\partial}{\partial\theta}\left[n\log\theta+\left(\theta-1\right)\sum_{i=1}^{n}\log x_{i}\right]=\frac{n}{\theta}+\sum_{i=1}^{n}\log x_{i}.
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{n}{\hat{\theta}}=-\sum_{i=1}^{n}\log x_{i}\implies\hat{\theta}=-\frac{n}{\sum_{i=1}^{n}\log x_{i}}.
\]

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)\Bigr\rvert_{\theta=\hat{\theta}} & =\frac{\partial}{\partial\theta}\left[\frac{n}{\theta}+\sum_{i=1}^{n}\log x_{i}\right|_{\theta=\hat{\theta}}\\
 & =\left[-\frac{n}{\theta^{2}}\right|_{\theta=\hat{\theta}}\\
 & =-\frac{n}{\left(-\frac{n}{\sum_{i=1}^{n}\log x_{i}}\right)^{2}}\\
 & =-\frac{n}{\frac{n^{2}}{\left(\sum_{i=1}^{n}\log x_{i}\right)^{2}}}\\
 & =-\frac{\left(\sum_{i=1}^{n}\log x_{i}\right)^{2}}{n}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n\geq1$
\end_inset

, so that the denominator is positive.
 We have 
\begin_inset Formula $0<x<1$
\end_inset

, so that 
\begin_inset Formula $\log x<0$
\end_inset

, so that 
\begin_inset Formula $\sum_{i=1}^{n}\log x_{i}<0$
\end_inset

, so that 
\begin_inset Formula $\left(\sum_{i=1}^{n}\log x_{i}\right)^{2}>0$
\end_inset

.
 Both the numerator and the denominator are positive, so it follows that
 the expression is negative, so that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE.
\end_layout

\end_deeper
\begin_layout Enumerate
Find an unbiased estimator based on the MLE.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We have
\begin_inset Formula 
\[
\E\left[\hat{\theta}\right]=\E\left[-\frac{n}{\sum_{i=1}^{n}\log X_{i}}\right]=\E\left[\frac{n}{\sum_{i=1}^{n}-\log X_{i}}\right].
\]

\end_inset

Let 
\begin_inset Formula $Y=-\log X$
\end_inset

, so that 
\begin_inset Formula 
\[
Y=-\log X\implies-Y=\log X\implies\mathrm{e}^{-Y}=\mathrm{e}^{\log X}\implies X=\mathrm{e}^{-Y}.
\]

\end_inset

Noting that 
\begin_inset Formula $Y=-\log X$
\end_inset

 is monotone, we will find a pdf for 
\begin_inset Formula $Y$
\end_inset

 using 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:pdf-of-function-of-rv"

\end_inset

.
 We have 
\begin_inset Formula 
\[
\mathcal{Y}=\left\{ y:y=-\log x,0<x<1\right\} =\left\{ y:y>0\right\} 
\]

\end_inset

and
\begin_inset Formula 
\[
f_{Y}\left(y\right)=f_{X}\left(\mathrm{e}^{-y}\right)\left|\frac{\dif}{\dif y}\mathrm{e}^{-y}\right|=\theta\left(\mathrm{e}^{-y}\right)^{\theta-1}\left|-\mathrm{e}^{-y}\right|=\theta\mathrm{e}^{y-\theta y}\mathrm{e}^{-y}=\theta\mathrm{e}^{-\theta y},
\]

\end_inset

which is the pdf of an 
\begin_inset Formula $\text{Exp}\left(\theta\right)$
\end_inset

 random variable, i.e., 
\begin_inset Formula $Y\sim\text{Exp}\left(\theta\right)$
\end_inset

.
 Let 
\begin_inset Formula 
\[
U=\sum_{i=1}^{n}-\log X_{i}=\sum_{i=1}^{n}Y_{i}\sim\Gamma\left(n,\theta\right),
\]

\end_inset

i.e., 
\begin_inset Formula $U$
\end_inset

 is a 
\begin_inset Formula $\text{Gamma}\left(n,\theta\right)$
\end_inset

 random variable with pdf
\begin_inset Formula 
\[
f_{U}\left(u|n,\theta\right)=\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\qquad u>0.
\]

\end_inset

Noting that 
\begin_inset Formula $\Gamma\left(n\right)=\left(n-1\right)\Gamma\left(n-1\right)$
\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
\E\left[\hat{\theta}\right] & =\E\left[\frac{n}{\sum_{i=1}^{n}-\log X_{i}}\right]\\
 & =\E\left[\frac{n}{U}\right]\\
 & =n\E\left[\frac{1}{U}\right]\\
 & =n\int_{0}^{\infty}\frac{1}{u}\cdot f_{U}\left(u|n,\theta\right)\dif u\\
 & =n\int_{0}^{\infty}\frac{1}{u}\cdot\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\dif u\\
 & =n\int_{0}^{\infty}\frac{\theta\cdot\theta^{n-1}}{\left(n-1\right)\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\frac{n\theta}{n-1}\int_{0}^{\infty}\frac{\theta^{n-1}}{\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\Gamma\left(n-1,\theta\right)$
\end_inset

 random variable, so that we have
\begin_inset Formula 
\[
\E\left[\hat{\theta}\right]=\frac{n\theta}{n-1}\int_{0}^{\infty}\frac{\theta^{n-1}}{\Gamma\left(n-1\right)}u^{\left(n-1\right)-1}\mathrm{e}^{-\theta u}\dif u=\frac{n\theta}{n-1}.
\]

\end_inset

It follows that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a biased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
 We can construct an unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

 based on 
\begin_inset Formula $\hat{\theta}$
\end_inset

 by multiplying 
\begin_inset Formula $\hat{\theta}$
\end_inset

 by 
\begin_inset Formula $\left(n-1\right)/n$
\end_inset

, i.e., let 
\begin_inset Formula 
\[
\tilde{\theta}=\frac{n-1}{n}\hat{\theta},\quad\text{so that}\quad\E\left[\tilde{\theta}\right]=\E\left[\frac{n-1}{n}\hat{\theta}\right]=\frac{n-1}{n}\E\left[\hat{\theta}\right]=\frac{n-1}{n}\cdot\frac{n\theta}{n-1}=\theta.
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Enumerate
Does the unbiased estimator from (b) attain the Cramér-Rao Lower Bound?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The Cramér-Rao Lower Bound for the variance of an estimator 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 of 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]}.
\]

\end_inset

We have 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

, so that 
\begin_inset Formula $\tau'\left(\theta\right)=1$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]=\E\left[-\frac{n}{\theta^{2}}\right]=-\frac{n}{\theta^{2}},
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{1}{-\left(-\frac{n}{\theta^{2}}\right)}=\frac{\theta^{2}}{n}.
\]

\end_inset

The variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(\tilde{\theta}\right)=\Var\left(\frac{n-1}{n}\hat{\theta}\right)=\left(\frac{n-1}{n}\right)^{2}\Var\left(\hat{\theta}\right)=\frac{\left(n-1\right)^{2}}{n^{2}}\left(\E\left[\hat{\theta}^{2}\right]-\left(\E\left[\hat{\theta}\right]\right)^{2}\right).
\]

\end_inset

The expected value of 
\begin_inset Formula $\hat{\theta}^{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[\hat{\theta}^{2}\right] & =\E\left[\left(-\frac{n}{\sum_{i=1}^{n}\log X_{i}}\right)^{2}\right]\\
 & =\E\left[\left(\frac{n}{\sum_{i=1}^{n}-\log X_{i}}\right)^{2}\right]\\
 & =n^{2}\E\left[\frac{1}{U^{2}}\right]\\
 & =n^{2}\int_{0}^{\infty}\frac{1}{u^{2}}\cdot f_{U}\left(u|n,\theta\right)\dif u\\
 & =n^{2}\int_{0}^{\infty}\frac{1}{u^{2}}\cdot\frac{\theta^{n}}{\Gamma\left(n\right)}u^{n-1}\mathrm{e}^{-\theta u}\dif u\\
 & =n^{2}\int_{0}^{\infty}\frac{\theta^{2}\cdot\theta^{n-2}}{\left(n-1\right)\Gamma\left(n-1\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =n^{2}\int_{0}^{\infty}\frac{\theta^{2}\cdot\theta^{n-2}}{\left(n-1\right)\left(n-2\right)\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u\\
 & =\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}\int_{0}^{\infty}\frac{\theta^{n-2}}{\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\text{Gamma}\left(n-2,\theta\right)$
\end_inset

 random variable, so that we have
\begin_inset Formula 
\[
\E\left[\hat{\theta}^{2}\right]=\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}\int_{0}^{\infty}\frac{\theta^{n-2}}{\Gamma\left(n-2\right)}u^{\left(n-2\right)-1}\mathrm{e}^{-\theta u}\dif u=\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}.
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\Var\left(\tilde{\theta}\right) & =\frac{\left(n-1\right)^{2}}{n^{2}}\left(\E\left[\hat{\theta}^{2}\right]-\left(\E\left[\hat{\theta}\right]\right)^{2}\right)\\
 & =\frac{\left(n-1\right)^{2}}{n^{2}}\left(\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}-\left(\frac{n\theta}{n-1}\right)^{2}\right)\\
 & =\frac{\left(n-1\right)^{2}}{n^{2}}\left(\frac{n^{2}\theta^{2}}{\left(n-1\right)\left(n-2\right)}-\frac{n^{2}\theta^{2}}{\left(n-1\right)^{2}}\right)\\
 & =\frac{\left(n-1\right)^{2}}{n^{2}}\left(\frac{n^{2}\theta^{2}\left(n-1\right)-n^{2}\theta^{2}\left(n-2\right)}{\left(n-1\right)^{2}\left(n-2\right)}\right)\\
 & =\frac{1}{n^{2}}\left(\frac{n^{2}\theta^{2}\left[\left(n-1\right)-\left(n-2\right)\right]}{n-2}\right)\\
 & =\frac{\theta^{2}\left(n-1-n+2\right)}{n-2}\\
 & =\frac{\theta^{2}}{n-2}.
\end{flalign*}

\end_inset

We will check whether the variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 attains the Cramér-Rao Lower Bound.
 Noting that 
\begin_inset Formula $\theta>0$
\end_inset

, we have 
\begin_inset Formula 
\[
\Var\left(\tilde{\theta}\right)>\frac{\theta^{2}}{n}\implies\frac{\theta^{2}}{n-2}>\frac{\theta^{2}}{n}\implies\frac{1}{n-2}>\frac{1}{n}.
\]

\end_inset

By definition, we have 
\begin_inset Formula $n\geq1$
\end_inset

.
 In the case that 
\begin_inset Formula $n=1$
\end_inset

, the inequality will be false.
 In the case that 
\begin_inset Formula $n=2$
\end_inset

, the denominator will be equal to zero, so the expression will be defined
 (and true) if and only if 
\begin_inset Formula $n>2\Leftrightarrow n\geq3$
\end_inset

.
 Thus, we conclude that the variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is greater than the Cramér-Rao Lower Bound, so 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is not a best unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
The corollary below gives a way to find a UMVUE by giving an estimator that
 attains the Cramér-Rao Lower Bound.
\end_layout

\begin_layout Corollary
\begin_inset ERT
status open

\begin_layout Plain Layout

[Attainment]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "cor:cramér-rao-attainment"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 , where 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

 satisfies the conditions of the Cramér-Rao Theorem.
 Let 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)=\prod_{i=1}^{n}f\left(x_{i}|\theta\right)$
\end_inset

 denote the likelihood function.
 If 
\begin_inset Formula $W\left(\mathbf{X}\right)=W\left(X_{1},\ldots,X_{n}\right)$
\end_inset

 is any unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, then 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 attains the Cramér-Rao Lower Bound if and only if
\begin_inset Formula 
\[
a\left(\theta\right)\left[W\left(\mathbf{x}\right)-\tau\left(\theta\right)\right]=\frac{\partial}{\partial\theta}\log\mathcal{L}\left(\theta|\mathbf{x}\right)
\]

\end_inset

for some function 
\begin_inset Formula $a\left(\theta\right)$
\end_inset

.
 (This is Corollary 7.3.15 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
As shown in the proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cramér-rao"

\end_inset

, the Cramér-Rao Inequality can be written as
\begin_inset Formula 
\[
\left[\Cov\left(U,V\right)\right]^{2}\leq\Var\left(U\right)\Var\left(V\right),
\]

\end_inset

where
\begin_inset Formula 
\[
U=W\left(\mathbf{X}\right)\quad\text{and}\quad V=\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right),
\]

\end_inset

i.e., 
\begin_inset Formula 
\[
\left[\Cov_{\theta}\left(W\left(\mathbf{X}\right),\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f\left(X_{i}|\theta\right)\right)\right]^{2}\leq\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)\Var_{\theta}\left(\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f\left(X_{i}|\theta\right)\right).
\]

\end_inset

Recalling that 
\begin_inset Formula $\E_{\theta}\left[W\left(\mathbf{X}\right)\right]=\tau\left(\theta\right)$
\end_inset

 and that 
\begin_inset Formula 
\[
\E_{\theta}\left[\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f\left(X_{i}|\theta\right)\right]=0,
\]

\end_inset

and using the results of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:correlation-values"

\end_inset

, we can have equality if and only if 
\begin_inset Formula $W\left(\mathbf{x}\right)-\tau\left(\theta\right)$
\end_inset

 is proportional to 
\begin_inset Formula $\frac{\partial}{\partial\theta}\log\prod_{i=1}^{n}f\left(x_{i}|\theta\right)$
\end_inset

.
 That is exactly what is expressed in the corollary above.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid with pdf
\begin_inset Formula 
\[
f\left(x|\theta\right)=\frac{\theta}{\left(1+x\right)^{1+\theta}},\quad x>0,\quad\theta>0.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The joint density of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\prod_{i=1}^{n}f\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\frac{\theta}{\left(1+x_{i}\right)^{1+\theta}}=\theta^{n}\prod_{i=1}^{n}\frac{1}{\left(1+x_{i}\right)^{1+\theta}},
\]

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log L\left(\theta|\mathbf{x}\right) & =\log\theta^{n}\prod_{i=1}^{n}\frac{1}{\left(1+x_{i}\right)^{1+\theta}}\\
 & =n\log\theta+\log\prod_{i=1}^{n}\frac{1}{\left(1+x_{i}\right)^{1+\theta}}\\
 & =n\log\theta+\sum_{i=1}^{n}\log\left(1+x_{i}\right)^{-\left(1+\theta\right)}\\
 & =n\log\theta-\left(1+\theta\right)\sum_{i=1}^{n}\log\left(1+x_{i}\right).
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)=\frac{\partial}{\partial\theta}\left[n\log\theta-\left(1+\theta\right)\sum_{i=1}^{n}\log\left(1+x_{i}\right)\right]=\frac{n}{\theta}-\sum_{i=1}^{n}\log\left(1+x_{i}\right).
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{n}{\hat{\theta}}=\sum_{i=1}^{n}\log\left(1+x_{i}\right)\implies\hat{\theta}=\frac{n}{\sum_{i=1}^{n}\log\left(1+x_{i}\right)}.
\]

\end_inset

We will evaluate the second derivative of the log-likelihood at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a maximum.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\theta^{2}}\log L\left(\theta|\mathbf{x}\right)\Bigr\rvert_{\theta=\hat{\theta}} & =\frac{\partial}{\partial\theta}\left[\frac{n}{\theta}-\sum_{i=1}^{n}\log\left(1+x_{i}\right)\right|_{\theta=\hat{\theta}}\\
 & =\left[-\frac{n}{\theta^{2}}\right|_{\theta=\hat{\theta}}\\
 & =-\frac{n}{\left(\frac{n}{\sum_{i=1}^{n}\log\left(1+x_{i}\right)}\right)^{2}}\\
 & =-\frac{n}{\frac{n^{2}}{\left(\sum_{i=1}^{n}\log\left(1+x_{i}\right)\right)^{2}}}\\
 & =-\frac{\left(\sum_{i=1}^{n}\log\left(1+x_{i}\right)\right)^{2}}{n}
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $x>0$
\end_inset

, so that 
\begin_inset Formula $1+x>1$
\end_inset

, so that 
\begin_inset Formula $\log\left(1+x\right)>0$
\end_inset

, so that the numerator is positive.
 We have 
\begin_inset Formula $n\geq1$
\end_inset

, so that the denominator is positive.
 It follows that the expression will be negative, so that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the MLE.
\end_layout

\end_deeper
\begin_layout Enumerate
Find the Cramér-Rao Lower Bound for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The Cramér-Rao Lower Bound for the variance of an estimator 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 of 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]}.
\]

\end_inset

We have 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

, so that 
\begin_inset Formula $\tau'\left(\theta\right)=1$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]=\E\left[-\frac{n}{\theta^{2}}\right]=-\frac{n}{\theta^{2}},
\]

\end_inset

so that we have 
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{1}{-\left(-\frac{n}{\theta^{2}}\right)}=\frac{\theta^{2}}{n}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Find the UMVUE of 
\begin_inset Formula $1/\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We found above that
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right)=\frac{n}{\theta}-\sum_{i=1}^{n}\log\left(1+x_{i}\right),
\]

\end_inset

which we can write as 
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log f\left(\mathbf{x}|\theta\right)=-n\left(-\frac{1}{\theta}+\frac{1}{n}\sum_{i=1}^{n}\log\left(1+x_{i}\right)\right)=\underbrace{-n}_{a\left(\theta\right)}\left(\underbrace{\frac{1}{n}\sum_{i=1}^{n}\log\left(1+x_{i}\right)}_{W\left(\mathbf{x}\right)}-\underbrace{\frac{1}{\theta}}_{\tau\left(\theta\right)}\right).
\]

\end_inset

We will now check whether 
\begin_inset Formula $W\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}\log\left(1+X_{i}\right)$
\end_inset

 is unbiased for 
\begin_inset Formula $1/\theta$
\end_inset

.
\begin_inset Formula 
\begin{flalign*}
\E\left[W\left(\mathbf{X}\right)\right] & =\E\left[\frac{1}{n}\sum_{i=1}^{n}\log\left(1+X_{i}\right)\right]\\
 & =\frac{1}{n}\E\left[\sum_{i=1}^{n}\log\left(1+X_{i}\right)\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\E\left[\log\left(1+X_{i}\right)\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\int_{0}^{\infty}\log\left(1+x_{i}\right)\cdot f\left(x_{i}|\theta\right)\text{d}x_{i}\\
 & =\frac{1}{n}\sum_{i=1}^{n}\int_{0}^{\infty}\log\left(1+x_{i}\right)\cdot\frac{\theta}{\left(1+x_{i}\right)^{1+\theta}}\text{d}x_{i}\\
 & =\frac{1}{n}\sum_{i=1}^{n}\lim_{c\rightarrow\infty}\int_{0}^{c}\log\left(1+x_{i}\right)\cdot\frac{\theta}{\left(1+x_{i}\right)^{1+\theta}}\text{d}x_{i}
\end{flalign*}

\end_inset

Let 
\begin_inset Formula 
\[
u=\log\left(1+x\right),\quad\text{so that}\quad\text{d}u=\frac{1}{1+x}\text{d}x
\]

\end_inset

and let 
\begin_inset Formula 
\[
\text{d}v=\frac{\theta}{\left(1+x\right)^{1+\theta}}\text{d}x=\theta\left(1+x\right)^{-\left(1+\theta\right)}\text{d}x=\theta\left(1+x\right)^{-\theta-1},
\]

\end_inset

so that
\begin_inset Formula 
\[
v=-\left(1+x\right)^{-\theta}=-\frac{1}{\left(1+x\right)^{\theta}}.
\]

\end_inset

Then, integration by parts gives
\begin_inset Formula 
\begin{flalign*}
\int_{a}^{b}u\text{d}v & =\left[uv\right|_{a}^{b}-\int_{a}^{b}v\text{d}u\\
\Leftrightarrow\int_{0}^{c}\log\left(1+x\right)\cdot\frac{\theta}{\left(1+x\right)^{1+\theta}}\text{d}x & =\left[\log\left(1+x\right)\cdot\left(-\frac{1}{\left(1+x\right)^{\theta}}\right)\right|_{0}^{c}-\int_{0}^{c}\left(-\frac{1}{\left(1+x\right)^{\theta}}\right)\frac{1}{1+x}\text{d}x\\
 & =\left[-\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}-\left(-\frac{\log\left(1+0\right)}{\left(1+0\right)^{\theta}}\right)\right]+\int_{0}^{c}\frac{1}{\left(1+x\right)^{\theta+1}}\text{d}x\\
 & =\left[-\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}+\log1\right]+\left[-\frac{1}{\theta\left(1+x\right)^{\theta}}\right|_{0}^{c}\\
 & =-\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}+0+\left[-\frac{1}{\theta\left(1+c\right)^{\theta}}-\left(-\frac{1}{\theta\left(1+0\right)^{\theta}}\right)\right]\\
 & =-\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}-\frac{1}{\theta\left(1+c\right)^{\theta}}+\frac{1}{\theta},
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
\E\left[W\left(\mathbf{X}\right)\right] & =\frac{1}{n}\sum_{i=1}^{n}\lim_{c\rightarrow\infty}\left[-\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}-\frac{1}{\theta\left(1+c\right)^{\theta}}+\frac{1}{\theta}\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\left[-\lim_{c\rightarrow\infty}\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}-\lim_{c\rightarrow\infty}\frac{1}{\theta\left(1+c\right)^{\theta}}+\lim_{c\rightarrow\infty}\frac{1}{\theta}\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\left[-\lim_{c\rightarrow\infty}\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}-0+\frac{1}{\theta}\right].
\end{flalign*}

\end_inset

Using L'Hôpital's rule to evaluate this limit gives
\begin_inset Formula 
\[
\lim_{c\rightarrow\infty}\frac{\log\left(1+c\right)}{\left(1+c\right)^{\theta}}=\lim_{c\rightarrow\infty}\frac{\left(1+c\right)^{-1}}{\theta\left(1+c\right)^{\theta-1}}=\lim_{c\rightarrow\infty}\frac{1}{\theta\left(1+c\right)^{\theta}}=0,
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\E\left[W\left(\mathbf{X}\right)\right]=\frac{1}{n}\sum_{i=1}^{n}\left[0+\frac{1}{\theta}\right]=\frac{1}{n}\left(n\frac{1}{\theta}\right)=\frac{1}{\theta}.
\]

\end_inset

We have 
\begin_inset Formula $\E\left[W\left(\mathbf{X}\right)\right]=1/\theta$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

, so it follows that 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 is an unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
 Then, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:cramér-rao-attainment"

\end_inset

, 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 attains the Cramér-Rao Lower Bound, and it follows that 
\begin_inset Formula $W\left(\mathbf{X}\right)=\frac{1}{n}\sum_{i=1}^{n}\log\left(1+X_{i}\right)$
\end_inset

 is the UMVUE of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Sufficiency and Unbiasedness
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Rao-Blackwell]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:rao-blackwell"

\end_inset

Let 
\begin_inset Formula $W$
\end_inset

 be any unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, and let 
\begin_inset Formula $T$
\end_inset

 be a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 Define 
\begin_inset Formula $\phi\left(T\right)=\E\left[W|T\right]$
\end_inset

.
 Then 
\begin_inset Formula $\E_{\theta}\left[\phi\left(T\right)\right]=\tau\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\Var_{\theta}\left(\phi\left(T\right)\right)\leq\Var_{\theta}\left(W\right)$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

; that is, 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is a uniformly better unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
 (This is Theorem 7.3.17 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Bernoulli Rao-Blackwellization]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\text{Bernoulli}\left(\theta\right)$
\end_inset

 and consider 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

.
\end_layout

\begin_layout Example
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, we found that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\sum_{i=1}^{n}X_{i}$
\end_inset

 was a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

.
 Consider the estimator 
\begin_inset Formula $W\left(\mathbf{X}\right)=X_{1}$
\end_inset

, which has the pmf 
\begin_inset Formula $p_{X_{1}}\left(x\right)=\theta^{x}\left(1-\theta\right)^{1-x}$
\end_inset

, where 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

.
 Then,
\begin_inset Formula 
\[
\E\left[X_{1}\right]=\sum_{x=0}^{1}x\cdot p_{X_{1}}\left(x|\theta\right)=0\cdot\theta^{0}\left(1-\theta\right)^{1-0}+1\cdot\theta^{1}\left(1-\theta\right)^{1-1}=0+\theta=\theta,
\]

\end_inset

so it follows that 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 is an unbiased estimator for 
\begin_inset Formula $\theta$
\end_inset

.
 Then, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rao-blackwell"

\end_inset

 that 
\begin_inset Formula 
\[
\phi\left(T\right)=\E\left[W\left(\mathbf{X}\right)|T\left(\mathbf{X}\right)\right]=\E\left[X_{1}|\sum_{i=1}^{n}X_{i}\right]
\]

\end_inset

is a uniformly better unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 than 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

.
 We will show that 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is unbiased, beginning by finding an expression for 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

.
 Noting that
\begin_inset Formula 
\[
\sum_{i=1}^{n}X_{i}\sim\text{Binomial}\left(n,\theta\right),
\]

\end_inset

we have
\begin_inset Formula 
\begin{flalign*}
\phi\left(T\right) & =\E\left[X_{1}|\sum_{i=1}^{n}X_{i}\right]\\
 & =\sum_{x_{1}=0}^{1}x_{1}\cdot P\left(\left\{ X_{1}=x_{1}\right\} |\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)\\
 & =0\cdot P\left(\left\{ X_{1}=0\right\} |\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)+1\cdot P\left(\left\{ X_{1}=1\right\} |\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)\\
 & =P\left(\left\{ X_{1}=1\right\} |\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)\\
 & =\frac{P\left(\left\{ X_{1}=1\right\} \cap\left\{ \sum_{i=2}^{n}X_{i}=s-1\right\} \right)}{P\left(\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)}\\
 & =\frac{P\left(\left\{ X_{1}=1\right\} \right)\cdot P\left(\left\{ \sum_{i=2}^{n}X_{i}=s-1\right\} \right)}{P\left(\left\{ \sum_{i=1}^{n}X_{i}=s\right\} \right)}\\
 & =\frac{\left[\theta^{1}\left(1-\theta\right)^{0}\right]\left[\binom{n-1}{s-1}\theta^{s-1}\left(1-\theta\right)^{\left(n-1\right)-\left(s-1\right)}\right]}{\binom{n}{s}\theta^{s}\left(1-\theta\right)^{n-s}}\\
 & =\frac{\theta^{s}\left(1-\theta\right)^{n-s}}{\theta^{s}\left(1-\theta\right)^{n-s}}\frac{\frac{\left(n-1\right)!}{\left(s-1\right)!\left(\left(n-1\right)-\left(s-1\right)\right)!}}{\frac{n!}{s!\left(n-s\right)!}}\\
 & =\frac{s!\left(n-1\right)!\left(n-s\right)!}{n!\left(s-1\right)!\left(n-s\right)!}\\
 & =\frac{s\left(s-1\right)!\left(n-1\right)!}{n\left(n-1\right)!\left(s-1\right)!}\\
 & =\frac{s}{n}\\
 & =\frac{1}{n}\sum_{i=1}^{n}X_{i}\\
 & =\bar{X}.
\end{flalign*}

\end_inset

Then, the expected value of 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is
\begin_inset Formula 
\[
\E\left[\phi\left(T\right)\right]=\E\left[\bar{X}\right]=\E\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}\right]=\frac{1}{n}\E\left[\sum_{i=1}^{n}X_{i}\right]=\frac{1}{n}\sum_{i=1}^{n}\E\left[X_{i}\right]=\frac{1}{n}\left(n\theta\right)=\theta,
\]

\end_inset

so it follows that 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is unbiased for 
\begin_inset Formula $\theta$
\end_inset

.
 The variance of 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(\phi\left(T\right)\right) & =\Var\left(\bar{X}\right)\\
 & =\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\Var\left(X_{i}\right)\\
 & =\frac{1}{n^{2}}\sum_{i=1}^{n}\theta\left(1-\theta\right)\\
 & =\frac{n\theta\left(1-\theta\right)}{n^{2}}\\
 & =\frac{\theta\left(1-\theta\right)}{n}.
\end{flalign*}

\end_inset

Noting that the variance of 
\begin_inset Formula $W\left(\mathbf{X}\right)=X_{1}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)=\Var\left(X_{1}\right)=\theta\left(1-\theta\right),
\]

\end_inset

we will verify that 
\begin_inset Formula $\Var\left(\phi\left(T\right)\right)\leq\Var\left(W\right)$
\end_inset

, i.e., that 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is a uniformly better estimator than 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

.
 We have 
\begin_inset Formula $n\geq1$
\end_inset

 and 
\begin_inset Formula $\theta\in\left(0,1\right)$
\end_inset

, so it follows that 
\begin_inset Formula 
\[
\Var\left(\phi\left(T\right)\right)\leq\Var\left(W\right)\Leftrightarrow\frac{\theta\left(1-\theta\right)}{n}\leq\theta\left(1-\theta\right)\Leftrightarrow\theta\left(1-\theta\right)\leq n\theta\left(1-\theta\right)\Leftrightarrow1\leq n,
\]

\end_inset

and we see that the variance of 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is indeed less than or equal to the variance of 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

.
 We will now check whether the variance of 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 attains the Cramér-Rao lower bound for the variance of an unbiased estimator
 
\begin_inset Formula $V\left(\mathbf{X}\right)$
\end_inset

 of 
\begin_inset Formula $\theta$
\end_inset

.
 The CRLB is given by
\begin_inset Formula 
\[
\Var\left(V\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]}.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}},
\]

\end_inset

so that the denominator above is
\begin_inset Formula 
\begin{flalign*}
-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right] & =-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log\theta^{\sum_{i=1}^{n}X_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}X_{i}}\right]\\
 & =-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\left[\log\left(\theta\right)\sum_{i=1}^{n}X_{i}+\log\left(1-\theta\right)\left(n-\sum_{i=1}^{n}X_{i}\right)\right]\right]\\
 & =-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\left[n\bar{X}\log\theta+n\left(1-\bar{X}\right)\log\left(1-\theta\right)\right]\right]\\
 & =-\E\left[\frac{\partial}{\partial\theta}\left[\frac{n\bar{X}}{\theta}-\frac{n\left(1-\bar{X}\right)}{1-\theta}\right]\right]\\
 & =-\E\left[-\frac{n\bar{X}}{\theta^{2}}+\frac{n\left(1-\bar{X}\right)}{\left(1-\theta\right)^{2}}\right]\\
 & =-\left[\E\left[-\frac{n\bar{X}}{\theta^{2}}\right]+\E\left[\frac{n\left(1-\bar{X}\right)}{\left(1-\theta\right)^{2}}\right]\right]\\
 & =-\left[-\frac{n}{\theta^{2}}\E\left[\bar{X}\right]+\frac{n}{\left(1-\theta\right)^{2}}\E\left[1-\bar{X}\right]\right]\\
 & =-\left[-\frac{n\theta}{\theta^{2}}+\frac{n}{\left(1-\theta\right)^{2}}\left(\E\left[1\right]-\E\left[\bar{X}\right]\right)\right]\\
 & =-\left[-\frac{n}{\theta}+\frac{n\left(1-\theta\right)}{\left(1-\theta\right)^{2}}\right]\\
 & =-\left[-\frac{n}{\theta}+\frac{n}{1-\theta}\right]\\
 & =-\left[-\frac{n\left(1-\theta\right)+n\theta}{\theta\left(1-\theta\right)}\right]\\
 & =\frac{n-n\theta+n\theta}{\theta\left(1-\theta\right)}\\
 & =\frac{n}{\theta\left(1-\theta\right)}.
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

, so that 
\begin_inset Formula $\tau'\left(\theta\right)=1$
\end_inset

, so that the Cramér-Rao lower bound is given by
\begin_inset Formula 
\begin{flalign*}
\Var\left(V\left(\mathbf{X}\right)\right)\geq & \frac{\left[\tau'\left(\theta\right)\right]^{2}}{-\E_{\theta}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f\left(\mathbf{X}|\theta\right)\right]}=\frac{1}{\frac{n}{\theta\left(1-\theta\right)}}=\frac{\theta\left(1-\theta\right)}{n},
\end{flalign*}

\end_inset

and we see that 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 attains this lower bound.
\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $W$
\end_inset

 is a best unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, then 
\begin_inset Formula $W$
\end_inset

 is unique.
 (This is Theorem 7.3.19 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Suppose 
\begin_inset Formula $W'$
\end_inset

 is another best unbiased estimator, and consider the estimator 
\begin_inset Formula $W^{*}=\frac{1}{2}\left(W+W'\right)$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
\E_{\theta}\left[W^{*}\right]=\E_{\theta}\left[\frac{1}{2}\left(W+W'\right)\right]=\frac{1}{2}\left(\E_{\theta}\left[W\right]+\E_{\theta}\left[W'\right]\right)=\frac{1}{2}\left(\tau\left(\theta\right)+\tau\left(\theta\right)\right)=\frac{1}{2}\left(2\tau\left(\theta\right)\right)=\tau\left(\theta\right)
\]

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
\Var_{\theta}\left(W^{*}\right) & =\Var_{\theta}\left(\frac{1}{2}W+\frac{1}{2}W'\right)\\
 & =\left(\frac{1}{2}\right)^{2}\Var\left(W\right)+\left(\frac{1}{2}\right)^{2}\Var\left(W'\right)+2\left(\frac{1}{2}\cdot\frac{1}{2}\right)\Cov\left(W,W'\right)\\
 & =\frac{1}{4}\Var\left(W\right)+\frac{1}{4}\Var\left(W'\right)+\frac{1}{2}\Cov\left(W,W'\right).
\end{flalign*}

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cauchy-schwarz-inequality"

\end_inset

, we have
\begin_inset Formula 
\[
\left|\Cov\left(W,W'\right)\right|\leq\left[\Var\left(W\right)\Var\left(W'\right)\right]^{1/2}.
\]

\end_inset

Because 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $W'$
\end_inset

 are both best unbiased estimators, it follows that 
\begin_inset Formula $\Var\left(W\right)=\Var\left(W'\right)$
\end_inset

, so that we have
\begin_inset Formula 
\[
\left|\Cov\left(W,W'\right)\right|\leq\left[\Var\left(W\right)^{2}\right]^{1/2}=\Var\left(W\right)
\]

\end_inset

and 
\begin_inset Formula 
\begin{flalign*}
\Var_{\theta}\left(W^{*}\right) & =\frac{1}{4}\Var\left(W\right)+\frac{1}{4}\Var\left(W'\right)+\frac{1}{2}\Cov\left(W,W'\right)\\
 & \leq\frac{1}{4}\Var\left(W\right)+\frac{1}{4}\Var\left(W\right)+\frac{1}{2}\Var\left(W\right)\\
 & =\Var\left(W\right).
\end{flalign*}

\end_inset

But if the above inequality is strict, then the best unbiasedness of 
\begin_inset Formula $W$
\end_inset

 is contradicted, so we must have equality for all 
\begin_inset Formula $\theta$
\end_inset

.
 Since the inequality is an application of Cauchy-Schwarz, we can have the
 equality only if 
\begin_inset Formula $W'=a\left(\theta\right)W+b\left(\theta\right)$
\end_inset

.
 Now using properties of covariance, we have
\begin_inset Formula 
\[
\Cov_{\theta}\left(W,W'\right)=\Cov_{\theta}\left(W,a\left(\theta\right)W+b\left(\theta\right)\right)=\Cov_{\theta}\left(W,a\left(\theta\right)W\right)=a\left(\theta\right)\Var_{\theta}\left(W\right),
\]

\end_inset

but 
\begin_inset Formula $\Cov_{\theta}\left(W,W'\right)=\Var_{\theta}\left(W\right)$
\end_inset

 since we had equality above.
 Hence 
\begin_inset Formula $a\left(\theta\right)=1$
\end_inset

 and, since 
\begin_inset Formula $\E_{\theta}\left[W'\right]=\tau\left(\theta\right)$
\end_inset

, we must have 
\begin_inset Formula $b\left(\theta\right)=0$
\end_inset

 and 
\begin_inset Formula $W=W'$
\end_inset

, showing that 
\begin_inset Formula $W$
\end_inset

 is unique.
\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $\E_{\theta}\left[W\right]=\tau\left(\theta\right)$
\end_inset

, 
\begin_inset Formula $W$
\end_inset

 is the best unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

 if and only if 
\begin_inset Formula $W$
\end_inset

 is uncorrelated with all unbiased estimators of 0.
 (This is Theorem 7.3.20 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Lehmann-Scheffé Theorem]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:lehmann-scheffé"

\end_inset

Let 
\begin_inset Formula $T$
\end_inset

 be a complete sufficient statistic for a parameter 
\begin_inset Formula $\theta$
\end_inset

, and let 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 be any estimator based only on 
\begin_inset Formula $T$
\end_inset

.
 Then 
\begin_inset Formula $\phi\left(T\right)$
\end_inset

 is the unique best unbiased estimator of its expected value.
 (This is Theorem 7.3.23 from Casella & Berger.)
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $T$
\end_inset

 is complete and sufficient, and 
\begin_inset Formula $h\left(X\right)$
\end_inset

 is unbiased for 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

, then 
\begin_inset Formula $\phi\left(T\right)=\E\left[h\left(X\right)|T\right]$
\end_inset

 is the best unbiased estimator of 
\begin_inset Formula $\tau\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid with pdf
\begin_inset Formula 
\[
f\left(x|\theta\right)=\frac{2x}{\theta^{2}},\quad0<x\leq\theta.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Show that the MLE of 
\begin_inset Formula $\theta$
\end_inset

 is complete and sufficient.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The likelihood function is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=\prod_{i=1}^{n}f\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\frac{2x_{i}}{\theta^{2}}I_{\left\{ 0<x_{i}\leq\theta\right\} }=\frac{2^{n}}{\theta^{2n}}\left(\prod_{i=1}^{n}x_{i}\right)I_{\left\{ 0<x_{\left(n\right)}\leq\theta\right\} },
\]

\end_inset

which can be written as
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=\begin{cases}
\frac{2^{n}}{\theta^{2n}}\prod_{i=1}^{n}x_{i}, & \theta\geq x_{\left(n\right)}\\
0, & \text{otherwise}
\end{cases}.
\]

\end_inset

When 
\begin_inset Formula $\theta\geq X_{\left(n\right)}$
\end_inset

, i.e., when 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)>0$
\end_inset

, 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 is decreasing in 
\begin_inset Formula $\theta$
\end_inset

.
 It follows that 
\begin_inset Formula $\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 attains its maximum at 
\begin_inset Formula $\theta=X_{\left(n\right)}$
\end_inset

, i.e., the MLE of 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $\hat{\theta}=X_{\left(n\right)}$
\end_inset

.
 We will now show that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is sufficient.
 We can write the joint density of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 as
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\underbrace{\left(\prod_{i=1}^{n}x_{i}\right)}_{h\left(\mathbf{x}\right)}\underbrace{\frac{2^{n}}{\theta^{2n}}I_{\left\{ 0<x_{\left(n\right)}\leq\theta\right\} }}_{g\left(T\left(\mathbf{x}\right)|\theta\right)},
\]

\end_inset

so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

.
 We will now find a pdf for 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

.
 The cdf of 
\begin_inset Formula $X$
\end_inset

 is given by
\begin_inset Formula 
\[
F_{X}\left(t|\theta\right)=\int_{0}^{t}f_{X}\left(x|\theta\right)\dif x=\int_{0}^{t}\frac{2x}{\theta^{2}}\dif x=\left[\frac{x^{2}}{\theta^{2}}\right|_{0}^{t}=\frac{t^{2}}{\theta^{2}},
\]

\end_inset

so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:order-stat-continuous"

\end_inset

 that a pdf for 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{X_{\left(n\right)}}\left(x\right) & =\frac{n!}{\left(n-1\right)!\left(n-n\right)!}f_{X}\left(x\right)\left[F_{X}\left(x\right)\right]^{n-1}\left[1-F_{X}\left(x\right)\right]^{n-n}\\
 & =\frac{n\left(n-1\right)!}{\left(n-1\right)!0!}\left[\frac{2x}{\theta^{2}}\right]\left[\frac{x^{2}}{\theta^{2}}\right]^{n-1}\left[1-\frac{x^{2}}{\theta^{2}}\right]^{0}\\
 & =\frac{2nx}{\theta^{2}}\left(\frac{x^{2n-2}}{\theta^{2n-2}}\right)\\
 & =\frac{2nx^{2n-1}}{\theta^{2n}}
\end{flalign*}

\end_inset

where 
\begin_inset Formula $0<x\leq\theta$
\end_inset

 and 
\begin_inset Formula $f_{X_{\left(n\right)}}\left(x\right)=0$
\end_inset

 otherwise.
 To show that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is complete, suppose that 
\begin_inset Formula $g\left(T\left(\mathbf{X}\right)\right)$
\end_inset

 is a function satisfying 
\begin_inset Formula $\E\left[g\left(T\left(\mathbf{X}\right)\right)\right]=0$
\end_inset

, so that we have
\begin_inset Formula 
\[
0=\E\left[g\left(T\left(\mathbf{X}\right)\right)\right]=\E\left[g\left(X_{\left(n\right)}\right)\right]=\int_{0}^{\theta}g\left(x\right)\cdot f_{X_{\left(n\right)}}\left(x\right)\dif x=\int_{0}^{\theta}g\left(x\right)\frac{2nx^{2n-1}}{\theta^{2n}}\dif x.
\]

\end_inset

By assumption, 
\begin_inset Formula $\mbox{E}\left[g\left(T\right)\right]=0$
\end_inset

, i.e., it is constant as a function of 
\begin_inset Formula $\theta$
\end_inset

, so that its derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 is zero.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
0 & =\frac{\partial}{\partial\theta}\E\left[g\left(T\left(\mathbf{X}\right)\right)\right]\\
 & =\frac{\partial}{\partial\theta}\int_{0}^{\theta}g\left(x\right)\frac{2nx^{2n-1}}{\theta^{2n}}\dif x\\
 & =\frac{\partial}{\partial\theta}\left[\frac{2n}{\theta^{2n}}\int_{0}^{\theta}g\left(x\right)\cdot x^{2n-1}\dif x\right]\\
 & =2n\theta^{-2n-1}\left(-2n\right)\int_{0}^{\theta}g\left(x\right)\cdot x^{2n-1}\dif x+\frac{2n}{\theta^{2n}}\frac{\partial}{\partial\theta}\int_{0}^{\theta}g\left(x\right)\cdot x^{2n-1}\dif x\\
 & =-\frac{2n}{\theta}\int_{0}^{\theta}g\left(x\right)\frac{2nx^{2n-1}}{\theta^{2n}}\dif x+\frac{2n}{\theta^{2n}}\left[g\left(\theta\right)\cdot\theta^{2n-1}\right]\\
 & =-\frac{2n}{\theta}\cdot0+\frac{2n}{\theta}g\left(\theta\right)\\
 & =\frac{2n}{\theta}g\left(\theta\right).
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $n,\theta>0$
\end_inset

, so it follows that 
\begin_inset Formula $\E\left[g\left(T\right)\right]=0$
\end_inset

 if and only if 
\begin_inset Formula $g\left(\theta\right)=0$
\end_inset

 for all 
\begin_inset Formula $\theta$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\E\left[g\left(T\right)\right]=0\quad\forall\theta\implies P\left(\left\{ g\left(T\right)=0\right\} \right)=1\quad\forall\theta,
\]

\end_inset

so by definition 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a complete statistic.
\end_layout

\end_deeper
\begin_layout Enumerate
Find the UMVUE of 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The expected value of 
\begin_inset Formula $\hat{\theta}=X_{\left(n\right)}$
\end_inset

 is given by 
\begin_inset Formula 
\begin{flalign*}
\E\left[\hat{\theta}\right] & =\E\left[X_{\left(n\right)}\right]\\
 & =\int_{0}^{\theta}x\cdot f_{X_{\left(n\right)}}\left(x\right)\dif x\\
 & =\int_{0}^{\theta}x\cdot\frac{2nx^{2n-1}}{\theta^{2n}}\dif x\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}x^{2n}\dif x\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+1}x^{2n+1}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{\theta^{2n+1}}{2n+1}-0\right]\\
 & =\frac{2n}{2n+1}\theta,
\end{flalign*}

\end_inset

so it follows that 
\begin_inset Formula 
\[
\tilde{\theta}=\frac{2n+1}{2n}X_{\left(n\right)}
\]

\end_inset

is unbiased for 
\begin_inset Formula $\theta$
\end_inset

.
 The estimator 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is based only on 
\begin_inset Formula $T\left(\mathbf{X}\right)=X_{\left(n\right)}$
\end_inset

, which is a complete sufficient statistic, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:lehmann-scheffé"

\end_inset

 that 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is the UMVUE for 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Show that the variance of the UMVUE of 
\begin_inset Formula $\theta$
\end_inset

 is less than the Cramér-Rao lower bound.
 Why is it so?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is given by
\begin_inset Formula 
\[
\Var\left(\tilde{\theta}\right)=\Var\left(\frac{2n+1}{2n}X_{\left(n\right)}\right)=\left(\frac{2n+1}{2n}\right)^{2}\Var\left(X_{\left(n\right)}\right).
\]

\end_inset

The expected value of 
\begin_inset Formula $X_{\left(n\right)}^{2}$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
\E\left[X_{\left(n\right)}^{2}\right] & =\int_{0}^{\theta}x^{2}\cdot f_{X_{\left(n\right)}}\left(x\right)\dif x\\
 & =\int_{0}^{\theta}x^{2}\cdot\frac{2nx^{2n-1}}{\theta^{2n}}\dif x\\
 & =\frac{2n}{\theta^{2n}}\int_{0}^{\theta}x^{2n+1}\dif x\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{1}{2n+2}x^{2n+2}\right|_{0}^{\theta}\\
 & =\frac{2n}{\theta^{2n}}\left[\frac{\theta^{2n+2}}{2n+2}-0\right]\\
 & =\frac{2n\theta^{2}}{2n+2}\\
 & =\frac{2n\theta^{2}}{2\left(n+1\right)}\\
 & =\frac{n\theta^{2}}{n+1},
\end{flalign*}

\end_inset

so that we have
\begin_inset Formula 
\begin{flalign*}
\Var\left(X_{\left(n\right)}\right) & =\E\left[X_{\left(n\right)}^{2}\right]-\left(\E\left[X_{\left(n\right)}\right]\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\left(\frac{2n\theta}{2n+1}\right)^{2}\\
 & =\frac{n\theta^{2}}{n+1}-\frac{4n^{2}\theta^{2}}{\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}\left(2n+1\right)^{2}-4n^{2}\theta^{2}\left(n+1\right)}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}\left[4n^{2}+4n+1-4n\left(n+1\right)\right]}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}\left(4n^{2}+4n+1-4n^{2}-4n\right)}{\left(n+1\right)\left(2n+1\right)^{2}}\\
 & =\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}}
\end{flalign*}

\end_inset

and
\begin_inset Formula 
\[
\Var\left(\tilde{\theta}\right)=\left(\frac{2n+1}{2n}\right)^{2}\Var\left(X_{\left(n\right)}\right)=\frac{\left(2n+1\right)^{2}}{4n^{2}}\frac{n\theta^{2}}{\left(n+1\right)\left(2n+1\right)^{2}}=\frac{\theta^{2}}{4n\left(n+1\right)}.
\]

\end_inset

 The Cramér-Rao lower bound for the variance of an unbiased estimator 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 of 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

 that satisfies
\begin_inset Formula 
\[
\frac{\dif}{\dif\theta}\E_{\theta}\left[W\left(\mathbf{X}\right)\right]=\int_{\mathcal{X}}\frac{\partial}{\partial\theta}\left[W\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta\right)\right]\dif\mathbf{x}\quad\mbox{and}\quad\Var_{\theta}\left(W\left(\mathbf{X}\right)\right)<\infty
\]

\end_inset

is given by
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}.
\]

\end_inset

In the case that 
\begin_inset Formula $0<X_{\left(n\right)}\leq\theta$
\end_inset

, we have 
\begin_inset Formula 
\begin{flalign*}
\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right] & =\E\left[\left(\frac{\partial}{\partial\theta}\log\frac{2^{n}}{\theta^{2n}}\left(\prod_{i=1}^{n}X_{i}\right)\right)^{2}\right]\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\left(\log\frac{2^{n}}{\theta^{2n}}+\log\prod_{i=1}^{n}X_{i}\right)\right)^{2}\right]\\
 & =\E\left[\left(\frac{\partial}{\partial\theta}\left(n\log2-2n\log\theta+\sum_{i=1}^{n}\log X_{i}\right)\right)^{2}\right]\\
 & =\E\left[\left(0-\frac{2n}{\theta}+0\right)^{2}\right]\\
 & =\E\left[\left(-\frac{2n}{\theta}\right)^{2}\right]\\
 & =\E\left[\frac{4n^{2}}{\theta^{2}}\right]\\
 & =\frac{4n^{2}}{\theta^{2}}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $\tau\left(\theta\right)=\theta$
\end_inset

, so that 
\begin_inset Formula $\tau'\left(\theta\right)=1$
\end_inset

, so that the Cramér-Rao lower bound is given by
\begin_inset Formula 
\[
\Var\left(W\left(\mathbf{X}\right)\right)\geq\frac{\left[\tau'\left(\theta\right)\right]^{2}}{\E\left[\left(\frac{\partial}{\partial\theta}\log f\left(\mathbf{X}|\theta\right)\right)^{2}\right]}=\frac{1}{\frac{4n^{2}}{\theta^{2}}}=\frac{\theta^{2}}{4n^{2}}.
\]

\end_inset

Noting that 
\begin_inset Formula $n,\theta>0$
\end_inset

, we will check that the variance of 
\begin_inset Formula $\tilde{\theta}$
\end_inset

 is less than the CRLB.
\begin_inset Formula 
\[
\frac{\theta^{2}}{4n\left(n+1\right)}<\frac{\theta^{2}}{4n^{2}}\implies\left(\frac{\theta^{2}}{4n\left(n+1\right)}\right)\frac{4n}{\theta^{2}}<\left(\frac{\theta^{2}}{4n^{2}}\right)\frac{4n}{\theta^{2}}\implies\frac{1}{n+1}<\frac{1}{n}\implies n<n+1
\]

\end_inset

We see that 
\begin_inset Formula $n$
\end_inset

 is indeed less than 
\begin_inset Formula $n+1$
\end_inset

, so it follows that 
\begin_inset Formula $\Var\left(\tilde{\theta}\right)$
\end_inset

 is less than the Cramér-Rao lower bound.
 This occurs because 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 does not satisfy the regularity conditions of the theorem, i.e., the interchange
 of differentiation and integration, because the bounds of the support of
 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)$
\end_inset

 depend on 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Loss function optimality
\end_layout

\begin_layout Standard
Mean squared error is a special case of a function called a loss function.
 The study of the performance, and the optimality, of estimators evaluated
 through loss functions is a branch of decision theory.
 The loss function is a nonnegative function that generally increases as
 the distance between an estimator 
\begin_inset Formula $\hat{\theta}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 increases.
 If 
\begin_inset Formula $\theta$
\end_inset

 is real-valued, two commonly used loss functions are
\begin_inset Formula 
\[
\text{absolute error loss,}\qquad L\left(\theta,\hat{\theta}\right)=\left|\hat{\theta}-\theta\right|
\]

\end_inset

and
\begin_inset Formula 
\[
\text{squared error loss,}\qquad L\left(\theta,\hat{\theta}\right)=\left(\hat{\theta}-\theta\right)^{2}.
\]

\end_inset

In a loss function, the quality of an estimator is quantified in its risk
 function.
 For an estimator 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 of 
\begin_inset Formula $\theta$
\end_inset

, the risk function is
\begin_inset Formula 
\[
\text{risk}=R\left(\theta,T\right)=\E_{\theta}\left[L\left(\theta,T\right)\right]=\int_{\mathcal{X}}L\left(\theta,T\left(\mathbf{x}\right)\right)f\left(\mathbf{x}|\theta\right)\dif\mathbf{x}.
\]

\end_inset

At a given 
\begin_inset Formula $\theta$
\end_inset

, the risk function is the average loss that will be incurred if the estimator
 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is used (averaged over 
\begin_inset Formula $f\left(x|\theta\right)$
\end_inset

).
 For two different estimators 
\begin_inset Formula $T_{1}$
\end_inset

 and 
\begin_inset Formula $T_{2}$
\end_inset

, if 
\begin_inset Formula $R\left(\theta,T_{1}\right)<R\left(\theta,T_{2}\right)$
\end_inset

 for all 
\begin_inset Formula $\theta\in\Theta$
\end_inset

, then 
\begin_inset Formula $T_{1}$
\end_inset

 is the preferred estimator.
 More typically, the two risk functions will cross.
 Then the judgment as to which estimator is better may not be so clear-cut.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\mathcal{N}\left(\theta,1\right)$
\end_inset

 and assume we use a squared error loss.
 Consider two estimators
\begin_inset Formula 
\[
\hat{\theta}_{1}=X\qquad\hat{\theta}_{2}=3.
\]

\end_inset

The risk function for 
\begin_inset Formula $\hat{\theta}_{1}$
\end_inset

 is 
\begin_inset Formula 
\[
R\left(\theta,\hat{\theta}_{1}\right)=\E\left[\left(X-\theta\right)^{2}\right]=\E\left[X^{2}-2\theta X+\theta^{2}\right]=\E\left[X^{2}\right]-2\theta\E\left[X\right]+\E\left[\theta^{2}\right].
\]

\end_inset

We have 
\begin_inset Formula 
\[
\Var\left(X\right)=\E\left[X^{2}\right]-\left(\E\left[X\right]\right)^{2}\Leftrightarrow\E\left[X^{2}\right]=\Var\left(X\right)+\left(\E\left[X\right]\right)^{2}=1+\theta^{2},
\]

\end_inset

so that 
\begin_inset Formula 
\[
R\left(\theta,\hat{\theta}_{1}\right)=\E\left[X^{2}\right]-2\theta\E\left[X\right]+\E\left[\theta^{2}\right]=1+\theta^{2}-2\theta\left(\theta\right)+\theta^{2}=1+2\theta^{2}-2\theta^{2}=1.
\]

\end_inset

The risk function for 
\begin_inset Formula $\hat{\theta}_{2}$
\end_inset

 is
\begin_inset Formula 
\[
R\left(\theta,\hat{\theta}_{2}\right)=\E\left[\left(3-\theta\right)^{2}\right]=\left(3-\theta\right)^{2}.
\]

\end_inset

If 
\begin_inset Formula $2<\theta<4$
\end_inset

, then 
\begin_inset Formula $R\left(\theta,\hat{\theta}_{2}\right)<R\left(\theta,\hat{\theta}_{1}\right)$
\end_inset

, otherwise 
\begin_inset Formula $R\left(\theta,\hat{\theta}_{1}\right)<R\left(\theta,\hat{\theta}_{2}\right)$
\end_inset

.
 Neither estimator uniformly dominates the other.
\end_layout

\begin_layout Standard
Under squared error loss, the risk function is the MSE
\begin_inset Formula 
\[
R_{T}\left(\theta\right)=\E\left[\left(T\left(\mathbf{X}\right)-\theta\right)^{2}\right]=\text{MSE}\left(T\left(\mathbf{X}\right)\right)=\Var\left(T\left(\mathbf{X}\right)\right)+\left[\text{Bias}\left(T\left(\mathbf{X}\right)\right)\right]^{2}.
\]

\end_inset

The risk function for squared error loss clearly indicates that a good estimator
 should have both a small variance and a small bias.
 Restricting the set of allowable estimators to the set of unbiased estimators
 would not be typical in decision theory (in that case, minimizing the risk
 would just be minimizing the variance).
 A decision theoretic analysis would be more comprehensive in that both
 the variance and bias are in the risk and will be considered simultaneously.
\end_layout

\begin_layout Subsubsection
Bayes risk
\end_layout

\begin_layout Standard
In a Bayesian analysis, we would use the prior distribution to compute an
 average risk
\begin_inset Formula 
\begin{flalign*}
r\left(T\right) & =\int_{\Theta}R\left(\theta,T\right)\pi\left(\theta\right)\dif\theta\\
 & =\int_{\Theta}\left[\int_{\mathcal{X}}L\left(\theta,T\left(\mathbf{x}\right)\right)f\left(\mathbf{x}|\theta\right)\dif x\right]\pi\left(\theta\right)\dif\theta\\
 & =\int_{\mathcal{X}}\left[\int_{\Theta}L\left(\theta,T\left(\mathbf{x}\right)\right)\pi\left(\theta|\mathbf{x}\right)\dif\theta\right]f\left(\mathbf{x}\right)\dif\mathbf{x}\\
 & =\int_{\mathcal{X}}r\left(T,\mathbf{x}\right)f\left(\mathbf{x}\right)\dif\mathbf{x},
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\pi\left(\theta|\mathbf{x}\right)$
\end_inset

 is the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $f\left(\mathbf{x}\right)$
\end_inset

 is the marginal distribution of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, i.e.,
\begin_inset Formula 
\[
\pi\left(\theta|\mathbf{x}\right)=\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{f\left(\mathbf{x}\right)}\implies\pi\left(\theta\right)f\left(\mathbf{x}|\theta\right)=\pi\left(\theta|\mathbf{x}\right)f\left(\mathbf{x}\right).
\]

\end_inset


\begin_inset Formula $r\left(T,\mathbf{X}\right)$
\end_inset

 is the expected value of the loss function with respect to the posterior
 distribution, called the posterior expected loss.
 
\begin_inset Formula $r\left(T,\mathbf{X}\right)$
\end_inset

 is a function only of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and not a function of 
\begin_inset Formula $\theta$
\end_inset

.
 Thus, for each 
\begin_inset Formula $\mathbf{x}$
\end_inset

, if we choose the estimator 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 to minimize the posterior expected loss, we will minimize the Bayes risk.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 be iid 
\begin_inset Formula $\text{Bernoulli}\left(\theta\right)$
\end_inset

 random variables and consider a 
\begin_inset Formula $\text{Beta}\left(\alpha,\beta\right)$
\end_inset

 prior for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the Bayes estimator for a squared error loss.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The loss function is given by
\begin_inset Formula $L\left(\theta,T\right)=\left(T-\theta\right)^{2}$
\end_inset

, so that the Bayes risk is given by
\begin_inset Formula 
\[
r\left(T\right)=\int_{\mathcal{X}}r\left(T,\mathbf{x}\right)f\left(\mathbf{x}\right)\text{d}\mathbf{x}.
\]

\end_inset

Then, the posterior expected loss is given by
\begin_inset Formula 
\[
r\left(T,\mathbf{x}\right)=\int_{\Theta}L\left(\theta,T\left(\mathbf{x}\right)\right)\pi\left(\theta|\mathbf{x}\right)\text{d}\theta.
\]

\end_inset

The density of the prior distribution is given by
\begin_inset Formula 
\[
\pi\left(\theta\right)=\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1},\qquad0<\theta<1.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-bernoulli"

\end_inset

, the joint pmf of 
\begin_inset Formula $\mathbf{X}=X_{1},\ldots,X_{n}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)=\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}},
\]

\end_inset

so that the posterior distribution is given by
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)}{\int_{\Theta}f\left(\mathbf{x}|\theta\right)\pi\left(\theta\right)\text{d}\theta}\\
 & =\frac{\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\left[\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\right]}{\int_{0}^{1}\left[\theta^{\sum_{i=1}^{n}x_{i}}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}}\right]\left[\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\right]\text{d}\theta}\\
 & =\frac{\theta^{\sum_{i=1}^{n}x_{i}+\alpha-1}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}+\beta-1}}{\int_{0}^{1}\theta^{\sum_{i=1}^{n}x_{i}+\alpha-1}\left(1-\theta\right)^{n-\sum_{i=1}^{n}x_{i}+\beta-1}\text{d}\theta}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $\alpha^{*}=\alpha+\sum_{i=1}^{n}x_{i}$
\end_inset

 and let 
\begin_inset Formula $\beta^{*}=\beta+n-\sum_{i=1}^{n}x_{i}$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}}{\int_{0}^{1}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta}\\
 & =\frac{\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}}{\frac{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+\beta^{*}\right)}\int_{0}^{1}\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta}.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\text{Beta}\left(\alpha^{*},\beta^{*}\right)$
\end_inset

 random variable, which integrates to 1, so that we have
\begin_inset Formula 
\begin{flalign*}
\pi\left(\theta|\mathbf{x}\right) & =\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1},
\end{flalign*}

\end_inset

i.e., 
\begin_inset Formula $\pi\left(\theta|\mathbf{x}\right)\sim\text{Beta}\left(\alpha^{*},\beta^{*}\right)$
\end_inset

.
 Then, the posterior expected loss is given by
\begin_inset Formula 
\begin{flalign*}
r\left(T,\mathbf{x}\right) & =\int_{\Theta}L\left(\theta,T\left(\mathbf{x}\right)\right)\pi\left(\theta|\mathbf{x}\right)\text{d}\theta\\
 & =\int_{0}^{1}\left(t-\theta\right)^{2}\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\\
 & =\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\int_{0}^{1}\left(t^{2}-2\theta t+\theta^{2}\right)\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\\
 & =\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\left[t^{2}\int_{0}^{1}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\right.\\
 & \quad\left.-2t\int_{0}^{1}\theta^{\alpha^{*}}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta+\int_{0}^{1}\theta^{\alpha^{*}+1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\right]\\
 & =\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\left[t^{2}\frac{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+\beta^{*}\right)}\int_{0}^{1}\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}-1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\right.\\
 & \quad-2t\frac{\Gamma\left(\alpha^{*}+1\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+1+\beta^{*}\right)}\int_{0}^{1}\frac{\Gamma\left(\alpha^{*}+1+\beta^{*}\right)}{\Gamma\left(\alpha^{*}+1\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\\
 & \quad\left.+\frac{\Gamma\left(\alpha^{*}+2\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+2+\beta^{*}\right)}\int_{0}^{1}\frac{\Gamma\left(\alpha^{*}+2+\beta^{*}\right)}{\Gamma\left(\alpha^{*}+2\right)\Gamma\left(\beta^{*}\right)}\theta^{\alpha^{*}+1}\left(1-\theta\right)^{\beta^{*}-1}\text{d}\theta\right]\\
 & =\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}\left[t^{2}\frac{\Gamma\left(\alpha^{*}\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+\beta^{*}\right)}\cdot1-2t\frac{\Gamma\left(\alpha^{*}+1\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+1+\beta^{*}\right)}\cdot1+\frac{\Gamma\left(\alpha^{*}+2\right)\Gamma\left(\beta^{*}\right)}{\Gamma\left(\alpha^{*}+2+\beta^{*}\right)}\cdot1\right]\\
 & =t^{2}-2t\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)\Gamma\left(\alpha^{*}+1\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\alpha^{*}+1+\beta^{*}\right)}+\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)\Gamma\left(\alpha^{*}+2\right)}{\Gamma\left(\alpha^{*}\right)\Gamma\left(\alpha^{*}+2+\beta^{*}\right)}\\
 & =t^{2}-2t\frac{\alpha^{*}\Gamma\left(\alpha^{*}\right)\Gamma\left(\alpha^{*}+\beta^{*}\right)}{\Gamma\left(\alpha^{*}\right)\left(\alpha^{*}+\beta^{*}\right)\Gamma\left(\alpha^{*}+\beta^{*}\right)}+\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)\left(\alpha^{*}+1\right)\Gamma\left(\alpha^{*}+1\right)}{\Gamma\left(\alpha^{*}\right)\left(\alpha^{*}+\beta^{*}+1\right)\Gamma\left(\alpha^{*}+\beta^{*}+1\right)}\\
 & =t^{2}-2t\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}+\frac{\Gamma\left(\alpha^{*}+\beta^{*}\right)\left(\alpha^{*}+1\right)\alpha^{*}\Gamma\left(\alpha^{*}\right)}{\Gamma\left(\alpha^{*}\right)\left(\alpha^{*}+\beta^{*}+1\right)\left(\alpha^{*}+\beta^{*}\right)\Gamma\left(\alpha^{*}+\beta^{*}\right)}\\
 & =t^{2}-2t\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}+\frac{\alpha^{*}\left(\alpha^{*}+1\right)}{\left(\alpha^{*}+\beta^{*}\right)\left(\alpha^{*}+\beta^{*}+1\right)}.
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $t$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial t}r\left(T,\mathbf{x}\right)=\frac{\partial}{\partial t}\left[t^{2}-2t\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}+\frac{\alpha^{*}\left(\alpha^{*}+1\right)}{\left(\alpha^{*}+\beta^{*}\right)\left(\alpha^{*}+\beta^{*}+1\right)}\right]=2t-2\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}.
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
2\hat{t}=2\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}\Leftrightarrow\hat{t}=\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}=\frac{\alpha+\sum_{i=1}^{n}x_{i}}{\alpha+\sum_{i=1}^{n}x_{i}+\left(\beta+n-\sum_{i=1}^{n}x_{i}\right)}=\frac{\alpha+\sum_{i=1}^{n}x_{i}}{\alpha+\beta+n},
\]

\end_inset

so it follows that the Bayes estimator of 
\begin_inset Formula $\theta$
\end_inset

 with respect to a 
\begin_inset Formula $\text{Beta}\left(\alpha,\beta\right)$
\end_inset

 prior distribution is 
\begin_inset Formula $\hat{t}=\left(\alpha+\sum_{i=1}^{n}X_{i}\right)/\left(\alpha+\beta+n\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Show that this estimator corresponds to the posterior mean.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We found that the posterior distribution 
\begin_inset Formula $\pi\left(\theta|\mathbf{x}\right)$
\end_inset

 has the distribution of a 
\begin_inset Formula $\text{Beta}\left(\alpha^{*},\beta^{*}\right)$
\end_inset

 random variable.
 The expected value (mean) of a 
\begin_inset Formula $\text{Beta}\left(\gamma,\psi\right)$
\end_inset

 random variable is given by 
\begin_inset Formula $\gamma/\left(\gamma+\psi\right)$
\end_inset

, so it follows that 
\begin_inset Formula 
\[
\E\left[\pi\left(\theta|\mathbf{x}\right)\right]=\frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}=\frac{\alpha+\sum_{i=1}^{n}x_{i}}{\alpha+\sum_{i=1}^{n}x_{i}+\left(\beta+n-\sum_{i=1}^{n}x_{i}\right)}=\frac{\alpha+\sum_{i=1}^{n}x_{i}}{\alpha+\beta+n}
\]

\end_inset

is the posterior mean, which agrees with the estimator we found.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
We can find an explicit formula for the Bayes estimator for specific loss
 functions.
 For a squared error loss function, i.e., 
\begin_inset Formula $L\left(\theta,\hat{\theta}\right)=\left(\hat{\theta}-\theta\right)^{2}$
\end_inset

, the Bayes estimator is the posterior mean.
 For an absolute error loss function, i.e., 
\begin_inset Formula $L\left(\theta,\hat{\theta}\right)=\left|\theta-\hat{\theta}\right|$
\end_inset

, the Bayes estimator is the posterior median.
 For a zero-one loss function, i.e., 
\begin_inset Formula 
\[
L\left(\theta,\hat{\theta}\right)=\begin{cases}
0, & \text{if }\theta=\hat{\theta},\\
1, & \text{if }\theta\neq\hat{\theta}
\end{cases},
\]

\end_inset

the Bayes estimator is the posterior mode.
\end_layout

\begin_layout Chapter
Hypothesis testing
\begin_inset CommandInset label
LatexCommand label
name "chap:hypothesis-testing"

\end_inset


\end_layout

\begin_layout Definition
A 
\shape italic
hypothesis
\shape default
 is a statement about a population parameter.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Definition
The two complementary hypotheses in a hypothesis testing problem are called
 the 
\shape italic
null hypothesis
\shape default
 and the 
\shape italic
alternative hypothesis
\shape default
.
 They are denoted by 
\begin_inset Formula $H_{0}$
\end_inset

 and 
\begin_inset Formula $H_{1}$
\end_inset

, respectively.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\theta$
\end_inset

 denotes a population parameter, the general format of the null and alternative
 hypotheses if 
\begin_inset Formula $H_{0}:\theta\in\Theta_{0}$
\end_inset

 and 
\begin_inset Formula $H_{1}:\theta\in\Theta_{0}^{c}$
\end_inset

, which 
\begin_inset Formula $\Theta_{0}$
\end_inset

 is some subset of the parameter space and 
\begin_inset Formula $\Theta_{0}^{c}$
\end_inset

 is its complement.
 Hypotheses that specify only one possible distribution for the sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

 are called simple hypotheses.
 In most realistic problems, the hypotheses of interest specify more than
 one possible distribtion for the sample.
 Such hypotheses are called 
\shape italic
composite hypotheses.

\shape default
 Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}\overset{\text{iid}}{\sim}N\left(\theta,1\right)$
\end_inset

.
 Then
\begin_inset Formula 
\begin{flalign*}
H_{0}:\theta=\frac{1}{2},\quad H_{1}:\theta=\frac{1}{4} & \qquad\text{are called simple hypotheses,}\\
H_{0}:\theta\leq\frac{1}{2},\quad H_{1}:\theta>\frac{1}{2} & \qquad\text{are called one-sided composite hypotheses, and}\\
H_{0}:\theta=\frac{1}{2},\quad H_{1}:\theta\neq\frac{1}{2} & \qquad\text{are called two-sided composite hypotheses.}
\end{flalign*}

\end_inset


\end_layout

\begin_layout Definition
A 
\shape italic
hypothesis testing procedure
\shape default
 or 
\shape italic
hypothesis test
\shape default
 is a rule that specifies:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
For which sample values the decision is made to accept 
\begin_inset Formula $H_{0}$
\end_inset

 as true.
\end_layout

\begin_layout Enumerate
For which sample values 
\begin_inset Formula $H_{0}$
\end_inset

 is rejected and 
\begin_inset Formula $H_{1}$
\end_inset

 is accepted as true.
\end_layout

\end_deeper
\begin_layout Definition
The subset of the sample space for which 
\begin_inset Formula $H_{0}$
\end_inset

 will be rejected is called the 
\shape italic
rejection region
\shape default
 or 
\shape italic
critical region
\shape default
.
 The complement of the rejection region is called the 
\shape italic
acceptance region
\shape default
.
\end_layout

\begin_layout Standard
Typically, a hypothesis test is specified in terms of a 
\shape italic
test statistic
\shape default
 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

, a function of the sample.
 If the rejection region is denoted as 
\begin_inset Formula $R$
\end_inset

, then a test may reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $W\left(\mathbf{X}\right)\in R$
\end_inset

 and fail to reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $W\left(\mathbf{X}\right)\in R^{c}$
\end_inset

.
\end_layout

\begin_layout Section
Methods of finding tests
\end_layout

\begin_layout Subsection
Likelihood ratio tests
\end_layout

\begin_layout Standard
The likelihood ratio method of hypothesis testing is related to maximum
 likelihood estimators.
\end_layout

\begin_layout Definition
The 
\shape italic
likelihood ratio test statistic
\shape default
 for testing 
\begin_inset Formula $H_{0}:\theta\in\Theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta\in\Theta_{0}^{c}$
\end_inset

 is
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\frac{\sup_{\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}L\left(\theta|\mathbf{x}\right)}.
\]

\end_inset

A 
\shape italic
likelihood ratio test
\shape default
 (LRT) is any test that has a rejection region of the form 
\begin_inset Formula $\left\{ \mathbf{x}:\lambda\left(\mathbf{x}\right)\leq c\right\} $
\end_inset

, where 
\begin_inset Formula $c$
\end_inset

 is any number satisfying 
\begin_inset Formula $0\leq c\leq1$
\end_inset

.
\end_layout

\begin_layout Standard
The numerator of the LRT statistic, 
\begin_inset Formula $\sup_{\Theta_{0}}L\left(\theta|\mathbf{x}\right)$
\end_inset

, is the maximized likelihood under the null hypothesis.
 The denominator, 
\begin_inset Formula $\sup_{\Theta}L\left(\theta|\mathbf{x}\right)$
\end_inset

, is the unrestricted maximized likelihood.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal LRT with known variance]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:LRT-normal-rv"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\overset{\text{iid}}{\sim}N\left(\theta,1\right)$
\end_inset

.
 Specify the LRT for 
\begin_inset Formula 
\[
H_{0}:\theta=\theta_{0}\quad\text{vs.}\quad H_{1}:\theta\neq\theta_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
The likelihood function is given by
\begin_inset Formula 
\[
L\left(\theta|\mathbf{x}\right)=\prod_{i=1}^{n}f_{X}\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\cdot1}}e^{-\left(x_{i}-\theta\right)^{2}/\left(2\cdot1\right)}=\left(2\pi\right)^{-n/2}e^{\sum_{i=1}^{n}-\left(x_{i}-\theta\right)^{2}/2}.
\]

\end_inset

The LRT test statistic is 
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\frac{\sup_{\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}L\left(\theta|\mathbf{x}\right)}.
\]

\end_inset

Under the null hypothesis, we have 
\begin_inset Formula $\Theta_{0}=\left\{ \theta:\theta=\theta_{0}\right\} $
\end_inset

, i.e., only one value of 
\begin_inset Formula $\theta$
\end_inset

 is specified by 
\begin_inset Formula $H_{0}$
\end_inset

, so that the numerator is
\begin_inset Formula 
\begin{flalign*}
\sup_{\Theta_{0}}L\left(\theta|\mathbf{x}\right) & =\sup_{\theta=\theta_{0}}\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{2}\right\} =\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right\} .
\end{flalign*}

\end_inset

Under the alternative hypothesis, we have 
\begin_inset Formula $\Theta=\left\{ \theta:\theta\neq\theta_{0}\right\} $
\end_inset

, so we must maximize 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 over the parameter space 
\begin_inset Formula $\theta\in\mathbb{R}$
\end_inset

, i.e., find the MLE of 
\begin_inset Formula $\theta$
\end_inset

.
 The log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\log L\left(\theta|\mathbf{x}\right) & =\log\left(2\pi\right)^{-n/2}e^{\sum_{i=1}^{n}-\left(x_{i}-\theta\right)^{2}/2}\\
 & =-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{2}\\
 & =-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}^{2}-2\theta x_{i}+\theta^{2}\right)\\
 & =-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\left(\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}2\theta x_{i}+\sum_{i=1}^{n}\theta^{2}\right)\\
 & =-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}+\theta\sum_{i=1}^{n}x_{i}-\frac{n\theta^{2}}{2}.
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\log L\left(\theta|\mathbf{x}\right) & =\frac{\partial}{\partial\theta}\left[-\frac{n}{2}\log\left(2\pi\right)-\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}+\theta\sum_{i=1}^{n}x_{i}-\frac{n\theta^{2}}{2}\right]\\
 & =0-0+\sum_{i=1}^{n}x_{i}-n\theta\\
 & =\sum_{i=1}^{n}x_{i}-n\theta.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
n\hat{\theta}=\sum_{i=1}^{n}x_{i}\implies\hat{\theta}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\bar{x}.
\]

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $\log L\left(\theta|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that this is a maximum.
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta^{2}}\left[\log L\left(\theta|\mathbf{x}\right)\right|_{\theta=\hat{\theta}}=\frac{\partial}{\partial\theta}\left[\sum_{i=1}^{n}x_{i}-n\theta\right|_{\theta=\hat{\theta}}=\left[0-n\right|_{\theta=\hat{\theta}}=-n
\]

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

, so that 
\begin_inset Formula $-n<0$
\end_inset

.
 It follows that 
\begin_inset Formula $\hat{\theta}=\bar{X}$
\end_inset

 is the MLE of 
\begin_inset Formula $\theta$
\end_inset

, so that 
\begin_inset Formula 
\[
\sup_{\Theta}L\left(\theta|\mathbf{x}\right)=\sup_{\theta=\bar{x}}\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{2}\right\} =\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right\} .
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\Theta_{0}}L\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}L\left(\theta|\mathbf{x}\right)}\\
 & =\frac{\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right\} }{\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right\} }\\
 & =\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right\} \exp\left\{ \frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right\} \\
 & =\exp\left\{ \frac{1}{2}\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}-\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right]\right\} \\
 & =\exp\left\{ \frac{1}{2}\left[\sum_{i=1}^{n}\left(x_{i}^{2}-2\bar{x}x_{i}+\bar{x}^{2}\right)-\sum_{i=1}^{n}\left(x_{i}^{2}-2\theta_{0}x_{i}+\theta_{0}^{2}\right)\right]\right\} \\
 & =\exp\left\{ \frac{1}{2}\left(\sum_{i=1}^{n}x_{i}^{2}-2\bar{x}\sum_{i=1}^{n}x_{i}+n\bar{x}^{2}-\sum_{i=1}^{n}x_{i}^{2}+2\theta_{0}\sum_{i=1}^{n}x_{i}-n\theta_{0}^{2}\right)\right\} \\
 & =\exp\left\{ \frac{1}{2}\left(-2n\bar{x}^{2}+n\bar{x}^{2}+2\theta_{0}n\bar{x}-n\theta_{0}^{2}\right)\right\} \\
 & =\exp\left\{ -\frac{n}{2}\left(\theta_{0}^{2}-2\theta_{0}\bar{x}+\bar{x}^{2}\right)\right\} \\
 & =\exp\left\{ -\frac{n}{2}\left(\theta_{0}-\bar{x}\right)^{2}\right\} .
\end{flalign*}

\end_inset

Then, the likelihood ratio test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
c\geq\lambda\left(\mathbf{x}\right)=\exp\left\{ -\frac{n}{2}\left(\theta_{0}-\bar{x}\right)^{2}\right\} \Leftrightarrow\log c\geq-\frac{n}{2}\left(\theta_{0}-\bar{x}\right)^{2}\Leftrightarrow-\frac{2}{n}\log c\leq\left(\theta_{0}-\bar{x}\right)^{2}\Leftrightarrow\left|\theta_{0}-\bar{x}\right|\geq\sqrt{-\frac{2}{n}\log c}
\]

\end_inset

for some 
\begin_inset Formula $c\in\left(0,1\right]$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Exponential LRT]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\overset{\text{iid}}{\sim}\text{Exp}\left(\theta\right)$
\end_inset

, i.e., 
\begin_inset Formula 
\[
X\sim f\left(x|\theta\right)=\frac{1}{\theta}\mathrm{e}^{-x/\theta}.
\]

\end_inset

Specify the LRT for 
\begin_inset Formula 
\[
H_{0}:\theta\leq\theta_{0}\quad\text{vs.}\quad H_{1}:\theta>\theta_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
The likelihood function is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=\prod_{i=1}^{n}f_{X}\left(x_{i}|\theta\right)=\prod_{i=1}^{n}\frac{1}{\theta}\mathrm{e}^{-x_{i}/\theta}=\theta^{-n}\mathrm{e}^{\sum_{i=1}^{n}-x_{i}/\theta},
\]

\end_inset

so that the log-likelihood is given by
\begin_inset Formula 
\[
\log\mathcal{L}\left(\theta|\mathbf{x}\right)=\log\theta^{-n}\mathrm{e}^{\sum_{i=1}^{n}-x_{i}/\theta}=-n\log\theta-\frac{1}{\theta}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log\mathcal{L}\left(\theta|\mathbf{x}\right)=\frac{\partial}{\partial\theta}\left[-n\log\theta-\frac{1}{\theta}\sum_{i=1}^{n}x_{i}\right]=-\frac{n}{\theta}+\frac{1}{\theta^{2}}\sum_{i=1}^{n}x_{i}.
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{n}{\hat{\theta}}=\frac{1}{\hat{\theta}^{2}}\sum_{i=1}^{n}x_{i}\implies n\hat{\theta}^{2}=\hat{\theta}\sum_{i=1}^{n}x_{i}\implies\hat{\theta}=\frac{1}{n}\sum_{i=1}^{n}x_{i}=\bar{x}.
\]

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $\log\mathcal{L}\left(\theta|\mathbf{x}\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that this is a maximum.
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta^{2}}\left[\mathcal{L}\left(\theta|\mathbf{x}\right)\right|_{\theta=\hat{\theta}}=\frac{\partial}{\partial\theta}\left[-\frac{n}{\theta}+\frac{1}{\theta^{2}}\sum_{i=1}^{n}x_{i}\right|_{\theta=\hat{\theta}}=\left[\frac{n}{\theta^{2}}-\frac{2}{\theta^{3}}\sum_{i=1}^{n}x_{i}\right|_{\theta=\hat{\theta}}=\frac{n}{\bar{x}^{2}}-\frac{2}{\bar{x}^{3}}n\bar{x}=-\frac{n}{\bar{x}^{2}}
\]

\end_inset

We have 
\begin_inset Formula $n>0$
\end_inset

 and 
\begin_inset Formula $x\geq0$
\end_inset

, so that 
\begin_inset Formula $\bar{x}^{2}\geq0$
\end_inset

, so that 
\begin_inset Formula $-n/\bar{x}^{2}<0$
\end_inset

 in the case that at least one 
\begin_inset Formula $x_{i}>0$
\end_inset

.
 It follows that 
\begin_inset Formula $\hat{\theta}=\bar{X}$
\end_inset

 is the MLE for 
\begin_inset Formula $\theta$
\end_inset

.
 The derivative of the log-likelihood can be written as
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log\mathcal{L}\left(\theta|\mathbf{x}\right)=-\frac{n}{\theta}+\frac{1}{\theta^{2}}\sum_{i=1}^{n}x_{i}=\frac{n\bar{x}}{\theta^{2}}-\frac{n}{\theta}=\frac{n}{\theta}\left(\frac{\bar{x}}{\theta}-1\right).
\]

\end_inset

The log-likelihood function attains its maximum at 
\begin_inset Formula $\theta=\bar{x}$
\end_inset

, so in the case that 
\begin_inset Formula $\theta<\bar{x}$
\end_inset

, we will have 
\begin_inset Formula $\bar{x}/\theta>1$
\end_inset

, so that 
\begin_inset Formula $\left(\bar{x}/\theta\right)-1>0$
\end_inset

, i.e., the derivative will be positive, so 
\begin_inset Formula $\log L\left(\theta|\mathbf{x}\right)$
\end_inset

 is increasing in 
\begin_inset Formula $\theta$
\end_inset

.
 In the case that 
\begin_inset Formula $\theta>\bar{x}$
\end_inset

, we will have 
\begin_inset Formula $\bar{x}/\theta<1$
\end_inset

, so that 
\begin_inset Formula $\left(\bar{x}/\theta\right)-1<0$
\end_inset

, i.e., the derivative will be negative, so 
\begin_inset Formula $\log L\left(\theta|\mathbf{x}\right)$
\end_inset

 is decreasing in 
\begin_inset Formula $\theta$
\end_inset

.
 Under the null hypothesis, we have 
\begin_inset Formula $\Theta_{0}=\left\{ \theta:\theta\leq\theta_{0}\right\} $
\end_inset

.
 If 
\begin_inset Formula $\theta_{0}\leq\bar{x}$
\end_inset

, then 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 will be maximized at 
\begin_inset Formula $\theta=\theta_{0}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)=\sup_{\theta=\theta_{0}}\theta^{-n}\mathrm{e}^{\sum_{i=1}^{n}-x_{i}/\theta}=\theta_{0}^{-n}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\theta_{0}}.
\]

\end_inset

If 
\begin_inset Formula $\theta_{0}>\bar{x}$
\end_inset

 , then 
\begin_inset Formula $L\left(\theta|\mathbf{x}\right)$
\end_inset

 will be maximized at 
\begin_inset Formula $\theta=\bar{x}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)=\sup_{\theta=\bar{x}}\theta^{-n}\mathrm{e}^{\sum_{i=1}^{n}-x_{i}/\theta}=\bar{x}^{-n}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\bar{x}}.
\]

\end_inset

The LRT test statistic is given by 
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\frac{\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)},
\]

\end_inset

so in the case that 
\begin_inset Formula $\theta_{0}>\bar{x}$
\end_inset

, we have
\begin_inset Formula 
\[
\frac{\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)}=\frac{\bar{x}^{-n}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\bar{x}}}{\bar{x}^{-n}\mathrm{e}^{-\sum_{i=1}^{n}x_{i}/\bar{x}}}=1.
\]

\end_inset

In the case that 
\begin_inset Formula $\theta_{0}\leq\bar{x}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)} & =\frac{\theta_{0}^{-n}\exp\left\{ -\frac{1}{\theta_{0}}\sum_{i=1}^{n}x_{i}\right\} }{\bar{x}^{-n}\exp\left\{ -\frac{1}{\bar{x}}\sum_{i=1}^{n}x_{i}\right\} }\\
 & =\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -\frac{1}{\theta_{0}}\sum_{i=1}^{n}x_{i}+\frac{1}{\bar{x}}\sum_{i=1}^{n}x_{i}\right\} \\
 & =\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -\frac{n\bar{x}}{\theta_{0}}+\frac{n\bar{x}}{\bar{x}}\right\} \\
 & =\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -n\left(\frac{\bar{x}}{\theta_{0}}-1\right)\right\} ,
\end{flalign*}

\end_inset

i.e., 
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\begin{cases}
1, & \theta_{0}>\bar{x}\\
\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -n\left(\frac{\bar{x}}{\theta_{0}}-1\right)\right\} , & \theta_{0}\leq\bar{x}
\end{cases}.
\]

\end_inset

In the case that 
\begin_inset Formula $\theta_{0}>\bar{x}$
\end_inset

, the LRT will never reject 
\begin_inset Formula $H_{0}$
\end_inset

.
 In the case that 
\begin_inset Formula $\theta_{0}\leq\bar{x}$
\end_inset

, the LRT rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
c\geq\lambda\left(\mathbf{x}\right)=\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -n\left(\frac{\bar{x}}{\theta_{0}}-1\right)\right\} \implies\log c\geq\log\left(\frac{\bar{x}}{\theta_{0}}\right)^{n}\exp\left\{ -n\left(\frac{\bar{x}}{\theta_{0}}-1\right)\right\} =n\log\frac{\bar{x}}{\theta_{0}}-n\left(\frac{\bar{x}}{\theta_{0}}-1\right).
\]

\end_inset


\end_layout

\begin_layout Theorem
If 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\lambda^{*}\left(t\right)$
\end_inset

 and 
\begin_inset Formula $\lambda\left(\mathbf{x}\right)$
\end_inset

 are the LRT statistics based on 
\begin_inset Formula $T$
\end_inset

 and 
\begin_inset Formula $\mathbf{X}$
\end_inset

, respectively, then 
\begin_inset Formula $\lambda^{*}\left(T\left(\mathbf{x}\right)\right)=\lambda\left(\mathbf{x}\right)$
\end_inset

 for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the sample space.
 (This is Theorem 8.2.4 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, the pdf or pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 can be written as 
\begin_inset Formula $f\left(\mathbf{x}|\theta\right)=g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)$
\end_inset

, where 
\begin_inset Formula $g\left(t|\theta\right)$
\end_inset

 is the pdf or pmf of 
\begin_inset Formula $T$
\end_inset

 and 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 Thus
\begin_inset Formula 
\begin{flalign*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)}\\
 & =\frac{\sup_{\Theta_{0}}f\left(\mathbf{x}|\theta\right)}{\sup_{\Theta}f\left(\mathbf{x}|\theta\right)}\\
 & =\frac{\sup_{\Theta_{0}}g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}{\sup_{\Theta}g\left(T\left(\mathbf{x}\right)|\theta\right)h\left(\mathbf{x}\right)}\\
 & =\frac{\sup_{\Theta_{0}}g\left(T\left(\mathbf{x}\right)|\theta\right)}{\sup_{\Theta}g\left(T\left(\mathbf{x}\right)|\theta\right)}\\
 & =\frac{\sup_{\Theta_{0}}\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)}{\sup_{\Theta}\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)}\\
 & =\lambda^{*}\left(T\left(\mathbf{x}\right)\right).
\end{flalign*}

\end_inset

The third equality follows from the Factorization Theorem (because 
\begin_inset Formula $T\left(\mathbf{x}\right)$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

).
 The fourth equality follows from the fact that 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so the terms in the numerator and denominator cancel.
 The fifth equality follows from the fact that 
\begin_inset Formula $g\left(T\left(\mathbf{x}\right)|\theta\right)$
\end_inset

 is the pdf or pmf of 
\begin_inset Formula $T$
\end_inset

, and the resulting ratio is just the LRT test statistic based on 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

, 
\begin_inset Formula $H_{0}$
\end_inset

, and 
\begin_inset Formula $H_{1}$
\end_inset

 be as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:LRT-normal-rv"

\end_inset

.
 Then, the LRT test statistic is
\begin_inset Formula 
\[
\lambda\left(\mathbf{x}\right)=\exp\left\{ -\frac{n}{2}\left(\theta_{0}-\bar{x}\right)^{2}\right\} ,
\]

\end_inset

so that the LRT rejects 
\begin_inset Formula $H_{0}:\theta=\theta_{0}$
\end_inset

 if
\begin_inset Formula 
\[
\left|\theta_{0}-\bar{x}\right|\geq\sqrt{-\frac{2}{n}\log c}.
\]

\end_inset

From the proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:computing-sums-rand-samples"

\end_inset

, the joint density of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 can be written as
\begin_inset Formula 
\begin{flalign*}
f_{\mathbf{X}}\left(\mathbf{x}|\theta\right) & =\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\theta\right)^{2}\right\} \\
 & =\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i=1}^{n}\left(\bar{x}-\theta\right)^{2}\right]\right\} \\
 & =\left(2\pi\right)^{-n/2}\exp\left\{ -\frac{1}{2}\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n\left(\bar{x}-\theta\right)^{2}\right]\right\} .
\end{flalign*}

\end_inset

Consider the statistic 
\begin_inset Formula $T\left(\mathbf{X}\right)=\bar{X}$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 has a 
\begin_inset Formula $N\left(\theta,1/n\right)$
\end_inset

 distribution, i.e., 
\begin_inset Formula 
\[
f_{T\left(\mathbf{X}\right)}\left(t|\theta\right)=\frac{1}{\sqrt{2\pi/n}}\exp\left\{ -\frac{1}{2}\frac{\left(t-\theta\right)^{2}}{1/n}\right\} =\sqrt{\frac{n}{2\pi}}\exp\left\{ -\frac{n}{2}\left(t-\theta\right)^{2}\right\} .
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
\frac{f_{\mathbf{X}}\left(\mathbf{x}|\theta\right)}{f_{T\left(\mathbf{X}\right)}\left(t|\theta\right)} & =\frac{\left(2\pi\right)^{-n/2}\exp\left\{ -\left(1/2\right)\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n\left(\bar{x}-\theta\right)^{2}\right]\right\} }{n^{1/2}\left(2\pi\right)^{-1/2}\exp\left\{ -\left(n/2\right)\left(t-\theta\right)^{2}\right\} }\\
 & =n^{-1/2}\left(2\pi\right)^{-\left(n-1\right)/2}\exp\left\{ -\frac{1}{2}\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n\left(\bar{x}-\theta\right)^{2}\right]+\frac{n}{2}\left(t-\theta\right)^{2}\right\} \\
 & =n^{-1/2}\left(2\pi\right)^{-\left(n-1\right)/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}-\frac{n}{2}\left(\bar{x}-\theta\right)^{2}+\frac{n}{2}\left(\bar{x}-\theta\right)^{2}\right\} \\
 & =n^{-1/2}\left(2\pi\right)^{-\left(n-1\right)/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right\} ,
\end{flalign*}

\end_inset

which does not depend on 
\begin_inset Formula $\theta$
\end_inset

, so it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:suff-stat-ratio"

\end_inset

 that 
\begin_inset Formula $T\left(\mathbf{X}\right)=\bar{X}$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

.
 The log-likelihood of 
\begin_inset Formula $\theta$
\end_inset

 given 
\begin_inset Formula $T\left(\mathbf{X}\right)=T\left(\mathbf{x}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\log\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)=\log\sqrt{\frac{n}{2\pi}}\exp\left\{ -\frac{n}{2}\left(t-\theta\right)^{2}\right\} =\frac{1}{2}\log\frac{n}{2\pi}-\frac{n}{2}\left(\bar{x}-\theta\right)^{2}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}\log\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)=\frac{\partial}{\partial\theta}\left[\frac{1}{2}\log\frac{n}{2\pi}-\frac{n}{2}\left(\bar{x}-\theta\right)^{2}\right]=0-n\left(\bar{x}-\theta\right)\cdot-1=n\left(\bar{x}-\theta\right).
\]

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
n\left(\bar{x}-\hat{\theta}\right)=0\implies n\hat{\theta}=n\bar{x}\implies\hat{\theta}=\bar{x}.
\]

\end_inset

We will evaluate the second derivative of 
\begin_inset Formula $\log\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)$
\end_inset

 with respect to 
\begin_inset Formula $\theta$
\end_inset

 at 
\begin_inset Formula $\theta=\hat{\theta}$
\end_inset

 to verify that this is a maximum.
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta^{2}}\left[\log\mathcal{L}^{*}\left(\theta|T\left(\mathbf{x}\right)\right)\right|_{\theta=\hat{\theta}}=\frac{\partial}{\partial\theta}\left[n\left(\bar{x}-\theta\right)\right|_{\theta=\hat{\theta}}=\left[0-n\right|_{\theta=\hat{\theta}}=-n
\]

\end_inset

We have 
\begin_inset Formula $-n<0$
\end_inset

, so it follows that 
\begin_inset Formula $\hat{\theta}=\bar{X}$
\end_inset

 is the MLE of 
\begin_inset Formula $T\left(\mathbf{X}\right)=\bar{X}$
\end_inset

.
 Then, the LRT test statistic based on 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\lambda^{*}\left(T\left(\mathbf{x}\right)\right)=\frac{\sup_{\Theta_{0}}\mathcal{L}^{*}\left(\theta|t\right)}{\sup_{\Theta}\mathcal{L}^{*}\left(\theta|t\right)}=\frac{\sqrt{n/2\pi}\exp\left\{ -\left(n/2\right)\left(\bar{x}-\theta_{0}\right)^{2}\right\} }{\sqrt{n/2\pi}\exp\left\{ -\left(n/2\right)\left(\bar{x}-\bar{x}\right)^{2}\right\} }=\exp\left\{ -\frac{n}{2}\left(\bar{x}-\theta_{0}\right)^{2}\right\} ,
\]

\end_inset

so that the LRT rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
c\geq\lambda^{*}\left(\mathbf{x}\right)=\exp\left\{ -\frac{n}{2}\left(\bar{x}-\theta_{0}\right)^{2}\right\} \implies\log c\geq-\frac{n}{2}\left(\bar{x}-\theta_{0}\right)^{2}\implies-\frac{2}{n}\log c\leq\left(\bar{x}-\theta_{0}\right)^{2}\implies\left|\bar{x}-\theta_{0}\right|\geq\sqrt{-\frac{2}{n}\log c}.
\]

\end_inset

We have
\begin_inset Formula $\left|\bar{x}-\theta_{0}\right|=\left|\theta_{0}-\bar{x}\right|$
\end_inset

, so we see that the LRT statistics based on 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 are equal for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal LRT with unknown variance]
\end_layout

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\overset{\text{iid}}{\sim}\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 with 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 unknown.
 Specify the LRT for 
\begin_inset Formula 
\[
H_{0}:\mu=\mu_{0}\quad\text{vs.}\quad H_{1}:\mu\neq\mu_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
The likelihood function is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{ -\frac{\left(x_{i}-\mu\right)^{2}}{2\sigma^{2}}\right\} =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right\} .
\]

\end_inset

Under the alternative hypothesis, we have 
\begin_inset Formula $\Theta=\left\{ \left(\mu,\sigma^{2}\right):\mu\neq\mu_{0},\sigma^{2}>0\right\} $
\end_inset

, so we must maximize 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 over the parameter space, i.e., find the MLEs of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The log-likelihood is given by
\begin_inset Formula 
\[
\log\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right)=\log\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right\} =-\frac{n}{2}\log2\pi\sigma^{2}-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\mu$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\mu}\log\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right) & =\frac{\partial}{\partial\mu}\left[-\frac{n}{2}\log2\pi\sigma^{2}-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right]\\
 & =0-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2\left(x_{i}-\mu\right)\cdot-1\\
 & =\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right).
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\hat{\mu}\right)=0\implies\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}\hat{\mu}=0\implies n\bar{x}=n\hat{\mu}\implies\hat{\mu}=\bar{x}.
\]

\end_inset

Taking the derivative of the log-likelihood with respect to 
\begin_inset Formula $\sigma^{2}$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\sigma^{2}}\log\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right) & =\frac{\partial}{\partial\sigma^{2}}\left[-\frac{n}{2}\log2\pi\sigma^{2}-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right]\\
 & =-\frac{n}{2}\frac{1}{2\pi\sigma^{2}}\left(2\pi\right)+\frac{1}{2\left(\sigma^{2}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\\
 & =-\frac{n}{2\sigma^{2}}+\frac{1}{2\left(\sigma^{2}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}.
\end{flalign*}

\end_inset

Setting this equal to zero, we have
\begin_inset Formula 
\[
\frac{1}{2\left(\hat{\sigma}^{2}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}=\frac{n}{2\hat{\sigma}^{2}}\implies2n\left(\hat{\sigma}^{2}\right)^{2}=2\hat{\sigma}^{2}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\implies\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}.
\]

\end_inset

Substituting 
\begin_inset Formula $\mu=\hat{\mu}=\bar{x}$
\end_inset

 into our expression for 
\begin_inset Formula $\hat{\sigma}^{2}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\sup_{\Theta}\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right) & =\mathcal{L}\left(\hat{\mu},\hat{\sigma}^{2}|\mathbf{x}\right)\\
 & =\left(2\pi\hat{\sigma}^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\hat{\sigma}^{2}}\sum_{i=1}^{n}\left(x_{i}-\hat{\mu}\right)^{2}\right\} \\
 & =\left(2\pi\right)^{-n/2}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{\frac{2}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right\} \\
 & =\left(2\pi\right)^{-n/2}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{-n/2}\exp\left\{ -\frac{n}{2}\right\} .
\end{flalign*}

\end_inset

Under the null hypothesis, we have 
\begin_inset Formula $\Theta_{0}=\left\{ \left(\mu,\sigma^{2}\right):\mu=\mu_{0},\sigma^{2}>0\right\} $
\end_inset

, i.e., only one value of 
\begin_inset Formula $\mu$
\end_inset

 is specified by 
\begin_inset Formula $H_{0}$
\end_inset

 and no restrictions are placed on the value of 
\begin_inset Formula $\sigma^{2}$
\end_inset

, so that we have
\begin_inset Formula 
\begin{flalign*}
\sup_{\Theta_{0}}\mathcal{L}\left(\mu,\sigma^{2}|\mathbf{x}\right) & =\mathcal{L}\left(\mu_{0},\hat{\sigma}^{2}|\mathbf{x}\right)\\
 & =\left(2\pi\hat{\sigma}^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\hat{\sigma}^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}\right\} \\
 & =\left(2\pi\right)^{-n/2}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)\right)^{-n/2}\exp\left\{ -\frac{1}{\frac{2}{n}\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}\right\} \\
 & =\left(2\pi\right)^{-n/2}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)\right)^{-n/2}\exp\left\{ -\frac{n}{2}\right\} .
\end{flalign*}

\end_inset

Then, the LRT test statistic is given by
\begin_inset Formula 
\begin{flalign*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\Theta_{0}}L\left(\mu,\sigma^{2}|\mathbf{x}\right)}{\sup_{\Theta}L\left(\mu,\sigma^{2}|\mathbf{x}\right)}\\
 & =\frac{\left(2\pi\right)^{-n/2}\left[\left(1/n\right)\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)\right]^{-n/2}\mathrm{e}^{-n/2}}{\left(2\pi\right)^{-n/2}\left[\left(1/n\right)\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]^{-n/2}\mathrm{e}^{-n/2}}\\
 & =\frac{\left[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right]^{n/2}}{\left[\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}\right]^{n/2}}\\
 & =\left[\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}}\right]^{n/2},
\end{flalign*}

\end_inset

so that the LRT rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\begin{flalign*}
c & \geq\lambda\left(\mathbf{x}\right)\\
 & =\left[\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}}\right]^{n/2}\\
\Leftrightarrow c^{2/n} & \geq\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\mu_{0}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-\mu_{0}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left[\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)\left(\bar{x}-\mu_{0}\right)+\left(\bar{x}-\mu_{0}\right)^{2}\right]}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}-\mu_{0}\right)\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)+n\left(\bar{x}-\mu_{0}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}-\mu_{0}\right)\left(\sum_{i=1}^{n}x_{i}-n\bar{x}\right)+n\left(\bar{x}-\mu_{0}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}-\mu_{0}\right)\left(n\bar{x}-n\bar{x}\right)+n\left(\bar{x}-\mu_{0}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n\left(\bar{x}-\mu_{0}\right)^{2}}\\
\Leftrightarrow\frac{1}{c^{2/n}} & \leq\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n\left(\bar{x}-\mu_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\\
 & =\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}+\frac{n\left(\bar{x}-\mu_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\\
 & =1+\frac{n\left(\bar{x}-\mu_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\\
\Leftrightarrow\frac{n\left(\bar{x}-\mu_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} & \geq\frac{1}{c^{2/n}}-1\\
\Leftrightarrow\frac{n\left(\bar{x}-\mu_{0}\right)^{2}}{\frac{n-1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} & \geq\frac{1}{c^{2/n}}-1.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula $S^{2}=\left[1/\left(n-1\right)\right]\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$
\end_inset

, so that we have
\begin_inset Formula 
\[
\frac{n\left(\bar{x}-\mu_{0}\right)^{2}}{\left(n-1\right)S^{2}}\geq\frac{1}{c^{2/n}}-1\implies\frac{\left(\bar{x}-\mu_{0}\right)^{2}}{S^{2}/n}\geq\left(n-1\right)\left(\frac{1}{c^{2/n}}-1\right).
\]

\end_inset

Then, the LRT rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula 
\[
\frac{\left(\bar{x}-\mu_{0}\right)^{2}}{S^{2}/n}\geq\left(n-1\right)\left(\frac{1}{c^{2/n}}-1\right)\implies\sqrt{\frac{\left(\bar{x}-\mu_{0}\right)^{2}}{S^{2}/n}}\geq\sqrt{\left(n-1\right)\left(\frac{1}{c^{2/n}}-1\right)}\implies\frac{\left|\bar{x}-\mu_{0}\right|}{S/\sqrt{n}}\geq c^{*},
\]

\end_inset

where 
\begin_inset Formula $c^{*}=\sqrt{\left(n-1\right)\left(1/c^{2/n}-1\right)}$
\end_inset

.
\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, 
\begin_inset Formula $\bar{X}$
\end_inset

 has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}/n\right)$
\end_inset

 random variable.
 Let 
\begin_inset Formula $U=\left(\bar{X}-\mu\right)/\left(\sigma/\sqrt{n}\right)$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ U\leq u\right\} \right) & =P\left(\left\{ \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\leq u\right\} \right)\\
 & =P\left(\left\{ \bar{X}-\mu\leq\frac{\sigma u}{\sqrt{n}}\right\} \right)\\
 & =P\left(\left\{ \bar{X}\leq\frac{\sigma u}{\sqrt{n}}+\mu\right\} \right)\\
 & =\int_{-\infty}^{\sigma u/\sqrt{n}+\mu}\frac{1}{\sqrt{2\pi\sigma^{2}/n}}\exp\left\{ -\frac{\left(\bar{x}-\mu\right)^{2}}{2\sigma^{2}/n}\right\} \dif\bar{x}.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula 
\[
s=\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\Leftrightarrow\bar{x}=\frac{\sigma s}{\sqrt{n}}+\mu,
\]

\end_inset

so that we have
\begin_inset Formula 
\[
\frac{\dif s}{\dif\bar{x}}=\frac{1}{\sigma/\sqrt{n}}\implies\dif\bar{x}=\frac{\sigma}{\sqrt{n}}\dif s
\]

\end_inset

and
\begin_inset Formula 
\[
s\left(\frac{\sigma u}{\sqrt{n}}+\mu\right)=\frac{\left(\sigma u/\sqrt{n}+\mu\right)-\mu}{\sigma/\sqrt{n}}=\frac{\sigma u/\sqrt{n}}{\sigma/\sqrt{n}}=u.
\]

\end_inset

Then, we have
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ U\leq u\right\} \right) & =\int_{-\infty}^{u}\frac{1}{\sqrt{2\pi\sigma^{2}/n}}\exp\left\{ -\frac{\left(\sigma s/\sqrt{n}+\mu-\mu\right)^{2}}{2\sigma^{2}/n}\right\} \frac{\sigma}{\sqrt{n}}\dif s\\
 & =\int_{-\infty}^{u}\frac{\sqrt{n}}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{\left(\sigma s/\sqrt{n}\right)^{2}}{2\sigma^{2}/n}\right\} \frac{\sigma}{\sqrt{n}}\dif s\\
 & =\int_{-\infty}^{u}\frac{1}{\sqrt{2\pi}}\exp\left\{ -\frac{\sigma^{2}s^{2}/n}{2\sigma^{2}/n}\right\} \dif s\\
 & =\int_{-\infty}^{u}\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-s^{2}/2}\dif s,
\end{flalign*}

\end_inset

which we recognize as the cdf of a 
\begin_inset Formula $\mathcal{N}\left(0,1^{2}\right)$
\end_inset

 random variable, i.e., 
\begin_inset Formula $U$
\end_inset

 has the distribution of a standard normal random variable.
 Now consider 
\begin_inset Formula $V=\sqrt{S^{2}/\sigma^{2}}$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, the quantity 
\begin_inset Formula $\left(n-1\right)S^{2}/\sigma^{2}$
\end_inset

 has a chi-squared distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom, so it follows that 
\begin_inset Formula $V=\sqrt{S^{2}/\sigma^{2}}\sim\sqrt{\chi_{n-1}^{2}/\left(n-1\right)}$
\end_inset

.
 Then, we have
\begin_inset Formula 
\[
\frac{\left|\bar{x}-\mu_{0}\right|}{S/\sqrt{n}}=\frac{\left|\bar{x}-\mu_{0}\right|}{S/\sqrt{n}}\frac{\sigma}{\sigma}=\frac{\left|\bar{x}-\mu_{0}\right|}{\sigma/\sqrt{n}}\frac{\sigma}{S}=\frac{\left|\bar{x}-\mu_{0}\right|/\left(\sigma/\sqrt{n}\right)}{S/\sigma}=\frac{\left|\bar{x}-\mu_{0}\right|/\left(\sigma/\sqrt{n}\right)}{\sqrt{S^{2}/\sigma^{2}}}=\frac{U}{V},
\]

\end_inset

where 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

 are independent.
 We wish to find the distribution of 
\begin_inset Formula $U/\sqrt{V/p}$
\end_inset

, where 
\begin_inset Formula $V\sim\chi_{p}^{2}$
\end_inset

.
 The joint pdf of 
\begin_inset Formula $U$
\end_inset

and 
\begin_inset Formula $V$
\end_inset

 is
\begin_inset Formula 
\[
f_{U,V}\left(u,v\right)=f_{U}\left(u\right)f_{V}\left(v\right)=\left[\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-u^{2}/2}\right]\left[\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}}v^{\left(p/2\right)-1}\mathrm{e}^{-v/2}\right],\quad u\in\mathbb{R},\quad v>0.
\]

\end_inset

Let 
\begin_inset Formula 
\begin{flalign*}
\mathcal{A} & =\left\{ \left(u,v\right):f_{U,V}\left(u,v\right)>0\right\} ,
\end{flalign*}

\end_inset

and make the transformation 
\begin_inset Formula 
\[
t=\frac{u}{\sqrt{v/p}},\quad w=v\implies u=t\sqrt{\frac{v}{p}}=t\sqrt{\frac{w}{p}},
\]

\end_inset

so that we have 
\begin_inset Formula 
\begin{flalign*}
\mathcal{B} & =\left\{ \left(t,w\right):t=u/\sqrt{v/p},w=v,\left(u,v\right)\in\mathcal{A}\right\} .
\end{flalign*}

\end_inset

Then, 
\begin_inset Formula $f_{T,W}\left(t,w\right)$
\end_inset

 will be positive on 
\begin_inset Formula $\mathcal{B}$
\end_inset

.
 We have 
\begin_inset Formula $u\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $p,v>0$
\end_inset

, so the quantity 
\begin_inset Formula $t=u/\sqrt{v/p}\in\mathbb{R}$
\end_inset

, and 
\begin_inset Formula $w>0$
\end_inset

.
 The Jacobian of the transformation is
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{2.5}
\end_layout

\end_inset


\begin_inset Formula 
\[
J=\begin{vmatrix}\dfrac{\partial u}{\partial t} & \dfrac{\partial u}{\partial w}\\
\dfrac{\partial v}{\partial t} & \dfrac{\partial v}{\partial w}
\end{vmatrix}=\begin{vmatrix}\sqrt{w/p} & 0\\
0 & 1
\end{vmatrix}=\sqrt{w/p},
\]

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

and the joint pdf of 
\begin_inset Formula $T$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{T,W}\left(t,w\right) & =f_{U,V}\left(t\sqrt{\frac{w}{p}},w\right)\left|J\right|\\
 & =\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\left(t\sqrt{w/p}\right)^{2}/2}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}}w^{\left(p/2\right)-1}\mathrm{e}^{-w/2}\left|\sqrt{\frac{w}{p}}\right|\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}}\mathrm{e}^{-\left(1/2\right)t^{2}w/p}w^{\left(p/2\right)-1}\mathrm{e}^{-w/2}w^{1/2}p^{-1/2}\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\mathrm{e}^{-\left(1/2\right)t^{2}w/p-w/2}w^{\left(p/2\right)-1/2}\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\mathrm{e}^{-\left(1/2\right)\left(t^{2}w/p+w\right)}w^{\left(p-1\right)/2+1-1}\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\mathrm{e}^{-\left(1/2\right)\left(t^{2}/p+1\right)w}w^{\left(p+1\right)/2-1},
\end{flalign*}

\end_inset

so that the marginal pdf of 
\begin_inset Formula $T$
\end_inset

 is given by
\begin_inset Formula 
\begin{flalign*}
f_{T}\left(t\right) & =\int_{0}^{\infty}f_{T,W}\left(t,w\right)\dif w\\
 & =\int_{0}^{\infty}\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\mathrm{e}^{-\left(1/2\right)\left(t^{2}/p+1\right)w}w^{\left(p+1\right)/2-1}\dif w\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\int_{0}^{\infty}\mathrm{e}^{-\left(1/2\right)\left(t^{2}/p+1\right)w}w^{\left(p+1\right)/2-1}\dif w.
\end{flalign*}

\end_inset

We recognize the integrand as the kernel of a 
\begin_inset Formula $\text{Gamma}\left(\left(p+1\right)/2,2/\left(t^{2}/p+1\right)\right)$
\end_inset

 random variable, so we write
\begin_inset Formula 
\begin{flalign*}
f_{T}\left(t\right) & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\Gamma\left(\frac{p+1}{2}\right)\left(\frac{2}{t^{2}/p+1}\right)^{\left(p+1\right)/2}\\
 & \quad\cdot\int_{0}^{\infty}\frac{1}{\Gamma\left(\left(p+1\right)/2\right)\left[2/\left(t^{2}/p+1\right)\right]^{\left(p+1\right)/2}}\mathrm{e}^{-\left(1/2\right)\left(t^{2}/p+1\right)w}w^{\left(p+1\right)/2-1}\dif w\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\Gamma\left(\frac{p+1}{2}\right)\left(\frac{2}{t^{2}/p+1}\right)^{\left(p+1\right)/2}\cdot1\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{\Gamma\left(\frac{p}{2}\right)2^{p/2}p^{1/2}}\Gamma\left(\frac{p+1}{2}\right)\left(\frac{2}{t^{2}/p+1}\right)^{\left(p+1\right)/2},
\end{flalign*}

\end_inset

which from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "def:t-distribution"

\end_inset

 is the pdf of a 
\begin_inset Formula $t_{p}=t_{n-1}$
\end_inset

 distribution.
\end_layout

\begin_layout Section
Methods of evaluating tests
\end_layout

\begin_layout Subsection
Error probabilities and the power function
\end_layout

\begin_layout Standard
A hypothesis test of 
\begin_inset Formula $H_{0}:\theta\in\Theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta\in\Theta_{0}^{c}$
\end_inset

 might make one of two types of errors.
 These two types of errors traditionally are known as Type I Error and Type
 II Error.
 If 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

 but the hypothesis test incorrectly decides to reject 
\begin_inset Formula $H_{0}$
\end_inset

, then the test has made a 
\shape italic
Type I Error
\shape default
.
 If, on the other hand, 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

 but the test decides to accept 
\begin_inset Formula $H_{0}$
\end_inset

, a 
\shape italic
Type II Error
\shape default
 has been made.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Two types of errors in hypothesis testing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Decision
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Accept 
\begin_inset Formula $H_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Reject 
\begin_inset Formula $H_{0}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{0}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Correct decision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Type I Error
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Type II Error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Correct decision
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

Suppose 
\begin_inset Formula $R$
\end_inset

 denotes the rejection region for a test.
 Then for 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

, the test will make a mistake if 
\begin_inset Formula $\mathbf{x}\in R$
\end_inset

, so the probability of a Type I Error is 
\begin_inset Formula $P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
 For 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

, the probability of a Type II Error is 
\begin_inset Formula $P_{\theta}\left(\left\{ \mathbf{X}\in R^{c}\right\} \right)$
\end_inset

.
 We have
\begin_inset Formula 
\[
P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)=\begin{cases}
\text{probability of a Type I Error} & \text{if }\theta\in\Theta_{0}\\
\text{one minus the probability of a Type II Error} & \text{if }\theta\in\Theta_{0}^{c}
\end{cases}.
\]

\end_inset


\end_layout

\begin_layout Definition
The 
\shape italic
power function
\shape default
 of a hypothesis test with rejection region 
\begin_inset Formula $R$
\end_inset

 is the function of 
\begin_inset Formula $\theta$
\end_inset

 defined by 
\begin_inset Formula $\beta\left(\theta\right)=P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
\end_layout

\begin_layout Standard
The ideal power function is 0 for all 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

 and 1 for all 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

.
 Except in trivial situations, this ideal cannot be attained.
 Qualitatively, a good test has a power function near 1 for most 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

 and near 0 for most 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

.
 Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:type-I-II-errors}
\end_layout

\end_inset

 shows the relationships among the critical region, Type I Error, Type II
 Error, and the power function.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<type-I-II-errors, echo=FALSE, fig.height=3, fig.width=6, fig.align='center',
 fig.pos='h', fig.cap='Type I and Type II errors'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(1.5,2,0.1,2))
\end_layout

\begin_layout Plain Layout

x <- seq(-5, 7.5, 0.01)  
\end_layout

\begin_layout Plain Layout

plot(x, dnorm(x), type="l",       
\end_layout

\begin_layout Plain Layout

	xaxt = "n", yaxt = "n",
\end_layout

\begin_layout Plain Layout

	xaxs = "i", yaxs = "i",
\end_layout

\begin_layout Plain Layout

	xlim = c(-3.5,6.5),
\end_layout

\begin_layout Plain Layout

	ann = F)
\end_layout

\begin_layout Plain Layout

mu2 <- 3.25
\end_layout

\begin_layout Plain Layout

crit <- 2
\end_layout

\begin_layout Plain Layout

lines(x, dnorm(x, mean = mu2), type="l")
\end_layout

\begin_layout Plain Layout

x.h0 <- seq(-6, 6, 0.01)
\end_layout

\begin_layout Plain Layout

x.type1 <- seq(crit, 6, 0.01)
\end_layout

\begin_layout Plain Layout

x.type2 <- seq(-6, crit, 0.01)
\end_layout

\begin_layout Plain Layout

polygon(c(-6, x.type2, crit), c(0, dnorm(x.type2, mean = mu2), 0),
\end_layout

\begin_layout Plain Layout

	col = "lightgoldenrod") 
\end_layout

\begin_layout Plain Layout

polygon(c(crit, x.type1, 7), c(0, dnorm(x.type1, mean = mu2), 0),
\end_layout

\begin_layout Plain Layout

	col = "slateblue4")
\end_layout

\begin_layout Plain Layout

polygon(c(-6,x.h0,6), c(0,dnorm(x.h0),0),
\end_layout

\begin_layout Plain Layout

	col = "slateblue4", density = 30, angle = 90)
\end_layout

\begin_layout Plain Layout

polygon(c(crit,x.type1,6), c(0, dnorm(-x.type1), 0),          
\end_layout

\begin_layout Plain Layout

	col = "red", density = 30, angle = 90)
\end_layout

\begin_layout Plain Layout

abline(v = 0, lty = 2)
\end_layout

\begin_layout Plain Layout

abline(v = crit, lty = 2)
\end_layout

\begin_layout Plain Layout

abline(v = mu2, lty = 2)
\end_layout

\begin_layout Plain Layout

axis(1, at = c(0,crit,mu2), 
\end_layout

\begin_layout Plain Layout

	labels = c(expression('H'[0]*' is true'),
\end_layout

\begin_layout Plain Layout

				expression(paste(alpha, ' boundary')),
\end_layout

\begin_layout Plain Layout

				expression('H'[1]*' is true')), 
\end_layout

\begin_layout Plain Layout

	cex.axis=0.75) 
\end_layout

\begin_layout Plain Layout

legend("topright", legend = c("Type I error", "Type II error", "Power"),
         
\end_layout

\begin_layout Plain Layout

	fill = c("red", "lightgoldenrod", "slateblue4"), 
\end_layout

\begin_layout Plain Layout

	density = c(30,120,120), angle = 90, cex = 0.75) 
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mathcal{N}\left(\theta,5^{2}\right)$
\end_inset

.
 Consider the hypothesis test
\begin_inset Formula 
\[
H_{0}:\theta\leq17\quad\text{vs.}\quad H_{1}:\theta>17.
\]

\end_inset

We will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\bar{X}>17+5/\sqrt{n}$
\end_inset

.
 Find the probabilities of Type I and Type II Errors.
\end_layout

\begin_layout Example
The parameter spaces associated with the null and alternative hypotheses
 are 
\begin_inset Formula 
\[
\Theta_{0}=\left\{ \theta:\theta\leq17\right\} \quad\text{and}\quad\Theta_{0}^{c}=\left\{ \theta:\theta>17\right\} .
\]

\end_inset

The probability of a Type I error is equal to the probability that we reject
 
\begin_inset Formula $H_{0}$
\end_inset

 given that 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

, i.e., 
\begin_inset Formula $P\left(\left\{ \mathbf{X}\in R|\theta\in\Theta_{0}\right\} \right)$
\end_inset

.
 The power function for this test is 
\begin_inset Formula $\beta\left(\theta\right)=P_{\theta}\left(\left\{ \bar{X}>17+5/\sqrt{n}\right\} \right)$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, 
\begin_inset Formula $\bar{X}$
\end_inset

 has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(\theta,\left(5/\sqrt{n}\right)^{2}\right)$
\end_inset

 random variable, so it follows that 
\begin_inset Formula 
\[
Z=\frac{\bar{X}-\theta}{5/\sqrt{n}}
\]

\end_inset

has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(0,1^{2}\right)$
\end_inset

 random variable, and that 
\begin_inset Formula $\bar{X}=\left(5/\sqrt{n}\right)Z+\theta$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\beta\left(\theta\right) & =P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)\\
 & =P_{\theta}\left(\left\{ \bar{X}>17+\frac{5}{\sqrt{n}}\right\} \right)\\
 & =P_{\theta}\left(\left\{ \frac{5}{\sqrt{n}}Z+\theta>17+\frac{5}{\sqrt{n}}\right\} \right)\\
 & =P_{\theta}\left(\left\{ \frac{5}{\sqrt{n}}Z>17+\frac{5}{\sqrt{n}}-\theta\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z>\left(17+\frac{5}{\sqrt{n}}-\theta\right)\frac{\sqrt{n}}{5}\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z>\frac{17\sqrt{n}}{5}+1-\frac{\theta\sqrt{n}}{5}\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z>\frac{17\sqrt{n}+5-\theta\sqrt{n}}{5}\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z>\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right\} \right)\\
 & =1-P_{\theta}\left(\left\{ Z\leq\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right\} \right)\\
 & =1-\Phi\left(\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right),
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\Phi\left(z\right)$
\end_inset

 is the cdf of 
\begin_inset Formula $Z$
\end_inset

.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:properties-of-cdf"

\end_inset

 that 
\begin_inset Formula $\Phi\left(z\right)$
\end_inset

 is a nondecreasing function of 
\begin_inset Formula $z$
\end_inset

, that 
\begin_inset Formula $\lim_{z\rightarrow-\infty}\Phi\left(z\right)=0$
\end_inset

, and that 
\begin_inset Formula $\lim_{z\rightarrow\infty}\Phi\left(z\right)=1$
\end_inset

.
 It follows that as 
\begin_inset Formula $\theta$
\end_inset

 increases from 
\begin_inset Formula $-\infty$
\end_inset

 to 
\begin_inset Formula $\infty$
\end_inset

, 
\begin_inset Formula $\Phi\left(z\right)$
\end_inset

 increases from 0 to 1.
 Then, the probability of a Type I Error is given by
\begin_inset Formula 
\begin{flalign*}
\alpha & =P\left(\left\{ \mathbf{X}\in R|\theta\in\Theta_{0}\right\} \right)\\
 & =\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\\
 & =\sup_{\theta\leq17}\left[1-\Phi\left(\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right)\right]\\
 & =1-\Phi\left(\frac{17-17+5/\sqrt{n}}{5/\sqrt{n}}\right)\\
 & =1-\Phi\left(1\right)\\
 & \approx1-\Sexpr{pnorm(1)}\\
 & =\Sexpr{1-pnorm(1)}.
\end{flalign*}

\end_inset

The probability of a Type II Error is equal to the probability that we fail
 to reject 
\begin_inset Formula $H_{0}$
\end_inset

 given that 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

, i.e., 
\begin_inset Formula $P\left(\left\{ \mathbf{X}\in R^{c}|\theta\in\Theta_{0}^{c}\right\} \right)$
\end_inset

.
 The probability that 
\begin_inset Formula $\mathbf{X}\in R^{c}$
\end_inset

 is given by
\begin_inset Formula 
\[
P_{\theta}\left(\left\{ \mathbf{X}\in R^{c}\right\} \right)=1-P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)=1-\beta\left(\theta\right)=1-\left(1-\Phi\left(\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right)\right)=\Phi\left(\frac{17-\theta+5/\sqrt{n}}{5/\sqrt{n}}\right).
\]

\end_inset

To calculate the probability of a Type II Error, we would require knowledge
 of the true value of 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Definition
For 
\begin_inset Formula $0\leq\alpha\leq1$
\end_inset

, a test with power function 
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

 is a 
\shape italic
size 
\begin_inset Formula $\alpha$
\end_inset

 test
\shape default
 if 
\begin_inset Formula $\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
If the data are continuous, it is possible to achieve any particular 
\begin_inset Formula $\alpha$
\end_inset

.
 If the data are discrete, though, it may not be possible to achieve any
 particular 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Definition
For 
\begin_inset Formula $0\leq\alpha\leq1$
\end_inset

, a test with power function 
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

 is a 
\shape italic
level 
\begin_inset Formula $\alpha$
\end_inset

 test
\shape default
 if 
\begin_inset Formula $\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\leq\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
Level 
\begin_inset Formula $\alpha$
\end_inset

 tests have Type I Error probabilities at most 
\begin_inset Formula $\alpha$
\end_inset

 for all 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{10}$
\end_inset

 be a random sample (of size 
\begin_inset Formula $n=10$
\end_inset

) from the distribution of a 
\begin_inset Formula $\text{Bernoulli}\left(\theta\right)$
\end_inset

 random variable.
 Consider the hypothesis test
\begin_inset Formula 
\[
H_{0}:\theta=0.5\quad\text{vs.}\quad H_{1}:\theta\neq0.5.
\]

\end_inset

We will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\geq8$
\end_inset

.
 Find the level of this test.
\end_layout

\begin_layout Example
Noting that the random variable 
\begin_inset Formula $Y=\sum_{i=1}^{n}X_{i}$
\end_inset

 has a 
\begin_inset Formula $\text{Binomial}\left(n,\theta\right)$
\end_inset

 distribution, the power function for this test is 
\begin_inset Formula 
\[
\beta\left(\theta\right)=P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)=P_{\theta}\left(\left\{ \sum_{i=1}^{n}X_{i}\geq8\right\} \right)=P_{\theta}\left(\left\{ Y\geq8\right\} \right)=\sum_{y=8}^{10}\binom{10}{y}\theta^{y}\left(1-\theta\right)^{10-y}.
\]

\end_inset

The level 
\begin_inset Formula $\alpha$
\end_inset

 of this test is given by
\begin_inset Formula 
\begin{flalign*}
\alpha & =\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\\
 & =\beta\left(\frac{1}{2}\right)\\
 & =\sum_{y=8}^{10}\binom{10}{y}\left(\frac{1}{2}\right)^{y}\left(1-\frac{1}{2}\right)^{10-y}\\
 & =\binom{10}{8}\left(\frac{1}{2}\right)^{8}\left(\frac{1}{2}\right)^{2}+\binom{10}{9}\left(\frac{1}{2}\right)^{9}\left(\frac{1}{2}\right)^{1}+\binom{10}{10}\left(\frac{1}{2}\right)^{10}\left(\frac{1}{2}\right)^{0}\\
 & =45\frac{1}{2^{10}}+10\frac{1}{2^{10}}+1\frac{1}{2^{10}}\\
 & =\frac{56}{1024}\\
 & \approx\Sexpr{sum(dbinom(seq(8,10),10,0.5))}.
\end{flalign*}

\end_inset

Note that it is not possible to construct a test of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 of size 
\begin_inset Formula $\alpha=0.05$
\end_inset

.
 If we change the rejection region such that we will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\geq7$
\end_inset

, then we will have a level 
\begin_inset Formula $\alpha=\Sexpr{sum(dbinom(seq(7,10),10,0.5))}$
\end_inset

 test.
 If we change the rejection region such that we will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\sum_{i=1}^{n}X_{i}\geq9$
\end_inset

, then we will have a level 
\begin_inset Formula $\alpha=\Sexpr{sum(dbinom(seq(9,10),10,0.5))}$
\end_inset

 test.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:LRT-size-alpha-normal-rv"

\end_inset

Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mathcal{N}\left(\theta,1^{2}\right)$
\end_inset

.
 Consider the hypothesis test
\begin_inset Formula 
\[
H_{0}:\theta=\theta_{0}\quad\text{vs.}\quad H_{1}:\theta\neq\theta_{0}.
\]

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:LRT-normal-rv"

\end_inset

, the likelihood ratio test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula 
\[
\left|\theta_{0}-\bar{X}\right|\geq\sqrt{-\frac{2}{n}\log c}=\frac{\sqrt{-2\log c}}{\sqrt{n}}\implies\left|\theta_{0}-\bar{X}\right|\sqrt{n}\geq\sqrt{-2\log c}\implies\frac{\left|\theta_{0}-\bar{X}\right|}{1/\sqrt{n}}\geq k,
\]

\end_inset

where 
\begin_inset Formula $c\in\left(0,1\right]$
\end_inset

, so that 
\begin_inset Formula $-2\log c\geq0$
\end_inset

, so that 
\begin_inset Formula $k=\sqrt{-2\log c}\geq0$
\end_inset

.
 Specify a size 
\begin_inset Formula $\alpha=0.05$
\end_inset

 test.
\end_layout

\begin_layout Example
A size 
\begin_inset Formula $\alpha$
\end_inset

 test is given by
\begin_inset Formula 
\begin{flalign*}
\alpha & =\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\\
 & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)\\
 & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \frac{\left|\theta_{0}-\bar{X}\right|}{1/\sqrt{n}}\geq k\right\} \right).
\end{flalign*}

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, 
\begin_inset Formula $\bar{X}$
\end_inset

 has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(\theta,\left(1/\sqrt{n}\right)^{2}\right)$
\end_inset

 random variable, so it follows that 
\begin_inset Formula 
\[
Z=\frac{\bar{X}-\theta}{1/\sqrt{n}}
\]

\end_inset

has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(0,1^{2}\right)$
\end_inset

 random variable, and that 
\begin_inset Formula $\bar{X}=\left(1/\sqrt{n}\right)Z+\theta$
\end_inset

.
 Then, we have
\begin_inset Formula 
\begin{flalign*}
\alpha & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \frac{\left|\theta_{0}-\bar{X}\right|}{1/\sqrt{n}}\geq k\right\} \right)\\
 & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \frac{\left|\theta_{0}-\left[\left(1/\sqrt{n}\right)Z+\theta\right]\right|}{1/\sqrt{n}}\geq k\right\} \right)\\
 & =P_{\theta}\left(\left\{ \frac{\left|\theta_{0}-\left(1/\sqrt{n}\right)Z-\theta_{0}\right|}{1/\sqrt{n}}\geq k\right\} \right)\\
 & =P_{\theta}\left(\left\{ \frac{\left(1/\sqrt{n}\right)\left|-Z\right|}{1/\sqrt{n}}\geq k\right\} \right)\\
 & =P_{\theta}\left(\left\{ \left|-Z\right|\geq k\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z\geq k\right\} \cup\left\{ -Z\geq k\right\} \right)\\
 & =P_{\theta}\left(\left\{ Z\geq k\right\} \right)+P\left(\left\{ Z\leq-k\right\} \right),
\end{flalign*}

\end_inset

where the final equality follows from the fact that 
\begin_inset Formula $\left\{ Z\geq k\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ Z\leq-k\right\} $
\end_inset

 are disjoint.
 From the symmetry of 
\begin_inset Formula $\Phi\left(z\right)$
\end_inset

, the cdf of 
\begin_inset Formula $Z$
\end_inset

, we have 
\begin_inset Formula $P_{\theta}\left(\left\{ Z\geq k\right\} \right)=P_{\theta}\left(\left\{ Z\leq-k\right\} \right)$
\end_inset

, and it follows that
\begin_inset Formula 
\[
\alpha=P_{\theta}\left(\left\{ Z\geq k\right\} \right)+P_{\theta}\left(\left\{ Z\leq-k\right\} \right)=P_{\theta}\left(\left\{ Z\leq-k\right\} \right)+P_{\theta}\left(\left\{ Z\leq-k\right\} \right)=2\Phi\left(-k\right).
\]

\end_inset

For 
\begin_inset Formula $\alpha=0.05$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0.05 & =2\Phi\left(-k\right)\implies\Phi\left(-k\right)=0.025\implies k\approx\Sexpr{-qnorm(0.025)}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Subsection
Most powerful tests
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mathcal{C}$
\end_inset

 be a class of tests for testing 
\begin_inset Formula $H_{0}:\theta\in\Theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta\in\Theta_{0}^{c}$
\end_inset

.
 A test in class 
\begin_inset Formula $\mathcal{C}$
\end_inset

, with power function 
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

, is a 
\shape italic
uniformly most powerful
\shape default
 (UMP) 
\shape italic
class 
\begin_inset Formula $\mathcal{C}$
\end_inset

 test
\shape default
 if 
\begin_inset Formula $\beta\left(\theta\right)\geq\beta'\left(\theta\right)$
\end_inset

 for every 
\begin_inset Formula $\theta\in\Theta_{0}^{c}$
\end_inset

 and every 
\begin_inset Formula $\beta'\left(\theta\right)$
\end_inset

 that is a power function of a test in class 
\begin_inset Formula $\mathcal{C}$
\end_inset

.
\end_layout

\begin_layout Standard
The class 
\begin_inset Formula $\mathcal{C}$
\end_inset

 will be the class of all level 
\begin_inset Formula $\alpha$
\end_inset

 tests.
 The test described in the definition above is then called a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Neyman-Pearson Lemma]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:neyman-pearson"

\end_inset

Consider testing 
\begin_inset Formula $H_{0}:\theta=\theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta=\theta_{1}$
\end_inset

, where the pdf or pmf corresponding to 
\begin_inset Formula $\theta_{i}$
\end_inset

 is 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{i}\right)$
\end_inset

, 
\begin_inset Formula $i=0,1$
\end_inset

, using a test with rejection region 
\begin_inset Formula $R$
\end_inset

 that satisfies
\begin_inset Formula 
\begin{equation}
\begin{split} & \mathbf{x}\in R\quad\text{if}\quad f\left(\mathbf{x}|\theta_{1}\right)>kf\left(\mathbf{x}|\theta_{0}\right)\\
\text{and}\\
 & \mathbf{x}\in R^{c}\quad\text{if}\quad f\left(\mathbf{x}|\theta_{1}\right)<kf\left(\mathbf{x}|\theta_{0}\right),
\end{split}
\label{eq:neyman-pearson-condition-1}
\end{equation}

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, and
\begin_inset Formula 
\begin{equation}
\alpha=P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right).\label{eq:neyman-pearson-condition-2}
\end{equation}

\end_inset

Then
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset

(Sufficiency) Any test that satisfies 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

 is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

(Necessity) If there exists a test satifying 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

 with 
\begin_inset Formula $k>0$
\end_inset

, then every UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test is a size 
\begin_inset Formula $\alpha$
\end_inset

 test (satisfies 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

) and every UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test satisfies 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 except perhaps on a set 
\begin_inset Formula $A$
\end_inset

 satisfying 
\begin_inset Formula $P_{\theta_{0}}\left(\left\{ \mathbf{X}\in A\right\} \right)=P_{\theta_{1}}\left(\left\{ \mathbf{X}\in A\right\} \right)=0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Theorem
(This is Theorem 8.3.12 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
We will prove the theorem for the case that 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{0}\right)$
\end_inset

 and 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{1}\right)$
\end_inset

 are pdfs of continuous random variables.
 The proof for discrete random variables can be accomplished by replacing
 integrals with sums.
\end_layout

\begin_layout Proof
Note first that any test satisfying 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

 is a size 
\begin_inset Formula $\alpha$
\end_inset

 and, hence, a level 
\begin_inset Formula $\alpha$
\end_inset

 test because 
\begin_inset Formula $\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \mathbf{X}\in R\right\} \right)=P_{\theta_{0}}\left(\mathbf{X}\in R\right)=\alpha$
\end_inset

, since 
\begin_inset Formula $\Theta_{0}$
\end_inset

 has only one point.
\end_layout

\begin_layout Proof
To ease notation, we define a 
\shape italic
test function
\shape default
, a function on the sample space that is 1 if 
\begin_inset Formula $\mathbf{x}\in R$
\end_inset

 and 0 if 
\begin_inset Formula $\mathbf{x}\in R^{c}$
\end_inset

.
 That is, it is the indicator function of the rejection region.
 Let 
\begin_inset Formula $\phi\left(\mathbf{x}\right)$
\end_inset

 be the test function of a test satisfying 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

.
 Let 
\begin_inset Formula $\phi'\left(\mathbf{x}\right)$
\end_inset

 be the test function of any other level 
\begin_inset Formula $\alpha$
\end_inset

 test, and let 
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

 and 
\begin_inset Formula $\beta'\left(\theta\right)$
\end_inset

 be the power functions corresponding to the tests 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\phi'$
\end_inset

, respectively.
 Because 
\begin_inset Formula $0\leq\phi'\left(\mathbf{x}\right)\leq1,$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 implies that 
\begin_inset Formula $\left(\phi\left(\mathbf{x}\right)-\phi'\left(\mathbf{x}\right)\right)\left(f\left(\mathbf{x}|\theta_{1}\right)-kf\left(\mathbf{x}|\theta_{0}\right)\right)\geq0$
\end_inset

 for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

 (since 
\begin_inset Formula $\phi=1$
\end_inset

 if 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{1}\right)>kf\left(\mathbf{x}|\theta_{0}\right)$
\end_inset

 and 
\begin_inset Formula $\phi=0$
\end_inset

 if 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{1}\right)<kf\left(\mathbf{x}|\theta_{0}\right)$
\end_inset

).
 Thus
\begin_inset Formula 
\begin{flalign}
0 & \leq\int\left[\phi\left(\mathbf{x}\right)-\phi'\left(\mathbf{x}\right)\right]\left[f\left(\mathbf{x}|\theta_{1}\right)-kf\left(\mathbf{x}|\theta_{0}\right)\right]\dif\mathbf{x}\label{eq:neyman-pearson-proof-inequality}\\
 & =\int\left[\phi\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta_{1}\right)-\phi\left(\mathbf{x}\right)kf\left(\mathbf{x}|\theta_{0}\right)-\phi'\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta_{1}\right)+\phi'\left(\mathbf{x}\right)kf\left(\mathbf{x}|\theta_{0}\right)\right]\dif\mathbf{x}\nonumber \\
 & =\int\phi\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}-\int\phi\left(\mathbf{x}\right)kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}-\int\phi'\left(\mathbf{x}\right)f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+\int\phi'\left(\mathbf{x}\right)kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}\nonumber \\
 & =\left[\int_{R}1\cdot f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+\int_{R^{c}}0\cdot f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}\right]-\left[\int_{R}1\cdot kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}+\int_{R^{c}}0\cdot kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}\right]\nonumber \\
 & \quad-\left[\int_{R'}1\cdot f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+\int_{\left(R'\right)^{c}}0\cdot f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}\right]+\left[\int_{R'}1\cdot kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}+\int_{\left(R'\right)^{c}}0\cdot kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}\right]\nonumber \\
 & =\left[\int_{R}f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+0\right]-\left[\int_{R}kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}+0\right]-\left[\int_{R'}f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+0\right]+\left[\int_{R'}kf\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}\right]\nonumber \\
 & =\int_{R}f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}-k\int_{R}f\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}-\int_{R'}f\left(\mathbf{x}|\theta_{1}\right)\dif\mathbf{x}+k\int_{R'}f\left(\mathbf{x}|\theta_{0}\right)\dif\mathbf{x}\nonumber \\
 & =P_{\theta_{1}}\left(\left\{ \mathbf{X}\in R\right\} \right)-kP_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)-P_{\theta_{1}}\left(\left\{ \mathbf{X}\in R'\right\} \right)+kP_{\theta_{0}}\left(\left\{ \mathbf{X}\in R'\right\} \right)\nonumber \\
 & =\beta\left(\theta_{1}\right)-k\beta\left(\theta_{0}\right)-\beta'\left(\theta_{1}\right)+k\beta'\left(\theta_{0}\right)\nonumber \\
 & =\beta\left(\theta_{1}\right)-\beta'\left(\theta_{1}\right)-k\left(\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)\right).\nonumber 
\end{flalign}

\end_inset


\end_layout

\begin_layout Proof
Statement (a) is proved by noting that, since 
\begin_inset Formula $\phi'$
\end_inset

 is a level 
\begin_inset Formula $\alpha$
\end_inset

 test and 
\begin_inset Formula $\phi$
\end_inset

 is a size 
\begin_inset Formula $\alpha$
\end_inset

 test, 
\begin_inset Formula 
\[
\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)=\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)-\beta'\left(\theta_{0}\right)=\alpha-\beta'\left(\theta_{0}\right)\geq0.
\]

\end_inset

Thus 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-proof-inequality"

\end_inset

 and 
\begin_inset Formula $k\geq0$
\end_inset

 imply that
\begin_inset Formula 
\[
0\leq\beta\left(\theta_{1}\right)-\beta'\left(\theta_{1}\right)-k\left(\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)\right)\leq\beta\left(\theta_{1}\right)-\beta'\left(\theta_{1}\right),
\]

\end_inset

showing that 
\begin_inset Formula $\beta\left(\theta_{1}\right)\geq\beta'\left(\theta_{1}\right)$
\end_inset

 and hence 
\begin_inset Formula $\phi$
\end_inset

 has greater power than 
\begin_inset Formula $\phi'$
\end_inset

.
 Since 
\begin_inset Formula $\phi'$
\end_inset

 was an arbitrary level 
\begin_inset Formula $\alpha$
\end_inset

 test and 
\begin_inset Formula $\theta_{1}$
\end_inset

 is the only point in 
\begin_inset Formula $\Theta_{0}^{c}$
\end_inset

, 
\begin_inset Formula $\phi$
\end_inset

 is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Proof
To prove statement (b), let 
\begin_inset Formula $\phi'$
\end_inset

 now be the test function for any UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
 By part (a), 
\begin_inset Formula $\phi$
\end_inset

, the test satisfying 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-2"

\end_inset

, is also a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test, thus 
\begin_inset Formula $\beta\left(\theta_{1}\right)=\beta'\left(\theta_{1}\right)$
\end_inset

.
 This fact, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-proof-inequality"

\end_inset

, and 
\begin_inset Formula $k>0$
\end_inset

 imply
\begin_inset Formula 
\[
0\leq\beta\left(\theta_{1}\right)-\beta'\left(\theta_{1}\right)-k\left(\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)\right)=-k\left(\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)\right)\implies\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)\leq0.
\]

\end_inset

Now, since 
\begin_inset Formula $\phi'$
\end_inset

 is a level 
\begin_inset Formula $\alpha$
\end_inset

 test, 
\begin_inset Formula $\sup_{\theta\in\Theta_{0}}\beta'\left(\theta\right)=\beta'\left(\theta_{0}\right)\leq\alpha$
\end_inset

.
 We also have 
\begin_inset Formula 
\[
\beta\left(\theta_{0}\right)-\beta'\left(\theta_{0}\right)=\alpha-\beta'\left(\theta_{0}\right)\leq0\implies\alpha\leq\beta'\left(\theta_{0}\right),
\]

\end_inset

so it follows that 
\begin_inset Formula $\beta'\left(\theta_{0}\right)=\alpha$
\end_inset

, that is, 
\begin_inset Formula $\phi'$
\end_inset

 is a size 
\begin_inset Formula $\alpha$
\end_inset

 test, and this also implies that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-proof-inequality"

\end_inset

 is an equality in this case.
 But the nonnegative integrated 
\begin_inset Formula $\left(\phi\left(\mathbf{x}\right)-\phi'\left(\mathbf{x}\right)\right)\left(f\left(\mathbf{x}|\theta_{1}\right)-kf\left(\mathbf{x}|\theta_{0}\right)\right)$
\end_inset

 will have a zero integral only if 
\begin_inset Formula $\phi'$
\end_inset

 satisfies 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

, except perhaps on a set 
\begin_inset Formula $A$
\end_inset

 with 
\begin_inset Formula $\int_{A}f\left(\mathbf{x}|\theta_{i}\right)\text{d}\mathbf{x}=0$
\end_inset

.
 This implies that the last assertion in statement (b) is true.
\end_layout

\begin_layout Corollary
\begin_inset CommandInset label
LatexCommand label
name "cor:neyman-pearson-corr"

\end_inset

Consider the hypothesis problem posed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

.
 Suppose 
\begin_inset Formula $T\left(\mathbf{X}\right)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $g\left(t|\theta_{i}\right)$
\end_inset

 is the pdf or pmf of 
\begin_inset Formula $T$
\end_inset

 corresponding to 
\begin_inset Formula $\theta_{i}$
\end_inset

, 
\begin_inset Formula $i=0,1$
\end_inset

.
 Then any test based on 
\begin_inset Formula $T$
\end_inset

 with rejection region 
\begin_inset Formula $S$
\end_inset

 (a subset of the sample space of 
\begin_inset Formula $T$
\end_inset

) is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test if it satisfies
\begin_inset Formula 
\begin{equation}
\begin{split} & t\in S\quad\text{if}\quad g\left(t|\theta_{1}\right)>kg\left(t|\theta_{0}\right)\\
\text{and}\\
 & t\in S^{c}\quad\text{if}\quad g\left(t|\theta_{1}\right)<kg\left(t|\theta_{0}\right),
\end{split}
\label{eq:neyman-pearson-corr-cond1}
\end{equation}

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, where
\begin_inset Formula 
\begin{equation}
\alpha=P_{\theta_{0}}\left(\left\{ T\in S\right\} \right).\label{eq:neyman-pearson-corr-cond2}
\end{equation}

\end_inset

(This is Corollary 8.3.13 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
In terms of the original sample 
\begin_inset Formula $\mathbf{X}$
\end_inset

, the test based on 
\begin_inset Formula $T$
\end_inset

 has the rejection region 
\begin_inset Formula $R=\left\{ \mathbf{x}:T\left(\mathbf{x}\right)\in S\right\} $
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

, the pdf or pmf of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 can be written as 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{i}\right)=g\left(T\left(\mathbf{x}\right)|\theta_{i}\right)h\left(\mathbf{x}\right)$
\end_inset

, 
\begin_inset Formula $i=0,1$
\end_inset

, for some nonnegative function 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

.
 Multiplying the inequalities in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-corr-cond1"

\end_inset

 by this nonnegative function, we see that 
\begin_inset Formula $R$
\end_inset

 satisfies
\begin_inset Formula 
\[
\mathbf{x}\in R\text{ if }f\left(\mathbf{x}|\theta_{1}\right)=g\left(T\left(\mathbf{x}\right)|\theta_{1}\right)h\left(\mathbf{x}\right)>kg\left(T\left(\mathbf{x}\right)|\theta_{0}\right)h\left(\mathbf{x}\right)=kf\left(\mathbf{x}|\theta_{0}\right)
\]

\end_inset

and
\begin_inset Formula 
\[
\mathbf{x}\in R^{c}\text{ if }f\left(\mathbf{x}|\theta_{1}\right)=g\left(T\left(\mathbf{x}\right)|\theta_{1}\right)h\left(\mathbf{x}\right)<kg\left(T\left(\mathbf{x}\right)|\theta_{0}\right)h\left(\mathbf{x}\right)=kf\left(\mathbf{x}|\theta_{0}\right).
\]

\end_inset

Also, by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-corr-cond2"

\end_inset

, 
\begin_inset Formula 
\[
P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)=P_{\theta_{0}}\left(T\left(\mathbf{X}\right)\in S\right)=\alpha.
\]

\end_inset

So, by the sufficient part of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

, the test based on 
\begin_inset Formula $T$
\end_inset

 is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Standard
When we derive a test that satisfies the inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:neyman-pearson-condition-1"

\end_inset

, and hence is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test, it is usually easier to rewrite the inequalities as 
\begin_inset Formula $f\left(\mathbf{x}|\theta_{1}\right)/f\left(\mathbf{x}|\theta_{0}\right)>k$
\end_inset

.
 (We must be careful about dividing by 0.)
\end_layout

\begin_layout Example
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:LRT-size-alpha-normal-rv"

\end_inset

, we found that a size 
\begin_inset Formula $\alpha=0.05$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 for 
\begin_inset Formula $k\approx1.96$
\end_inset

.
 Find an expression for the minimum sample size such that the power of the
 test is 0.90.
\end_layout

\begin_layout Example
The power of a test is equal to the probability that it rejects the null
 hypothesis given that the alternative hypothesis is true.
 A size 
\begin_inset Formula $\alpha=0.05$
\end_inset

 test will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula 
\[
\frac{\left|\theta_{0}-\bar{X}\right|}{1/\sqrt{n}}\geq k\Leftrightarrow\left|\theta_{0}-\bar{X}\right|\geq\frac{1.96}{\sqrt{n}},
\]

\end_inset

so that the power of such a test is given by 
\begin_inset Formula $\text{power}=P\left(\left\{ \left|\theta_{0}-\bar{X}\right|\geq1.96/\sqrt{n}|\theta=\theta_{1}\right\} \right)$
\end_inset

.
 Recalling that 
\begin_inset Formula $\bar{X}\sim\mathcal{N}\left(\theta,\left(1/\sqrt{n}\right)^{2}\right)$
\end_inset

 and letting 
\begin_inset Formula 
\[
Z=\frac{\bar{X}-\theta}{1/\sqrt{n}}\sim\mathcal{N}\left(0,1^{2}\right),
\]

\end_inset

we have
\begin_inset Formula 
\begin{flalign*}
\text{power} & =P\left(\left\{ \left|\theta_{0}-\bar{X}\right|\geq\frac{1.96}{\sqrt{n}}|\theta=\theta_{1}\right\} \right)\\
 & =P\left(\left\{ \left|\theta_{0}-\left(\frac{1}{\sqrt{n}}Z+\theta\right)\right|\geq\frac{1.96}{\sqrt{n}}|\theta=\theta_{1}\right\} \right)\\
 & =P\left(\left\{ \left|\theta_{0}-\frac{1}{\sqrt{n}}Z-\theta_{1}\right|\geq\frac{1.96}{\sqrt{n}}\right\} \right)\\
 & =P\left(\left\{ \left|-\frac{1}{\sqrt{n}}Z+\left(\theta_{0}-\theta_{1}\right)\right|\geq\frac{1.96}{\sqrt{n}}\right\} \right)\\
 & =P\left(\left\{ -\frac{1}{\sqrt{n}}Z+\left(\theta_{0}-\theta_{1}\right)\geq\frac{1.96}{\sqrt{n}}\right\} \cup\left\{ -\left(-\frac{1}{\sqrt{n}}Z+\left(\theta_{0}-\theta_{1}\right)\right)\geq\frac{1.96}{\sqrt{n}}\right\} \right)\\
 & =P\left(\left\{ -\frac{1}{\sqrt{n}}Z\geq\frac{1.96}{\sqrt{n}}-\left(\theta_{0}-\theta_{1}\right)\right\} \cup\left\{ \frac{1}{\sqrt{n}}Z\geq\frac{1.96}{\sqrt{n}}+\left(\theta_{0}-\theta_{1}\right)\right\} \right)\\
 & =P\left(\left\{ Z\leq-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \cup\left\{ Z\geq1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right)\\
 & =P\left(\left\{ Z\leq-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right)+P\left(\left\{ Z\geq1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right).
\end{flalign*}

\end_inset

From the symmetry of 
\begin_inset Formula $\Phi\left(z\right)$
\end_inset

, the cdf of 
\begin_inset Formula $Z$
\end_inset

, we have 
\begin_inset Formula $P\left(\left\{ Z\geq k\right\} \right)=P\left(\left\{ Z\leq-k\right\} \right)$
\end_inset

, so it follows that
\begin_inset Formula 
\begin{flalign*}
\text{power} & =P\left(\left\{ Z\leq-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right)+P\left(\left\{ Z\geq1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right)\\
 & =P\left(\left\{ Z\leq-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right\} \right)+P\left(\left\{ Z\leq-\left(1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right)\right\} \right)\\
 & =\Phi\left(-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right)+\Phi\left(-1.96-\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right).
\end{flalign*}

\end_inset

This function is plotted below.
 
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<echo = FALSE, fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos
 = 'h', fig.cap = 'power function for $H_{0}-H_{1}=0.2$'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

n <- seq(1, 1000)  
\end_layout

\begin_layout Plain Layout

sample.power <- function(n, size, theta_diff, tails = 2) {   
\end_layout

\begin_layout Plain Layout

	z_crit <- qnorm(size / tails)   
\end_layout

\begin_layout Plain Layout

	return(pnorm(z_crit + theta_diff * sqrt(n)) +             
\end_layout

\begin_layout Plain Layout

		pnorm(z_crit - theta_diff * sqrt(n))) 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

plot(n, sample.power(n, 0.05, 0.2), type="l", cex.lab=0.75, 
\end_layout

\begin_layout Plain Layout

	cex.axis=0.75, ylab = "power")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
For a desired power of 0.90, we have
\begin_inset Formula 
\begin{flalign*}
0.90 & =\Phi\left(-1.96+\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right)+\Phi\left(-1.96-\left(\theta_{0}-\theta_{1}\right)\sqrt{n}\right).
\end{flalign*}

\end_inset

For specified values of 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

, it is then possible to choose 
\begin_inset Formula $n$
\end_inset

 such that the resulting expression is (approximately) equal to 0.90.
 (Because the function is monotone and 
\begin_inset Formula $n$
\end_inset

 is discrete, there will be a single value of 
\begin_inset Formula $n$
\end_inset

 such that the power of the function evaluated at 
\begin_inset Formula $n$
\end_inset

 is greater than or equal to 0.90, but the power of the function evaluated
 at 
\begin_inset Formula $n-1$
\end_inset

 is less than 0.90.) We can now find the required sample size, implemented
 in the code below.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

sample.size <- function(power, size, theta_diff, tails = 2) {   
\end_layout

\begin_layout Plain Layout

	z_crit <- qnorm(size / tails)   
\end_layout

\begin_layout Plain Layout

	n <- 1   
\end_layout

\begin_layout Plain Layout

	x <- 0   
\end_layout

\begin_layout Plain Layout

	while (x < power) {     
\end_layout

\begin_layout Plain Layout

		x <- pnorm(z_crit + theta_diff * sqrt(n)) +       
\end_layout

\begin_layout Plain Layout

			pnorm(z_crit - theta_diff * sqrt(n))     
\end_layout

\begin_layout Plain Layout

	n <- n + 1   
\end_layout

\begin_layout Plain Layout

	}   
\end_layout

\begin_layout Plain Layout

	return(n - 1) 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $\theta_{0}-\theta_{1}=0.2$
\end_inset

.
 Then, the required sample size such that the test has a power of 0.90 is
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Sexpr{sample.size(0.9, 0.05, 0.2)}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $X$
\end_inset

 is a discrete random variable whose distributions under 
\begin_inset Formula $H_{0}$
\end_inset

 and under 
\begin_inset Formula $H_{1}$
\end_inset

 are shown below.
 Use Neyman-Pearson to find the most powerful test of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 with 
\begin_inset Formula $\alpha=0.04$
\end_inset

.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="8">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f\left(x|H_{0}\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.94
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f\left(x|H_{1}\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.04
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.03
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.79
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

, the uniformly most powerful test of level 
\begin_inset Formula $\alpha$
\end_inset

 of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 will be one that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
\frac{f\left(\mathbf{x}|H_{1}\right)}{f\left(\mathbf{x}|H_{0}\right)}>k\qquad\text{when }\mathbf{x}\in R
\]

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, where 
\begin_inset Formula $R$
\end_inset

 denotes the rejection region, and 
\begin_inset Formula $\alpha=P\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
 Noting that we have 
\begin_inset Formula $\mathbf{X}=X$
\end_inset

, the distribution of the ratio of the pmfs is
\begin_inset Formula 
\[
\frac{f\left(x|H_{1}\right)}{f\left(x|H_{0}\right)}=\begin{cases}
0.06/0.01=6, & x=1\\
0.05/0.01=5, & x=2\\
0.04/0.01=4, & x=3\\
0.03/0.01=3, & x=4\\
0.02/0.01=2, & x=5\\
0.01/0.01=1, & x=6\\
0.79/0.94=79/94, & x=7
\end{cases},
\]

\end_inset

so we must choose 
\begin_inset Formula $k$
\end_inset

 such that the corresponding rejection region satisfies 
\begin_inset Formula $P\left(\left\{ X\in R\right\} \right)=0.04$
\end_inset

.
 Recalling that the size of a test is equal to the probability that it rejects
 
\begin_inset Formula $H_{0}$
\end_inset

 given that 
\begin_inset Formula $H_{0}$
\end_inset

 is true and the power of a test is equal to the probability that it rejects
 
\begin_inset Formula $H_{0}$
\end_inset

 when 
\begin_inset Formula $H_{1}$
\end_inset

 is true, the following table shows the possible choices for 
\begin_inset Formula $k$
\end_inset

.
 Note that the outcomes are mutually exclusive, so we can simply add the
 corresponding probabilities, e.g., 
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X\in\left\{ 1,2\right\} |H_{0}\right\} \right) & =P\left(\left\{ X=1|H_{0}\right\} \cup\left\{ X=2|H_{0}\right\} \right)\\
 & =P\left(\left\{ X=1|H_{0}\right\} \right)+P\left(\left\{ X=2|H_{0}\right\} \right)\\
 & =0.01+0.01\\
 & =0.02.
\end{flalign*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $R$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{size}=P\left(\left\{ X\in R|H_{0}\right\} \right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{power}=P\left(\left\{ X\in R|H_{1}\right\} \right)$
\end_inset

 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $>6$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\emptyset$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $5<k<6$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x=1\right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $4<k<5$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.02
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.11
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3<k<4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2,3\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.03
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.15
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2<k<3$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2,3,4\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.04
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.18
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1<k<2$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2,3,4,5\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.05
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.20
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $79/94<k<1$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2,3,4,5,6\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.06
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.21
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k<79/94$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2,3,4,5,6,7\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

Choosing 
\begin_inset Formula $k\in\left(2,3\right)$
\end_inset

 will give a test of the desired size (
\begin_inset Formula $\alpha=0.04$
\end_inset

), so it follows that a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X\in\left\{ 1,2,3,4\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X\sim\text{Binomial}\left(2,\theta\right)$
\end_inset

.
 We want to test 
\begin_inset Formula $H_{0}:\theta=1/2$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta=3/4$
\end_inset

.
 Find the power and size for different choices of rejection region.
\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

, the uniformly most powerful test of level 
\begin_inset Formula $\alpha$
\end_inset

 of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 will be one that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
\frac{f\left(\mathbf{x}|H_{1}\right)}{f\left(\mathbf{x}|H_{0}\right)}>k\qquad\text{when }\mathbf{x}\in R
\]

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, where 
\begin_inset Formula $R$
\end_inset

 denotes the rejection region, and 
\begin_inset Formula $\alpha=P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
 Noting that we have 
\begin_inset Formula $\mathbf{X}=X$
\end_inset

, the ratio of the pmfs is
\begin_inset Formula 
\[
\frac{f\left(x|\theta_{1}\right)}{f\left(x|\theta_{0}\right)}=\frac{\binom{2}{x}\theta_{1}^{x}\left(1-\theta_{1}\right)^{2-x}}{\binom{2}{x}\theta_{0}^{x}\left(1-\theta_{0}\right)^{2-x}}=\frac{\left(3/4\right)^{x}\left(1/4\right)^{2-x}}{\left(1/2\right)^{x}\left(1/2\right)^{2-x}}=\frac{\left(3/4\right)^{x}}{\left(1/4\right)^{x}}\frac{\left(1/4\right)^{2}}{\left(1/2\right)^{2}}=3^{x}\left(\frac{1}{2}\right)^{2}=\frac{3^{x}}{4},
\]

\end_inset

so a UMP test of level 
\begin_inset Formula $\alpha$
\end_inset

 will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $f\left(x|\theta_{1}\right)/f\left(x|\theta_{0}\right)=3^{x}/4>k$
\end_inset

.
 Noting that
\begin_inset Formula 
\[
\frac{3^{x}}{4}=\begin{cases}
1/4, & x=0\\
3/4, & x=1\\
9/4, & x=2
\end{cases}
\]

\end_inset

and
\begin_inset Formula 
\begin{flalign*}
P\left(\left\{ X=0|H_{0}\right\} \right) & =\binom{2}{0}\left(\frac{1}{2}\right)^{0}\left(\frac{1}{2}\right)^{2}=\frac{1}{4}\\
P\left(\left\{ X=1|H_{0}\right\} \right) & =\binom{2}{1}\left(\frac{1}{2}\right)^{1}\left(\frac{1}{2}\right)^{1}=\frac{1}{2}\\
P\left(\left\{ X=2|H_{0}\right\} \right) & =\binom{2}{2}\left(\frac{1}{2}\right)^{2}\left(\frac{1}{2}\right)^{0}=\frac{1}{4}\\
P\left(\left\{ X=0|H_{1}\right\} \right) & =\binom{2}{0}\left(\frac{3}{4}\right)^{0}\left(\frac{1}{4}\right)^{2}=\frac{1}{16}\\
P\left(\left\{ X=1|H_{1}\right\} \right) & =\binom{2}{1}\left(\frac{3}{4}\right)^{1}\left(\frac{1}{4}\right)^{1}=\frac{3}{8}\\
P\left(\left\{ X=2|H_{1}\right\} \right) & =\binom{2}{2}\left(\frac{3}{4}\right)^{2}\left(\frac{1}{4}\right)^{0}=\frac{9}{16},
\end{flalign*}

\end_inset

the possible choices for 
\begin_inset Formula $k$
\end_inset

 are shown in the table below.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $R$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{size}=P\left(\left\{ X\in R|H_{0}\right\} \right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{power}=P\left(\left\{ X\in R|H_{1}\right\} \right)$
\end_inset

 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $>9/4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\emptyset$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3/4<k<9/4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x=2\right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1/4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9/16
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1/4<k<3/4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 1,2\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3/4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
15/16
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $k<1/4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left\{ x:x\in\left\{ 0,1,2\right\} \right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

Note that it is not possible to construct a UMP size 
\begin_inset Formula $\alpha=0.05$
\end_inset

 test (we will never reject 
\begin_inset Formula $H_{0}$
\end_inset

).
 Other level tests are possible, but the size of such a test may be unacceptably
 large, or the power of such a test unacceptably small.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X$
\end_inset

 be a random sample from 
\begin_inset Formula 
\[
f\left(x|\theta\right)=\theta x^{\theta-1}\qquad0<x<1,\quad\theta>0.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(a)]
\end_layout

\end_inset

Consider 
\begin_inset Formula $H_{0}:\theta\leq1$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta>1$
\end_inset

 and reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X\geq1/2$
\end_inset

.
 Find the power and size of this test.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The power function for this test is given by
\begin_inset Formula 
\[
\beta\left(\theta\right)=P_{\theta}\left(\left\{ X\in R\right\} \right)=P_{\theta}\left(\left\{ X\geq1/2\right\} \right)=\int_{1/2}^{1}f\left(x|\theta\right)\dif x=\int_{1/2}^{1}\theta x^{\theta-1}\dif x=x^{\theta}\Big\vert_{1/2}^{1}=1^{\theta}-\left(\frac{1}{2}\right)^{\theta}=1-\frac{1}{2^{\theta}}.
\]

\end_inset

This function may be evaluated at some particular value of 
\begin_inset Formula $\theta$
\end_inset

, e.g., 
\begin_inset Formula $\theta=2$
\end_inset

, to determine the power.
 The size of this test is given by
\begin_inset Formula 
\[
\alpha=\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=\sup_{\theta\leq1}\left(1-\frac{1}{2^{\theta}}\right).
\]

\end_inset

As 
\begin_inset Formula $\theta$
\end_inset

 increases from 0 to 1, 
\begin_inset Formula $1/2^{\theta}$
\end_inset

 decreases from 1 to 
\begin_inset Formula $1/2$
\end_inset

, so it follows that 
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

 attains its maximum under 
\begin_inset Formula $H_{0}$
\end_inset

 at 
\begin_inset Formula $\theta=1$
\end_inset

, i.e., 
\begin_inset Formula $\alpha=\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=1/2$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[(b)]
\end_layout

\end_inset

Consider 
\begin_inset Formula $H_{0}:\theta=2$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta=1$
\end_inset

.
 Find the UMP size 
\begin_inset Formula $\alpha$
\end_inset

 test.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

, the uniformly most powerful test of level 
\begin_inset Formula $\alpha$
\end_inset

 of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 will be one that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
\frac{f\left(\mathbf{x}|H_{1}\right)}{f\left(\mathbf{x}|H_{0}\right)}>k\qquad\text{when }\mathbf{x}\in R
\]

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, where 
\begin_inset Formula $R$
\end_inset

 denotes the rejection region, and 
\begin_inset Formula $\alpha=P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
 Noting that we have 
\begin_inset Formula $\mathbf{X}=X$
\end_inset

, the ratio of the pdfs is
\begin_inset Formula 
\[
\frac{f\left(x|H_{1}\right)}{f\left(x|H_{0}\right)}=\frac{f\left(x|\theta=1\right)}{f\left(x|\theta=2\right)}=\frac{x^{0}}{2x}=\frac{1}{2x},
\]

\end_inset

so a UMP test of level 
\begin_inset Formula $\alpha$
\end_inset

 will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula 
\[
\frac{f\left(x|H_{1}\right)}{f\left(x|H_{0}\right)}=\frac{1}{2x}>k\implies x<\frac{1}{2k}.
\]

\end_inset

The power function for this test is
\begin_inset Formula 
\[
\beta\left(\theta\right)=P_{\theta}\left(\left\{ X\in R\right\} \right)=P_{\theta}\left(X<\frac{1}{2k}\right)=\int_{0}^{1/\left(2k\right)}\theta x^{\theta-1}\dif x=x^{\theta}\Big\vert_{0}^{1/\left(2k\right)}=\left(\frac{1}{2k}\right)^{\theta}-0^{\theta}=\left(\frac{1}{2k}\right)^{\theta}.
\]

\end_inset

Then, the size of this test is
\begin_inset Formula 
\[
\alpha=\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=\sup_{\theta=2}\left(\frac{1}{2k}\right)^{\theta}=\left(\frac{1}{2k}\right)^{2}\implies\frac{1}{2k}=\sqrt{\alpha}\implies k=\frac{1}{2\sqrt{\alpha}},
\]

\end_inset

so that the rejection region for the UMP size 
\begin_inset Formula $\alpha$
\end_inset

 test is
\begin_inset Formula 
\[
R=\left\{ x:x<\frac{1}{2k}\right\} =\left\{ x:x<\frac{1}{2\left(1/\left(2\sqrt{\alpha}\right)\right)}\right\} =\left\{ x:x<\sqrt{\alpha}\right\} .
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Definition
A family of pdfs or pmfs 
\begin_inset Formula $\left\{ g\left(t|\theta\right):\theta\in\Theta\right\} $
\end_inset

 for a univariate random variable 
\begin_inset Formula $T$
\end_inset

 with real-valued parameter 
\begin_inset Formula $\theta$
\end_inset

 has a 
\shape italic
monotone likelihood ratio
\shape default
 (MLR) if, for every 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

, 
\begin_inset Formula $g\left(t|\theta_{2}\right)/g\left(t|\theta_{1}\right)$
\end_inset

 is a monotone (nonincreasing or nondecreasing) function of 
\begin_inset Formula $t$
\end_inset

 on 
\begin_inset Formula $\left\{ t:g\left(t|\theta_{1}\right)>0\text{ or }g\left(t|\theta_{2}\right)>0\right\} $
\end_inset

.
 Note that 
\begin_inset Formula $c/0$
\end_inset

 is defined as 
\begin_inset Formula $\infty$
\end_inset

 if 
\begin_inset Formula $0<c$
\end_inset

.
\end_layout

\begin_layout Standard
Consider the family of pdfs 
\begin_inset Formula $\mathcal{N}\left(\theta,1^{2}\right)$
\end_inset

, and let 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

.
 Then, the ratio of the pdfs of 
\begin_inset Formula $\mathcal{N}\left(\theta_{2},1^{2}\right)$
\end_inset

 and 
\begin_inset Formula $\mathcal{N}\left(\theta_{1},1^{2}\right)$
\end_inset

 random variables is 
\begin_inset Formula 
\begin{flalign*}
\frac{1/\left(\sqrt{2\pi}\right)\exp\left\{ -\left(x-\theta_{2}\right)^{2}/2\right\} }{1/\left(\sqrt{2\pi}\right)\exp\left\{ -\left(x-\theta_{1}\right)^{2}/2\right\} } & =\exp\left\{ -\frac{\left(x-\theta_{2}\right)^{2}}{2}\right\} \exp\left\{ \frac{\left(x-\theta_{1}\right)^{2}}{2}\right\} \\
 & =\exp\left\{ -\frac{\left(x-\theta_{2}\right)^{2}}{2}+\frac{\left(x-\theta_{1}\right)^{2}}{2}\right\} \\
 & =\exp\left\{ -\frac{1}{2}\left[\left(x-\theta_{2}\right)^{2}-\left(x-\theta_{1}\right)^{2}\right]\right\} \\
 & =\exp\left\{ -\frac{1}{2}\left[x^{2}-2\theta_{2}x+\theta_{2}^{2}-\left(x^{2}-2\theta_{1}x+\theta_{1}^{2}\right)\right]\right\} \\
 & =\exp\left\{ -\frac{1}{2}\left(-2\theta_{2}x+\theta_{2}^{2}+2\theta_{1}x-\theta_{1}^{2}\right)\right\} \\
 & =\exp\left\{ -\frac{1}{2}\left[2x\left(\theta_{1}-\theta_{2}\right)+\theta_{2}^{2}-\theta_{1}^{2}\right]\right\} \\
 & =\exp\left\{ x\left(\theta_{2}-\theta_{1}\right)+\frac{\theta_{1}^{2}-\theta_{2}^{2}}{2}\right\} \\
 & =\mathrm{e}^{x\left(\theta_{2}-\theta_{1}\right)}\mathrm{e}^{\left(\theta_{1}^{2}-\theta_{2}^{2}\right)/2}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $\theta_{1},\theta_{2}\in\mathbb{R}$
\end_inset

, so that 
\begin_inset Formula $\mathrm{e}^{\left(\theta_{1}^{2}-\theta_{2}^{2}\right)/2}>0$
\end_inset

.
 We have 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

, so that 
\begin_inset Formula $\theta_{2}-\theta_{1}>0$
\end_inset

, and it follows that the ratio of the pdfs is nondecreasing in 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
Consider the family of pdfs 
\begin_inset Formula $\text{Poisson}\left(\lambda\right)$
\end_inset

, and let 
\begin_inset Formula $\lambda_{2}>\lambda_{1}$
\end_inset

.
 Then, the ratio of the pdfs of 
\begin_inset Formula $\text{Poisson}\left(\lambda_{2}\right)$
\end_inset

 and 
\begin_inset Formula $\text{Poisson}\left(\lambda_{1}\right)$
\end_inset

 random variables is
\begin_inset Formula 
\[
\frac{\mathrm{e}^{-\lambda_{2}}\lambda_{2}^{x}/x!}{\mathrm{e}^{-\lambda_{1}}\lambda_{1}^{x}/x!}=\frac{\lambda_{2}^{x}}{\lambda_{1}^{x}}\mathrm{e}^{\lambda_{1}-\lambda_{2}}=\left(\frac{\lambda_{2}}{\lambda_{1}}\right)^{x}\mathrm{e}^{\lambda_{1}-\lambda_{2}},
\]

\end_inset

which is nondecreasing in 
\begin_inset Formula $x$
\end_inset

 because 
\begin_inset Formula $\lambda_{2}>\lambda_{1}$
\end_inset

, so that 
\begin_inset Formula $\lambda_{2}/\lambda_{1}>1$
\end_inset

 (note that 
\begin_inset Formula $\lambda_{1},\lambda_{2}\geq0$
\end_inset

).
\end_layout

\begin_layout Definition
A cdf 
\begin_inset Formula $F$
\end_inset

 is 
\shape italic
stochastically greater
\shape default
 than a cdf 
\begin_inset Formula $G$
\end_inset

 if 
\begin_inset Formula $F\left(x\right)\leq G\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

, with strict inequality for some 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
This definition implies that if 
\begin_inset Formula $X\sim F$
\end_inset

, 
\begin_inset Formula $Y\sim G$
\end_inset

, then 
\begin_inset Formula $P\left(\left\{ X>x\right\} \right)\geq P\left(\left\{ Y>x\right\} \right)$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

, with strict inequality for some 
\begin_inset Formula $x$
\end_inset

.
 In other words, 
\begin_inset Formula $F$
\end_inset

 gives more probability to greater values.
 Suppose that 
\begin_inset Formula $F\sim\mathcal{N}\left(1,1^{2}\right)$
\end_inset

 and 
\begin_inset Formula $G\sim\mathcal{N}\left(0,1^{2}\right)$
\end_inset

, whose plots are shown in figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:stochastic-incr}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<stochastic-incr, echo=FALSE, fig.height=3, fig.width=5, fig.align='center',
 fig.pos='h', fig.cap='$F$ is stochastically greater than $G$'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

x <- seq(-5,5,.01)
\end_layout

\begin_layout Plain Layout

plot(x, pnorm(x), type = "l", ylab = "f(x)", cex.lab=0.75, cex.axis=0.75)
\end_layout

\begin_layout Plain Layout

lines(x, pnorm(x, mean = 1))
\end_layout

\begin_layout Plain Layout

text(c(-0.5,1.5), c(0.6,0.4), labels = c("G(x)","F(x)"), cex = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:stochastically-increasing"

\end_inset

Suppose that 
\begin_inset Formula $X\sim F$
\end_inset

, let 
\begin_inset Formula $f$
\end_inset

 be a pdf for 
\begin_inset Formula $X$
\end_inset

 such that 
\begin_inset Formula $f$
\end_inset

 has a nondecreasing MLR, and let 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

.
 Then, 
\begin_inset Formula 
\[
\frac{\dif}{\dif x}\left[F\left(x|\theta_{2}\right)-F\left(x|\theta_{1}\right)\right]=f\left(x|\theta_{2}\right)-f\left(x|\theta_{1}\right)=\frac{f\left(x|\theta_{2}\right)f\left(x|\theta_{1}\right)}{f\left(x|\theta_{1}\right)}-f\left(x|\theta_{1}\right)=f\left(x|\theta_{1}\right)\left(\frac{f\left(x|\theta_{2}\right)}{f\left(x|\theta_{1}\right)}-1\right).
\]

\end_inset

Because 
\begin_inset Formula $f$
\end_inset

 has a nondecreasing MLR, 
\begin_inset Formula $f\left(x|\theta_{2}\right)/f\left(x|\theta_{1}\right)$
\end_inset

 is increasing.
 Because 
\begin_inset Formula $f$
\end_inset

 is a pdf, we have 
\begin_inset Formula $f\geq0$
\end_inset

 for all 
\begin_inset Formula $x$
\end_inset

.
 It follows that, as 
\begin_inset Formula $x$
\end_inset

 increases, 
\begin_inset Formula $f\left(x|\theta_{1}\right)\left[\left(f\left(x|\theta_{2}\right)/f\left(x|\theta_{1}\right)\right)-1\right]$
\end_inset

 can only change sign from negative to positive (when the ratio exceeds
 1), which implies that any interior extremum is a minimum.
 Thus, 
\begin_inset Formula $F\left(x|\theta_{2}\right)-F\left(x|\theta_{1}\right)$
\end_inset

 is maximized when 
\begin_inset Formula $x=\pm\infty$
\end_inset

, in which case it is equal to 0 (see 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:properties-of-cdf"

\end_inset

), i.e., 
\begin_inset Formula 
\[
\sup_{x\in\mathbb{R}}\left[F\left(x|\theta_{2}\right)-F\left(x|\theta_{1}\right)\right]=0\implies F\left(x|\theta_{2}\right)-F\left(x|\theta_{1}\right)\leq0\implies F\left(x|\theta_{2}\right)\leq F\left(x|\theta_{1}\right).
\]

\end_inset

It follows that 
\begin_inset Formula $F\left(x|\theta_{2}\right)$
\end_inset

 is stochastically greater than 
\begin_inset Formula $F\left(x|\theta_{1}\right)$
\end_inset

.
 Further, for some constant 
\begin_inset Formula $c$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
F\left(c|\theta_{2}\right) & \leq F\left(c|\theta_{1}\right)\\
\implies P_{\theta_{2}}\left(\left\{ X\leq c\right\} \right) & \leq P_{\theta_{1}}\left(\left\{ X\leq c\right\} \right)\\
\implies1-P_{\theta_{2}}\left(\left\{ X>c\right\} \right) & \leq1-P_{\theta_{1}}\left(\left\{ X>c\right\} \right)\\
\implies P_{\theta_{1}}\left(\left\{ X>c\right\} \right) & \leq P_{\theta_{2}}\left(\left\{ X>c\right\} \right).
\end{flalign*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Karlin-Rubin]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:karlin-rubin"

\end_inset

Consider testing 
\begin_inset Formula $H_{0}:\theta\leq\theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}:\theta>\theta_{0}$
\end_inset

.
 Suppose that 
\begin_inset Formula $T$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\theta$
\end_inset

 and the family of pdfs or pmfs 
\begin_inset Formula $\left\{ g\left(t|\theta\right):\theta\in\Theta\right\} $
\end_inset

 of 
\begin_inset Formula $T$
\end_inset

 has a nondecreasing MLR.
 Then for any 
\begin_inset Formula $t_{0}$
\end_inset

, the test that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if and only if 
\begin_inset Formula $T>t_{0}$
\end_inset

 is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test, where 
\begin_inset Formula $\alpha=P_{\theta_{0}}\left(\left\{ T>t_{0}\right\} \right)$
\end_inset

.
 (This is Theorem 8.3.17 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $\beta\left(\theta\right)=P_{\theta}\left(\left\{ T>t_{0}\right\} \right)$
\end_inset

 be the power function of the test.
 Fix 
\begin_inset Formula $\theta'>\theta_{0}$
\end_inset

 and consider testing 
\begin_inset Formula $H'_{0}:\theta=\theta_{0}$
\end_inset

 versus 
\begin_inset Formula $H'_{1}:\theta=\theta'$
\end_inset

.
 Since the family of pdfs or pmfs of 
\begin_inset Formula $T$
\end_inset

 has a nondecreasing MLR, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:stochastically-increasing"

\end_inset

 that
\begin_inset Formula $\beta\left(\theta\right)$
\end_inset

 is nondecreasing, so 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[i.]
\end_layout

\end_inset


\begin_inset Formula $\sup_{\theta\leq\theta_{0}}\beta\left(\theta\right)=\beta\left(\theta_{0}\right)=\alpha$
\end_inset

, and this is a level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout

[ii.]
\end_layout

\end_inset

If we define
\begin_inset Formula 
\[
k'=\inf_{t\in T}\frac{g\left(t|\theta'\right)}{g\left(t|\theta_{0}\right)},
\]

\end_inset

where 
\begin_inset Formula $\mathcal{T}=\left\{ t:t>t_{0}\text{ and either }g\left(t|\theta'\right)>0\text{ or }g\left(t|\theta_{0}\right)>0\right\} $
\end_inset

, it follows that
\begin_inset Formula 
\[
T>t_{0}\implies\frac{g\left(t|\theta'\right)}{g\left(t|\theta_{0}\right)}>k'.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Proof
Together with 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:neyman-pearson-corr"

\end_inset

, (i) and (ii) imply that 
\begin_inset Formula $\beta\left(\theta'\right)\geq\beta^{*}\left(\theta'\right)$
\end_inset

, where 
\begin_inset Formula $\beta^{*}\left(\theta\right)$
\end_inset

 is the power function for any other level 
\begin_inset Formula $\alpha$
\end_inset

 test of 
\begin_inset Formula $H'_{0}$
\end_inset

, that is, any test satisfying 
\begin_inset Formula $\beta\left(\theta_{0}\right)\leq\alpha$
\end_inset

.
 However, any level 
\begin_inset Formula $\alpha$
\end_inset

 test of 
\begin_inset Formula $H_{0}$
\end_inset

 satisfies 
\begin_inset Formula $\beta^{*}\left(\theta_{0}\right)\leq\sup_{\theta\in\Theta_{0}}\beta^{*}\left(\theta\right)\leq\alpha$
\end_inset

.
 Thus, 
\begin_inset Formula $\beta\left(\theta'\right)\geq\beta^{*}\left(\theta'\right)$
\end_inset

 for any level 
\begin_inset Formula $\alpha$
\end_inset

 test of 
\begin_inset Formula $H_{0}$
\end_inset

.
 Since 
\begin_inset Formula $\theta'$
\end_inset

 was arbitrary, the test is a UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test.
\end_layout

\begin_layout Proof
By an analogous argument, it can be shown that under the conditions of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:karlin-rubin"

\end_inset

, the test that rejects 
\begin_inset Formula $H_{0}:\theta\geq\theta_{0}$
\end_inset

 in favor of 
\begin_inset Formula $H_{1}:\theta<\theta_{0}$
\end_inset

 if and only if 
\begin_inset Formula $T<t_{0}$
\end_inset

 is a UMP level 
\begin_inset Formula $\alpha=P_{\theta_{0}}\left(\left\{ T<t_{0}\right\} \right)$
\end_inset

 test.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mathcal{U}\left(0,\theta\right)$
\end_inset

.
 Consider the hypothesis test 
\begin_inset Formula 
\[
H_{0}:\theta\leq\theta_{0}\quad\text{vs.}\quad H_{1}:\theta>\theta_{0}.
\]

\end_inset


\end_layout

\begin_layout Example
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-statistic-uniform"

\end_inset

, 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

, and a pdf for 
\begin_inset Formula $X_{\left(n\right)}$
\end_inset

 is given by
\begin_inset Formula 
\[
f_{X_{\left(n\right)}}\left(t\right)=nt^{n-1}\left(\frac{1}{\theta^{n}}\right)I_{\left(0,\theta\right)}\left(t\right).
\]

\end_inset

Let 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

, so that the ratio of the pdfs of 
\begin_inset Formula $f_{X_{\left(n\right)}}\left(t|\theta_{2}\right)$
\end_inset

 and 
\begin_inset Formula $f_{X_{\left(n\right)}}\left(t|\theta_{1}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\frac{f_{X_{\left(n\right)}}\left(t|\theta=\theta_{2}\right)}{f_{X_{\left(n\right)}}\left(t|\theta=\theta_{1}\right)}=\frac{\left(nt^{n-1}/\theta_{2}^{n}\right)I_{\left(0,\theta_{2}\right)}\left(t\right)}{\left(nt^{n-1}/\theta_{1}^{n}\right)I_{\left(0,\theta_{1}\right)}\left(t\right)}=\left(\frac{\theta_{1}}{\theta_{2}}\right)^{n}\frac{I_{\left(0,\theta_{2}\right)}\left(t\right)}{I_{\left(0,\theta_{1}\right)}\left(t\right)}=\begin{cases}
\left(\theta_{1}/\theta_{2}\right)^{n}, & t<\theta_{1}\\
\infty, & \theta_{1}<t<\theta_{2}
\end{cases},
\]

\end_inset

which is nondecreasing in 
\begin_inset Formula $t$
\end_inset

.
 Therefore, this family has MLR, and by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:karlin-rubin"

\end_inset

, the UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X_{\left(n\right)}>k$
\end_inset

.
 We have
\begin_inset Formula 
\begin{flalign*}
\alpha & =P_{\theta_{0}}\left(\left\{ X_{\left(n\right)}>k\right\} \right)\\
 & =\int_{k}^{\infty}f_{X_{\left(n\right)}}\left(t|\theta=\theta_{0}\right)\dif t\\
 & =\int_{k}^{\infty}\frac{nt^{n-1}}{\theta_{0}^{n}}I_{\left(0,\theta_{0}\right)}\left(t\right)\dif t\\
 & =\int_{k}^{\theta_{0}}\frac{nt^{n-1}}{\theta_{0}^{n}}\text{d}t+\int_{\theta_{0}}^{\infty}0\dif t\\
 & =\frac{t^{n}}{\theta_{0}^{n}}\Big\rvert_{k}^{\theta_{0}}+0\\
 & =\frac{\theta_{0}^{n}}{\theta_{0}^{n}}-\frac{k^{n}}{\theta_{0}^{n}}\\
 & =1-\frac{k^{n}}{\theta_{0}^{n}},
\end{flalign*}

\end_inset

so that
\begin_inset Formula 
\[
\alpha=1-\frac{k^{n}}{\theta_{0}^{n}}\implies1-\alpha=\frac{k^{n}}{\theta_{0}^{n}}\implies k^{n}=\theta_{0}^{n}\left(1-\alpha\right)\implies k=\left[\theta_{0}^{n}\left(1-\alpha\right)\right]^{1/n}=\theta_{0}\left(1-\alpha\right)^{1/n}.
\]

\end_inset

Then, the UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X_{\left(n\right)}>\theta_{0}\left(1-\alpha\right)^{1/n}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset Formula $X$
\end_inset

 is a single observation from 
\begin_inset Formula 
\[
f\left(x|\theta\right)=2\theta x+1-\theta,\qquad0\leq x\leq1,\qquad-1\leq\theta\leq1.
\]

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find the most powerful test of 
\begin_inset Formula $H_{0}:\theta=0$
\end_inset

 vs.
 
\begin_inset Formula $H_{1}:\theta=1$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:neyman-pearson"

\end_inset

, the uniformly most powerful test of level 
\begin_inset Formula $\alpha$
\end_inset

 of 
\begin_inset Formula $H_{0}$
\end_inset

 versus 
\begin_inset Formula $H_{1}$
\end_inset

 will be one that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
\frac{f\left(\mathbf{x}|H_{1}\right)}{f\left(\mathbf{x}|H_{0}\right)}>k\qquad\text{when }\mathbf{x}\in R
\]

\end_inset

for some 
\begin_inset Formula $k\geq0$
\end_inset

, where 
\begin_inset Formula $R$
\end_inset

 denotes the rejection region, and 
\begin_inset Formula $\alpha=P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)$
\end_inset

.
 Noting that we have 
\begin_inset Formula $\mathbf{X}=X$
\end_inset

, the ratio of the pdfs is
\begin_inset Formula 
\[
\frac{f\left(x|\theta=1\right)}{f\left(x|\theta=0\right)}=\frac{2x+1-1}{0+1-0}=2x,
\]

\end_inset

so we will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $2x>k\Rightarrow x>k/2$
\end_inset

.
 We have
\begin_inset Formula 
\begin{flalign*}
\alpha & =P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)\\
 & =P_{\theta_{0}}\left(\left\{ X>\frac{k}{2}\right\} \right)\\
 & =\int_{k/2}^{\infty}f\left(x|\theta=\theta_{0}\right)\dif x\\
 & =\int_{k/2}^{1}2\theta_{0}x+1-\theta_{0}\dif x+\int_{1}^{\infty}0\dif x\\
 & =\int_{k/2}^{1}0+1-0\dif x+0\\
 & =\int_{k/2}^{1}1\dif x\\
 & =x\rvert_{k/2}^{1}\\
 & =1-\frac{k}{2},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\[
\alpha=1-\frac{k}{2}\implies\frac{k}{2}=1-\alpha.
\]

\end_inset

Then, the UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X>1-\alpha$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Consider 
\begin_inset Formula $H_{0}:\theta\leq0$
\end_inset

 vs.
 
\begin_inset Formula $H_{1}:\theta>0$
\end_inset

 and reject if 
\begin_inset Formula $X>1/2$
\end_inset

.
 Find the power and size of this test.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The power function for this test is
\begin_inset Formula 
\begin{flalign*}
\beta\left(\theta\right) & =P_{\theta}\left(\left\{ X\in R\right\} \right)\\
 & =P_{\theta}\left(\left\{ X>\frac{1}{2}\right\} \right)\\
 & =\int_{1/2}^{1}2\theta x+1-\theta\dif x\\
 & =\theta x^{2}+x-\theta x\rvert_{1/2}^{1}\\
 & =\theta+1-\theta-\left(\frac{\theta}{4}+\frac{1}{2}-\frac{\theta}{2}\right)\\
 & =1-\left(\frac{1}{2}-\frac{\theta}{4}\right)\\
 & =\frac{1}{2}+\frac{\theta}{4}.
\end{flalign*}

\end_inset

The power of a test is equal to the probability that it rejects the null
 hypothesis given that the alternative hypothesis is true.
 Then, the power of this test for a particular alternative 
\begin_inset Formula $H_{1}:\theta=\theta_{1}$
\end_inset

 is
\begin_inset Formula 
\[
P_{\theta_{1}}\left(\left\{ X\in R\right\} \right)=P\left(\left\{ X>\frac{1}{2}|\theta=\theta_{1}\right\} \right)=\beta\left(\theta_{1}\right)=\frac{1}{2}+\frac{\theta_{1}}{4}.
\]

\end_inset

Noting that the power function for this test is increasing in 
\begin_inset Formula $\theta$
\end_inset

, the size of this test is
\begin_inset Formula 
\[
\alpha=\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)=\sup_{\theta\leq0}\beta\left(\theta\right)=\beta\left(0\right)=\frac{1}{2}+0=\frac{1}{2}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Find the UMP test for 
\begin_inset Formula $H_{0}:\theta\leq0$
\end_inset

 vs.
 
\begin_inset Formula $H_{1}:\theta>0$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Let 
\begin_inset Formula $g\left(T\left(x\right)|\theta\right)=2\theta x+1-\theta$
\end_inset

.
 Then, setting 
\begin_inset Formula $h\left(x\right)=1$
\end_inset

, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:factorization"

\end_inset

 that 
\begin_inset Formula $T\left(X\right)=X$
\end_inset

 is sufficient for 
\begin_inset Formula $\theta$
\end_inset

.
 Let 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

, so that the ratio of the pdfs of 
\begin_inset Formula $f\left(x|\theta_{2}\right)$
\end_inset

 and f
\begin_inset Formula $\left(x|\theta_{1}\right)$
\end_inset

 is
\begin_inset Formula 
\[
\frac{f\left(x|\theta=\theta_{2}\right)}{f\left(x|\theta=\theta_{1}\right)}=\frac{2\theta_{2}x+1-\theta_{2}}{2\theta_{1}x+1-\theta_{1}}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $x$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\dif}{\dif x}\frac{2\theta_{2}x+1-\theta_{2}}{2\theta_{1}x+1-\theta_{1}} & =2\theta_{2}\left(2\theta_{1}x+1-\theta\right)^{-1}+\left(2\theta_{2}x+1-\theta_{2}\right)\left[-\left(2\theta_{1}x+1-\theta_{1}\right)^{-2}\left(2\theta_{1}\right)\right]\\
 & =\frac{2\theta_{2}}{2\theta_{1}x+1-\theta_{1}}-\frac{2\theta_{1}\left(2\theta_{2}x+1-\theta_{2}\right)}{\left(2\theta_{1}x+1-\theta_{1}\right)^{2}}\\
 & =\frac{2\theta_{2}\left(2\theta_{1}x+1-\theta_{1}\right)-2\theta_{1}\left(2\theta_{2}x+1-\theta_{2}\right)}{\left(2\theta_{1}x+1-\theta_{1}\right)^{2}}\\
 & =\frac{2\left(2\theta_{1}\theta_{2}x+\theta_{2}-\theta_{1}\theta_{2}-2\theta_{1}\theta_{2}x-\theta_{1}+\theta_{1}\theta_{2}\right)}{\left(2\theta_{1}x+1-\theta_{1}\right)^{2}}\\
 & =\frac{2\left(\theta_{2}-\theta_{1}\right)}{\left(2\theta_{1}x+1-\theta_{1}\right)^{2}}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $\theta_{2}>\theta_{1}$
\end_inset

, so the numerator of this expression is positive.
 The denominator is also positive, so it follows that this expression is
 non-negative.
 Therefore, this ratio is nondecreasing in 
\begin_inset Formula $x$
\end_inset

 and this family of pdfs has a monotone likelihood ratio.
 It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:karlin-rubin"

\end_inset

 that the UMP test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X>k$
\end_inset

.
 The size of this test is
\begin_inset Formula 
\begin{flalign*}
\alpha & =\sup_{\theta\in\Theta_{0}}\beta\left(\theta\right)\\
 & =\sup_{\theta\leq0}P_{\theta}\left(\left\{ X>k\right\} \right)\\
 & =P\left(\left\{ X>k|\theta=0\right\} \right)\tag{\ensuremath{\beta\left(\theta\right)} is increasing in \ensuremath{\theta}}\\
 & =\int_{k}^{\infty}f\left(x|\theta=0\right)\dif x\\
 & =\int_{k}^{1}2\cdot0\cdot x+1-0\dif x+\int_{1}^{\infty}0\dif x\\
 & =\int_{k}^{1}1\dif x+0\\
 & =x\rvert_{k}^{1}\\
 & =1-k,
\end{flalign*}

\end_inset

which implies 
\begin_inset Formula $k=1-\alpha$
\end_inset

.
 Then, the UMP level 
\begin_inset Formula $\alpha$
\end_inset

 test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X>1-\alpha$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Find the LRT for 
\begin_inset Formula $H_{0}:\theta=0$
\end_inset

 vs.
 
\begin_inset Formula $H_{1}:\theta\neq0$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Noting that 
\begin_inset Formula $\mathbf{X}=X$
\end_inset

, the likelihood function is given by
\begin_inset Formula 
\[
\mathcal{L}\left(\theta|\mathbf{x}\right)=2\theta x+1-\theta=1+\theta\left(2x-1\right),
\]

\end_inset

and the LRT test statistic is 
\begin_inset Formula 
\begin{flalign*}
\lambda\left(\mathbf{x}\right) & =\frac{\sup_{\Theta_{0}}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)}\\
 & =\frac{\sup_{\theta=0}\mathcal{L}\left(\theta|\mathbf{x}\right)}{\sup_{\theta\in\Theta}\mathcal{L}\left(\theta|\mathbf{x}\right)}\\
 & =\frac{1+0\cdot\left(2x-1\right)}{\sup_{\theta\in\Theta}\mathcal{L}\left(\theta|x\right)}\\
 & =\frac{1}{\sup_{\theta\in\Theta}\mathcal{L}\left(\theta|x\right)}.
\end{flalign*}

\end_inset

We see that 
\begin_inset Formula 
\[
0\leq x<\frac{1}{2}\implies2x-1<0\quad\text{and}\quad\frac{1}{2}<x\leq1\implies2x-1>0.
\]

\end_inset

Then, for 
\begin_inset Formula $x\in\left[0,\frac{1}{2}\right)$
\end_inset

, 
\begin_inset Formula $\mathcal{L}\left(\theta|x\right)$
\end_inset

 is decreasing in 
\begin_inset Formula $\theta$
\end_inset

 and attains its maximum value at 
\begin_inset Formula $\hat{\theta}=-1$
\end_inset

.
 For 
\begin_inset Formula $x\in\left(\frac{1}{2},1\right]$
\end_inset

, 
\begin_inset Formula $\mathcal{L}\left(\theta|x\right)$
\end_inset

 is increasing in 
\begin_inset Formula $\theta$
\end_inset

 and attains its maximum value at 
\begin_inset Formula $\hat{\theta}=1$
\end_inset

.
 Thus,
\begin_inset Formula 
\[
\sup_{\theta\in\Theta}\mathcal{L}\left(\theta|x\right)=\begin{cases}
1+\left(-1\right)\left(2x-1\right)=2-2x, & x<\frac{1}{2}\\
1+1\left(2x-1\right)=2x, & x>\frac{1}{2}
\end{cases},
\]

\end_inset

and the LRT test statistic is
\begin_inset Formula 
\[
\lambda\left(x\right)=\begin{cases}
\frac{1}{2-2x}, & x<\frac{1}{2}\\
\frac{1}{2x}, & x>\frac{1}{2}
\end{cases}.
\]

\end_inset

Then, the likelihood ratio test rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
c\geq\lambda\left(x\right)\implies\text{reject }H_{0}\text{ if }\begin{cases}
\frac{1}{2-2x}\leq c, & x<\frac{1}{2}\\
\frac{1}{2x}\leq c, & x>\frac{1}{2}
\end{cases}
\]

\end_inset

for some 
\begin_inset Formula $c\in\left(0,1\right)$
\end_inset

.
 I.e., we will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula 
\[
\frac{1}{2-2x}\leq c\implies1\leq c\left(2-2x\right)\implies\frac{1}{c}\leq2-2x\implies\frac{1}{c}-2\leq-2x\implies x\leq1-\frac{1}{2c}
\]

\end_inset

for 
\begin_inset Formula $x<\frac{1}{2}$
\end_inset

 and
\begin_inset Formula 
\[
\frac{1}{2x}\leq c\implies1\leq c\left(2x\right)\implies x\geq\frac{1}{2c}
\]

\end_inset

for 
\begin_inset Formula $x>\frac{1}{2}$
\end_inset

.
 Then, size 
\begin_inset Formula $\alpha$
\end_inset

 test is given by
\begin_inset Formula 
\begin{flalign*}
\alpha & =P_{\theta_{0}}\left(\left\{ X\in R\right\} \right)\\
 & =P_{\theta=0}\left(\left(\left\{ X\leq1-\frac{1}{2c}\right\} \cap\left\{ X<\frac{1}{2}\right\} \right)\cup\left(\left\{ X\geq\frac{1}{2c}\right\} \cap\left\{ X>\frac{1}{2}\right\} \right)\right)\\
 & =P_{\theta=0}\left(\left\{ 0\leq X\leq1-\frac{1}{2c}\right\} \cup\left\{ \frac{1}{2c}\leq X\leq1\right\} \right)\\
 & =P_{\theta=0}\left(\left\{ 0\leq X\leq1-\frac{1}{2c}\right\} \right)+P_{\theta=0}\left(\left\{ \frac{1}{2c}\leq X\leq1\right\} \right)\\
 & =\int_{0}^{1-1/\left(2c\right)}f\left(x|\theta=0\right)\dif x+\int_{1/\left(2c\right)}^{1}f\left(x|\theta=0\right)\dif x\\
 & =\int_{0}^{1-1/\left(2c\right)}2\cdot0\cdot x+1-0\dif x+\int_{1/\left(2c\right)}^{1}2\cdot0\cdot x+1-0\dif x\\
 & =\int_{0}^{1-1/\left(2c\right)}1\dif x+\int_{1/\left(2c\right)}^{1}1\dif x\\
 & =x\Big\rvert_{0}^{1-1/\left(2c\right)}+x\Big\rvert_{1/\left(2c\right)}^{1}\\
 & =1-\frac{1}{2c}-0+1-\frac{1}{2c}\\
 & =2-\frac{1}{c},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\[
\alpha=2-\frac{1}{c}\implies\frac{1}{c}=2-\alpha\implies c\left(2-\alpha\right)=1\implies c=\frac{1}{2-\alpha}.
\]

\end_inset

Then, we will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if
\begin_inset Formula 
\[
x\leq1-\frac{1}{2c}=1-\frac{1}{2\left(\frac{1}{2-\alpha}\right)}=1-\frac{2-\alpha}{2}=1-\left(\frac{2}{2}-\frac{\alpha}{2}\right)=1-1+\frac{\alpha}{2}=\frac{\alpha}{2}
\]

\end_inset

for 
\begin_inset Formula $x<\frac{1}{2}$
\end_inset

 and 
\begin_inset Formula 
\[
x\geq\frac{1}{2c}=\frac{1}{2\left(\frac{1}{2-\alpha}\right)}=\frac{2-\alpha}{2}=1-\frac{\alpha}{2}
\]

\end_inset

for 
\begin_inset Formula $x>\frac{1}{2}$
\end_inset

.
 Then, the size 
\begin_inset Formula $\alpha$
\end_inset

 LRT rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $X\geq1-\frac{\alpha}{2}$
\end_inset

 or 
\begin_inset Formula $X\leq\frac{\alpha}{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
p-values
\end_layout

\begin_layout Standard
One method of reporting the results of a hypothesis test is to report the
 size, 
\begin_inset Formula $\alpha$
\end_inset

, of the test used and the decision to reject 
\begin_inset Formula $H_{0}$
\end_inset

 or accept 
\begin_inset Formula $H_{0}$
\end_inset

.
 If 
\begin_inset Formula $\alpha$
\end_inset

 is small, the decision to reject 
\begin_inset Formula $H_{0}$
\end_inset

 is fairly convincing, but if 
\begin_inset Formula $\alpha$
\end_inset

 is large, the decision to reject 
\begin_inset Formula $H_{0}$
\end_inset

 is not very convincing because the test has a large probability of incorrectly
 making that decision.
 Another way of reporting the results of a hypothesis test is to report
 the value of a certain kind of test statistic called a 
\shape italic
p-value
\shape default
.
\end_layout

\begin_layout Definition
A 
\shape italic
p-value
\shape default
 
\begin_inset Formula $p\left(\mathbf{X}\right)$
\end_inset

 is a test statistic satisfying 
\begin_inset Formula $0\leq p\left(\mathbf{x}\right)\leq1$
\end_inset

 for every sample point 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Small values of 
\begin_inset Formula $p\left(\mathbf{X}\right)$
\end_inset

 give evidence that 
\begin_inset Formula $H_{1}$
\end_inset

 is true.
 A p-value is 
\shape italic
valid
\shape default
 if, for every 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

 and every 
\begin_inset Formula $0\leq\alpha\leq1$
\end_inset

,
\begin_inset Formula 
\[
P_{\theta}\left(\left\{ p\left(\mathbf{X}\right)\leq\alpha\right\} \right)\leq\alpha.
\]

\end_inset


\end_layout

\begin_layout Standard
A p-value is the probability of observing a more 
\begin_inset Quotes eld
\end_inset

extreme
\begin_inset Quotes erd
\end_inset

 result than was observed under 
\begin_inset Formula $H_{0}$
\end_inset

.
 A p-value can also be viewed as the minimum 
\begin_inset Formula $\alpha\text{-level}$
\end_inset

 at which 
\begin_inset Formula $H_{0}$
\end_inset

 could have been rejected with the observed data.
 The most common way to define a valid p-value is given in the following
 theorem.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $W\left(\mathbf{X}\right)$
\end_inset

 be a test statistic such that large values of 
\begin_inset Formula $W$
\end_inset

 give evidence that 
\begin_inset Formula $H_{1}$
\end_inset

 is true.
 For each sample point 
\begin_inset Formula $\mathbf{x}$
\end_inset

, define
\begin_inset Formula 
\[
p\left(\mathbf{x}\right)=\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ W\left(\mathbf{X}\right)\geq W\left(\mathbf{x}\right)\right\} \right).
\]

\end_inset

Then, 
\begin_inset Formula $p\left(\mathbf{X}\right)$
\end_inset

 is a valid p-value.
 (This is Theorem 8.3.27 from Casella & Berger; the following proof is given
 there.)
\end_layout

\begin_layout Proof
Fix 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

.
 Let 
\begin_inset Formula $F_{\theta}\left(w\right)$
\end_inset

 denote the cdf of 
\begin_inset Formula $-W\left(\mathbf{X}\right)$
\end_inset

.
 Define
\begin_inset Formula 
\[
p_{\theta}\left(\mathbf{x}\right)=P_{\theta}\left(\left\{ W\left(\mathbf{X}\right)\geq W\left(\mathbf{x}\right)\right\} \right)=P_{\theta}\left(\left\{ -W\left(\mathbf{X}\right)\leq-W\left(\mathbf{x}\right)\right\} \right)=F_{\theta}\left(-W\left(\mathbf{x}\right)\right).
\]

\end_inset

Then the random variable 
\begin_inset Formula $p_{\theta}\left(\mathbf{X}\right)$
\end_inset

 is equal to 
\begin_inset Formula $F_{\theta}\left(-W\left(\mathbf{X}\right)\right)$
\end_inset

.
 Hence, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:prob-integral-transform"

\end_inset

, the distribution of 
\begin_inset Formula $p_{\theta}\left(\mathbf{X}\right)$
\end_inset

 is stochastically greater than or equal to a 
\begin_inset Formula $U\left(0,1\right)$
\end_inset

 distribution.
 That is, for every 
\begin_inset Formula $0\leq\alpha\leq1$
\end_inset

, 
\begin_inset Formula $P_{\theta}\left(\left\{ p_{\theta}\left(\mathbf{X}\right)\leq\alpha\right\} \right)\leq\alpha$
\end_inset

.
 Because 
\begin_inset Formula $p\left(\mathbf{x}\right)=\sup_{\theta'\in\Theta_{0}}p_{\theta'}\left(\mathbf{x}\right)\geq p_{\theta}\left(\mathbf{x}\right)$
\end_inset

 for every 
\begin_inset Formula $\mathbf{x}$
\end_inset

,
\begin_inset Formula 
\[
P_{\theta}\left(\left\{ p\left(\mathbf{X}\right)\leq\alpha\right\} \right)\leq P_{\theta}\left(\left\{ p_{\theta}\left(\mathbf{X}\right)\leq\alpha\right\} \right)\leq\alpha.
\]

\end_inset

This is true for every 
\begin_inset Formula $\theta\in\Theta_{0}$
\end_inset

 and for every 
\begin_inset Formula $0\leq\alpha\leq1$
\end_inset

; 
\begin_inset Formula $p\left(\mathbf{X}\right)$
\end_inset

 is a valid p-value.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $n=25$
\end_inset

 and 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim\mathcal{N}\left(\theta,5^{2}\right)$
\end_inset

.
 Consider the hypothesis test 
\begin_inset Formula 
\[
H_{0}:\theta=17\quad\text{vs.}\quad H_{1}:\theta>17.
\]

\end_inset

Find the p-value of the test that rejects 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\bar{X}>17+5/\sqrt{n}$
\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
when we observe 
\begin_inset Formula $\bar{x}=18$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:rand-sample-from-normal-dist"

\end_inset

, the test statistic 
\begin_inset Formula $\bar{X}$
\end_inset

 has the distribution of a 
\begin_inset Formula 
\[
\mathcal{N}\left(\theta,\left(\theta/\sqrt{n}\right)^{2}\right)=\mathcal{N}\left(\theta,\left(5/\sqrt{25}\right)^{2}\right)=\mathcal{N}\left(\theta,1^{2}\right)
\]

\end_inset

random variable, so it follows that 
\begin_inset Formula 
\[
Z=\frac{\bar{X}-\theta}{5/\sqrt{n}}=\frac{\bar{X}-\theta}{5/\sqrt{25}}=\frac{\bar{X}-\theta}{1}=\bar{X}-\theta
\]

\end_inset

has the distribution of a 
\begin_inset Formula $\mathcal{N}\left(0,1^{2}\right)$
\end_inset

 random variable, and that 
\begin_inset Formula $\bar{X}=Z+\theta$
\end_inset

.
 The size of this test is
\begin_inset Formula 
\begin{flalign*}
\alpha & =P_{\theta_{0}}\left(\left\{ \mathbf{X}\in R\right\} \right)\\
 & =P_{\theta_{0}}\left(\left\{ \bar{X}>17+\frac{5}{\sqrt{n}}\right\} \right)\\
 & =P\left(\left\{ \bar{X}>17+\frac{5}{\sqrt{n}}|\theta=17\right\} \right)\\
 & =P\left(\left\{ \frac{\bar{X}-\theta}{\sigma/\sqrt{n}}>\frac{17+5/\sqrt{n}-\theta}{\sigma/\sqrt{n}}|\theta=17\right\} \right)\\
 & =P\left(\left\{ Z>\frac{17+5/\sqrt{25}-17}{5/\sqrt{25}}\right\} \right)\\
 & =P\left(\left\{ Z>1\right\} \right)\\
 & \approx\Sexpr{1-pnorm(1)}.
\end{flalign*}

\end_inset

For a test of size 
\begin_inset Formula $\alpha=0.05$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
0.05 & =P_{\theta_{0}}\left(\left\{ \bar{X}>c\right\} \right)\\
 & =P\left(\left\{ \bar{X}>c|\theta=17\right\} \right)\\
 & =P\left(\left\{ \frac{\bar{X}-\theta}{\sigma/\sqrt{n}}>\frac{c-17}{5/\sqrt{25}}\right\} \right)\\
 & =P\left(\left\{ Z>c-17\right\} \right)\\
 & =1-P\left(\left\{ Z\leq c-17\right\} \right)\\
 & =1-\Phi\left(c-17\right),
\end{flalign*}

\end_inset

which implies
\begin_inset Formula 
\[
\Phi\left(c-17\right)=1-0.05=0.95\implies c-17\approx\Sexpr{qnorm(0.95)}\implies c\approx\Sexpr{17+qnorm(0.95)}.
\]

\end_inset

Then, a size 
\begin_inset Formula $\alpha=0.05$
\end_inset

 test will reject 
\begin_inset Formula $H_{0}$
\end_inset

 if 
\begin_inset Formula $\bar{X}>\Sexpr{17+qnorm(0.95)}$
\end_inset

.
 For our observed 
\begin_inset Formula $\bar{x}=18$
\end_inset

, the p-value is 
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{x}\right) & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \bar{X}\geq\bar{x}\right\} \right)\\
 & =\sup_{\theta=17}P_{\theta}\left\{ \bar{X}\geq18\right\} \\
 & =P\left(\left\{ \bar{X}\geq18|\theta=17\right\} \right)\\
 & =P\left(\left\{ \bar{X}-\theta\geq18-17\right\} \right)\\
 & =P\left(\left\{ Z\geq1\right\} \right)\\
 & =1-\Phi\left(1\right)\\
 & \approx\Sexpr{1-pnorm(1)}.
\end{flalign*}

\end_inset

At 
\begin_inset Formula $\alpha=0.05$
\end_inset

, we do not reject 
\begin_inset Formula $H_{0}$
\end_inset

 because this p-value is greater than 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
when we observe 
\begin_inset Formula $\bar{x}=19$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The p-value is 
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{x}\right) & =\sup_{\theta\in\Theta_{0}}P_{\theta}\left(\left\{ \bar{X}\geq\bar{x}\right\} \right)\\
 & =\sup_{\theta=17}P_{\theta}\left\{ \bar{X}\geq19\right\} \\
 & =P\left(\left\{ \bar{X}\geq19|\theta=17\right\} \right)\\
 & =P\left(\left\{ \bar{X}-\theta\geq19-17\right\} \right)\\
 & =P\left(\left\{ Z\geq2\right\} \right)\\
 & =1-\Phi\left(2\right)\\
 & \approx\Sexpr{1-pnorm(2)}.
\end{flalign*}

\end_inset

At 
\begin_inset Formula $\alpha=0.05$
\end_inset

, we reject 
\begin_inset Formula $H_{0}$
\end_inset

 because this p-value is greater than 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Part
Bayesian statistics
\end_layout

\begin_layout Chapter
Introduction to Bayesian paradigm
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Paternity dispute]
\end_layout

\end_inset

Suppose you are on a jury considering a paternity suit brough by Suzy Smith's
 mother against Al Edged.
 Suzy's mother has blood type O and Al Edged is type AB.
 You have other information as well.
 You hear testimony concerning whether Al Edged and Suzy's mother had sexual
 intercourse during the time that conception could have occurred, about
 the timing and frequency of such intercourse, about Al Edged's fertility,
 about the possibility that someone else is the father, and so on.
 You put all this information together in assessing 
\begin_inset Formula $P\left(F\right)$
\end_inset

, your probability that Al is Suzy's father.
 The evidence of interest is Suzy's blood type, which turns out to be B
 (if it were O, Al Edged would be excluded from paternity).
 According to Mendelian genetics, 
\begin_inset Formula $P\left(B|F\right)=1/2$
\end_inset

.
 You also accept the blood bank's estimate 
\begin_inset Formula $P\left(B|F^{c}\right)=0.09$
\end_inset

.
 According to Bayes' rule, 
\begin_inset Formula 
\[
P\left(F|B\right)=\frac{P\left(B|F\right)P\left(F\right)}{P\left(B|F\right)P\left(F\right)+P\left(B|F^{c}\right)P\left(F^{c}\right)}=\frac{\left(1/2\right)P\left(F\right)}{\left(1/2\right)P\left(F\right)+0.09P\left(F^{c}\right)}.
\]

\end_inset

The relationship between our prior probability, 
\begin_inset Formula $P\left(F\right)$
\end_inset

, and our posterior probability, 
\begin_inset Formula $P\left(F|B\right)$
\end_inset

, may be summarized:
\end_layout

\begin_layout Example
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="8">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $P\left(F\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.750
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.900
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $P\left(F|B\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.382
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.649
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.847
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.943
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.980
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $p\left(y_{1},\ldots,y_{n}\right)$
\end_inset

 be the joint distribution of 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 and let 
\begin_inset Formula $\pi_{1},\ldots,\pi_{n}$
\end_inset

 be a permutation of the indices 
\begin_inset Formula $1,\ldots,n$
\end_inset

.
 If 
\begin_inset Formula $p\left(y_{1},\ldots,y_{n}\right)=p\left(y_{\pi_{1}},\ldots,y_{\pi_{n}}\right)$
\end_inset

 for all permutations, then 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 are 
\shape italic
exchangeable
\shape default
.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[de Finetti's]
\end_layout

\end_inset

Let 
\begin_inset Formula $Y_{1},Y_{2},\ldots$
\end_inset

 be a sequence of random variables.
 If for any 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 are exchangeable, then there exists a prior distribution 
\begin_inset Formula $p\left(\theta\right)$
\end_inset

 and sampling model 
\begin_inset Formula $p\left(y|\theta\right)$
\end_inset

 such that
\begin_inset Formula 
\[
p\left(y_{1},\ldots,y_{n}\right)=\int_{\Theta}\left\{ \prod_{i=1}^{n}p\left(y_{i}|\theta\right)\right\} p\left(\theta\right)\dif\theta.
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Estimating the probability of a rare event]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:infectious-disease"

\end_inset

Suppose we are interested in the prevalance of an infectious disease in
 a small city.
 The higher the prevalance, the more public health precautions would need
 to be put into place.
 A small random sample of 20 individuals from the city is checked for infection.
 Interest is in 
\begin_inset Formula $\theta$
\end_inset

, the fraction of infected individuals in the city.
 The data 
\begin_inset Formula $y$
\end_inset

 records the total number of people in the sample who are infected.
 The parameter space and sample space are then
\begin_inset Formula 
\[
\Theta=\left[0,1\right],\qquad\mathcal{Y}=\left\{ 0,1,\ldots,20\right\} .
\]

\end_inset

Before the sample is obtained, the number of infected individuals in the
 sample is unknown.
 If the value of 
\begin_inset Formula $\theta$
\end_inset

 were known, a reasonable sampling model for 
\begin_inset Formula $Y$
\end_inset

 would be a 
\begin_inset Formula $\text{Bin}\left(20,\theta\right)$
\end_inset

 probability distribution, i.e., 
\begin_inset Formula $Y|\theta\sim\text{Bin}\left(20,\theta\right)$
\end_inset

.
 If, for example, the true infection rate is 
\begin_inset Formula $\theta=0.05$
\end_inset

, then the probability that there will be zero infected individuals in the
 sample is 
\begin_inset Formula 
\[
P\left(Y=0\right)=\binom{20}{0}\left(0.05\right)^{0}\left(1-0.05\right)^{20}=0.36.
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos = 'h'>>=
\end_layout

\begin_layout Plain Layout

n <- 20
\end_layout

\begin_layout Plain Layout

x <- 0:n
\end_layout

\begin_layout Plain Layout

del = 0.25
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

# finish up the code
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
Other studies from various parts of the country indicate that the infection
 rate in comparable cities ranges from about 0.05 to 0.20, with an average
 prevalance of 0.10.
 We will use a prior distribution 
\begin_inset Formula $p\left(\theta\right)$
\end_inset

 that has these characteristics and provides computational convenience.
 Specifically, we will take a Beta distribution
\begin_inset Formula 
\[
\theta\sim\text{Beta}\left(a,b\right),\quad p\left(\theta\right)=\frac{\Gamma\left(a+b\right)}{\Gamma\left(a\right)\Gamma\left(b\right)}\theta^{a-1}\left(1-\theta\right)^{b-1},\quad0\leq\theta\leq1.
\]

\end_inset

The expectation 
\begin_inset Formula $\E\left[\theta\right]=a/\left(a+b\right)$
\end_inset

 and the mode of 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $\left(a-1\right)/\left(a+b-2\right)$
\end_inset

.
 We will represent our information about 
\begin_inset Formula $\theta$
\end_inset

 with 
\begin_inset Formula $\text{Beta}\left(2,20\right)$
\end_inset

 probability distribution.
 Then,
\begin_inset Formula 
\[
\E\left[\theta\right]=\frac{2}{2+20}=\frac{1}{11}\quad\text{and}\quad\text{mode}\left(\theta\right)=\frac{2-1}{2+20-2}=\frac{1}{20}.
\]

\end_inset

Suppose for our study, a value 
\begin_inset Formula $Y=y$
\end_inset

 is observed.
 The posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 is given by 
\begin_inset Formula 
\[
p\left(\theta|y\right)=\frac{p\left(y|\theta\right)p\left(\theta\right)}{\int_{\Theta}p\left(y|\tilde{\theta}\right)p\left(\tilde{\theta}\right)\dif\tilde{\theta}}.
\]

\end_inset

In deriving posterior densities, an oft-used technique is to try and recognize
 the kernel of the posterior density of 
\begin_inset Formula $\theta$
\end_inset

.
 We have
\begin_inset Formula 
\begin{flalign*}
p\left(\theta|y\right) & =\frac{\left[\binom{n}{y}\theta^{y}\left(1-\theta\right)^{n-y}\right]\left[\frac{\Gamma\left(a+b\right)}{\Gamma\left(a\right)\Gamma\left(b\right)}\theta^{a-1}\left(1-\theta\right)^{b-1}\right]}{\int_{\Theta}\left[\binom{n}{y}\theta^{y}\left(1-\theta\right)^{n-y}\right]\left[\frac{\Gamma\left(a+b\right)}{\Gamma\left(a\right)\Gamma\left(b\right)}\theta^{a-1}\left(1-\theta\right)^{b-1}\right]\dif\theta}\\
 & \propto\theta^{y+a-1}\left(1-\theta\right)^{n-y+b-1},
\end{flalign*}

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\text{Beta}\left(y+a,n-y+b\right)$
\end_inset

 random variable.
 Thus, 
\begin_inset Formula 
\[
\theta|y\sim\text{Beta}\left(y+a,n-y+b\right)\quad\text{and}\quad p\left(\theta|y\right)=\frac{\Gamma\left(n+a-b\right)}{\Gamma\left(y+a\right)\Gamma\left(n-y+b\right)}\theta^{y+a-1}\left(1-\theta\right)^{n-y+b-1}.
\]

\end_inset

Suppose that for our study a value of 
\begin_inset Formula $Y=0$
\end_inset

 is observed, i.e., none of the sample individuals are infected.
 The posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 is then 
\begin_inset Formula $\theta|Y=0\sim\text{Beta}\left(2,40\right)$
\end_inset

.
 This density is further to the left than the prior distribution, and more
 peaked as well.
 The posterior distribution 
\begin_inset Formula $p\left(\theta|Y=0\right)$
\end_inset

 provides us with a model for learning about the city-wide infection rate
 
\begin_inset Formula $\theta$
\end_inset

.
 Suppose we are supposed to discuss the results of the survey with a group
 of city health officials.
 We might want to present the posterior results corresponding to a variety
 of prior distributions.
 The posterior expectation is a weighted average of the sample mean 
\begin_inset Formula $\bar{y}$
\end_inset

 and the prior expectation 
\begin_inset Formula $\theta_{0}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\E\left[\theta|Y=y\right]=\frac{a+y}{a+b+n}=\frac{n}{a+b+n}\frac{y}{n}+\frac{a+b}{a+b+n}\frac{a}{a+b}=\frac{n}{w+n}\bar{y}+\frac{w}{w+n}\theta_{0},
\]

\end_inset

where 
\begin_inset Formula $\theta_{0}=a/\left(a+b\right)$
\end_inset

 and 
\begin_inset Formula $w=a+b$
\end_inset

.
 
\begin_inset Formula $\theta_{0}$
\end_inset

 represents our prior guess at the true value of 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

 represents our confidence in this guess, expressed on the same scale as
 the sample size.
 We can compute such a posterior distribution for a wide range of 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

 values to perform a 
\shape italic
sensitivity analysis
\shape default
 (an exploration of how posterior information is affected by differences
 in prior opinion).
\end_layout

\begin_layout Section
Bayesian learning
\end_layout

\begin_layout Standard
The Bayesian perspective does not consider data that could have been observed,
 but is not.
 In the Bayesian approach, instead of supposing that 
\begin_inset Formula $\theta$
\end_inset

 is a fixed parameter, it is regarded as the realized value of a random
 variable 
\begin_inset Formula $\Theta$
\end_inset

, with density 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

 (the 
\shape italic
prior
\shape default
 distribution).
 The prior distribution summarizes any information we have about 
\begin_inset Formula $\theta$
\end_inset

 (not related to that provided by the data 
\begin_inset Formula $y$
\end_inset

).
 Bayesian inference refers to the updating of prior beliefs into 
\shape italic
posterior
\shape default
 beliefs conditional on observed data using Bayes' theorem.
\end_layout

\begin_layout Chapter
Single-parameter models
\end_layout

\begin_layout Standard
Various numerical summaries of the posterior distribution 
\begin_inset Formula $p\left(\theta|\mathbf{y}\right)$
\end_inset

 may be used, each of which corresponds to minimizing a different loss function.
 The posterior mean is the posterior expectation of the parameter, the mode
 is the 
\begin_inset Quotes eld
\end_inset

most likely
\begin_inset Quotes erd
\end_inset

 value, and the standard deviation or interquartile range summarize the
 variation.
 In addition to point summaries, it is always important to report posterior
 uncertainty.
\end_layout

\begin_layout Definition
A Bayesian 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 
\shape italic
credible interval
\shape default
 for the parameter 
\begin_inset Formula $\theta$
\end_inset

 is constructed by removing the upper and lower 
\begin_inset Formula $100\left(\alpha/2\right)$
\end_inset

 percentiles of the posterior distribution.
\end_layout

\begin_layout Standard
We can obtain a credible interval by finding 
\begin_inset Formula $\left(\theta_{l},\theta_{u}\right)$
\end_inset

 that satisfy
\begin_inset Formula 
\[
P\left(\theta<\theta_{l}|\mathbf{y}\right)=\alpha/2\quad\text{and}\quad P\left(\theta>\theta_{u}|\mathbf{y}\right)=\alpha/2.
\]

\end_inset

Then, 
\begin_inset Formula $\left(\theta_{l},\theta_{u}\right)$
\end_inset

 is a 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 credible interval.
 The credible interval has the specified probability coverage and has a
 natural interpretation, e.g., 
\begin_inset Quotes eld
\end_inset

we have 95% posterior confidence that 
\begin_inset Formula $\theta$
\end_inset

 falls in the interval.
\begin_inset Quotes erd
\end_inset

 We can obtain a 95% credible interval for the infection rate from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:infectious-disease"

\end_inset

 by identifying the values that cut off the upper and lower 2.5% of the posterior
 distribution.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

qbeta(c(0.025,0.975), 2, 40)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Thus, a 95% credible interval for 
\begin_inset Formula $\theta$
\end_inset

 is given by 
\begin_inset Formula $\left(0.006,0.129\right)$
\end_inset

.
 That is, we are 95% confident 
\shape italic
a posteriori
\shape default
 that 
\begin_inset Formula $\theta$
\end_inset

 lies in the interval 
\begin_inset Formula $\left(0.006,0.129\right)$
\end_inset

.
\end_layout

\begin_layout Definition
The 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 
\shape italic
highest posterior density (HPD) interval
\shape default
 is defined as the interval that contains 
\begin_inset Formula $100\left(1-\alpha\right)\%$
\end_inset

 of the highest area under the posterior density function.
\end_layout

\begin_layout Standard
For symmetric, unimodal, and concave distributions, HPD intervals and credible
 intervals are equivalent.
 For skewed or multimodal distributions, constructing HPD intervals is not
 straightforward.
 The basic idea is to move a horizontal line down across the density, stopping
 when the posterior probability of the 
\begin_inset Formula $\theta$
\end_inset

 values in the region reaches 
\begin_inset Formula $1-\alpha$
\end_inset

.
\end_layout

\begin_layout Definition
The 
\shape italic
posterior predictive distribution
\shape default
 for a future outcome 
\begin_inset Formula $\tilde{y}$
\end_inset

 given the data 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is
\begin_inset Formula 
\[
p\left(\tilde{y}|\mathbf{y}\right)=\int_{\theta}p\left(\tilde{y},\theta|\mathbf{y}\right)\dif\theta=\int_{\theta}p\left(\tilde{y}|\theta,\mathbf{y}\right)p\left(\theta|\mathbf{y}\right)\dif\theta=\int_{\theta}p\left(\tilde{y}|\theta\right)p\left(\theta|\mathbf{y}\right)\dif\theta.
\]

\end_inset


\end_layout

\begin_layout Standard
The posterior predictive distribution is an expected value over the posterior
 distribution, i.e., 
\begin_inset Formula $p\left(\tilde{y}|\mathbf{y}\right)=\E_{\theta|y}\left[p\left(\tilde{y}|\theta\right)\right]$
\end_inset

.
 Observing 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 gives information about 
\begin_inset Formula $\theta$
\end_inset

, which in turn gives information about 
\begin_inset Formula $\tilde{y}$
\end_inset

 (which is independent of the observed values 
\begin_inset Formula $\mathbf{y}$
\end_inset

).
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Predictive distribution for binary data]
\end_layout

\end_inset

Let 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 be the outcomes from a sample of 
\begin_inset Formula $n$
\end_inset

 binary random variables, and let 
\begin_inset Formula $\tilde{y}\in\left\{ 0,1\right\} $
\end_inset

 be an additional outcome from the same population that has yet to be observed.
 The predictive distribution for conditionally iid binary variables can
 be derived as
\begin_inset Formula 
\[
p\left(\tilde{y}=1|\mathbf{y}\right)=\int_{\theta}p\left(\tilde{y}=1|\theta\right)p\left(\theta|\mathbf{y}\right)\dif\theta=\int_{\theta}\theta\cdot p\left(\theta|\mathbf{y}\right)\dif\theta,
\]

\end_inset

where the final equality follows from the fact that 
\begin_inset Formula $\tilde{Y}\sim\text{Bernoulli}\left(\theta\right)$
\end_inset

, so that 
\begin_inset Formula $P\left(\left\{ \tilde{Y}=1\right\} \right)=\theta$
\end_inset

.
 Suppose that the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 is as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:infectious-disease"

\end_inset

, i.e., 
\begin_inset Formula 
\[
\theta|\mathbf{y}\sim\text{Beta}\left(a+\sum_{i=1}^{n}y_{i},b+n-\sum_{i=1}^{n}y_{i}\right).
\]

\end_inset

Then, 
\begin_inset Formula 
\[
P\left(\tilde{Y}=1|\mathbf{y}\right)=\E\left[\theta|\mathbf{y}\right]=\frac{a+\sum_{i=1}^{n}y_{i}}{a+b+n}\quad\text{and}\quad P\left(\tilde{Y}=0|\mathbf{y}\right)=1-\E\left[\theta|\mathbf{y}\right]=\frac{b+n-\sum_{i=1}^{n}y_{i}}{a+b+n}.
\]

\end_inset


\end_layout

\begin_layout Section
Conjugate prior distributions
\end_layout

\begin_layout Standard
Sampling models from exponential families all have conjugate priors.
 Recall that a one-parameter 
\shape italic
exponential family model
\shape default
 is any model with density that can be expressed as
\begin_inset Formula 
\[
p\left(y|\theta\right)=h\left(y\right)g\left(\theta\right)\exp\left\{ \phi\left(\theta\right)t\left(y\right)\right\} ,
\]

\end_inset

where 
\begin_inset Formula $\phi\left(\theta\right)$
\end_inset

 is called the 
\shape italic
natural parameter
\shape default
 and 
\begin_inset Formula $t\left(y\right)$
\end_inset

 is a 
\shape italic
sufficient statistic
\shape default
 for 
\begin_inset Formula $\theta$
\end_inset

.
 The conjugate prior has the form
\begin_inset Formula 
\[
p\left(\theta\right)\propto g\left(\theta\right)^{n_{0}}\exp\left\{ \phi\left(\theta\right)\nu\right\} ,
\]

\end_inset

where 
\begin_inset Formula $\nu$
\end_inset

 represents the prior expected value of 
\begin_inset Formula $t\left(Y\right)$
\end_inset

 and 
\begin_inset Formula $n_{0}/\left(n_{0}+n\right)$
\end_inset

 represents how informative the prior is relative to the data.
 With observed iid data 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

, the likelihood for 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\[
\mathcal{L}\left(\theta\right)=\prod_{i=1}^{n}p\left(y_{i}|\theta\right)=\left(\prod_{i=1}^{n}h\left(y_{i}\right)\right)g\left(\theta\right)^{n}\exp\left\{ \phi\left(\theta\right)\sum_{i=1}^{n}t\left(y_{i}\right)\right\} .
\]

\end_inset

The posterior distribution then becomes 
\begin_inset Formula 
\[
p\left(\theta|y\right)\propto g\left(\theta\right)^{n_{0}+n}\exp\left\{ \phi\left(\theta\right)\left(\nu+\sum_{i=1}^{n}t\left(y_{i}\right)\right)\right\} .
\]

\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial conjugate prior]
\end_layout

\end_inset

Suppose 
\begin_inset Formula $Y\sim\text{Binomial}\left(n,\theta\right)$
\end_inset

.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:exp-family-binomial"

\end_inset

 implies that 
\begin_inset Formula 
\[
p\left(y|\theta\right)=\binom{n}{y}\left(1-\theta\right)^{n}\exp\left\{ y\log\frac{\theta}{1-\theta}\right\} ,
\]

\end_inset

so that 
\begin_inset Formula $g\left(\theta\right)=1-\theta$
\end_inset

, 
\begin_inset Formula $\phi\left(\theta\right)=\log\left(\theta/\left(1-\theta\right)\right)$
\end_inset

 is the natural parameter, and 
\begin_inset Formula $t\left(y\right)=y$
\end_inset

 is a sufficient statistic.
 Then, the conjugate prior for 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
p\left(\theta\right) & \propto g\left(\theta\right)^{n_{0}}\exp\left\{ \phi\left(\theta\right)\nu\right\} \\
 & =\left(1-\theta\right)^{n_{0}}\exp\left\{ \nu\log\frac{\theta}{1-\theta}\right\} \\
 & =\left(1-\theta\right)^{n_{0}}\left[\exp\left\{ \log\frac{\theta}{1-\theta}\right\} \right]^{\nu}\\
 & =\left(1-\theta\right)^{n_{0}}\left(\frac{\theta}{1-\theta}\right)^{\nu}\\
 & =\theta^{\nu}\left(1-\theta\right)^{n_{0}}\left(1-\theta\right)^{-\nu}\\
 & =\theta^{\nu}\left(1-\theta\right)^{n_{0}-\nu},
\end{flalign*}

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\text{Beta}\left(\nu+1,n_{0}-\nu+1\right)$
\end_inset

 distribution, i.e., the conjugate prior for 
\begin_inset Formula $\theta$
\end_inset

 is a beta distribution.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Poisson conjugate prior]
\end_layout

\end_inset

Suppose 
\begin_inset Formula $X_{1},\ldots,X_{n}$
\end_inset

 is an iid sample from 
\begin_inset Formula $\text{Poisson}\left(\lambda\right)$
\end_inset

.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:suff-stat-poisson"

\end_inset

 implies that the joint pmf of the 
\begin_inset Formula $X_{i}\text{'s}$
\end_inset

 can be written as
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{x}|\theta\right) & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-\theta n}\theta^{\sum_{i=1}^{n}x_{i}}\\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-\theta n}\exp\left\{ \log\theta^{\sum_{i=1}^{n}x_{i}}\right\} \\
 & =\left(\prod_{i=1}^{n}\frac{1}{x_{i}!}\right)\mathrm{e}^{-\theta n}\exp\left\{ \left(\sum_{i=1}^{n}x_{i}\right)\log\theta\right\} .
\end{flalign*}

\end_inset

Then, we have 
\begin_inset Formula $g\left(\theta\right)=\mathrm{e}^{-\theta}$
\end_inset

 and 
\begin_inset Formula $\phi\left(\theta\right)=\log\theta$
\end_inset

, so that the conjugate prior for 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\[
p\left(\theta\right)\propto\mathrm{e}^{-\theta n_{0}}\exp\left\{ \nu\log\theta\right\} =\mathrm{e}^{-\theta n_{0}}\left(\exp\left\{ \log\theta\right\} \right)^{\nu}=\mathrm{e}^{-\theta n_{0}}\theta^{\nu},
\]

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\text{Gamma}\left(\nu+1,n_{0}\right)$
\end_inset

 random variable.
 Taking 
\begin_inset Formula $\theta\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

, the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\[
p\left(\theta|\mathbf{x}\right)=\frac{p\left(\mathbf{x}|\theta\right)p\left(\theta\right)}{\int_{\Theta}p\left(\mathbf{x}|\theta\right)p\left(\theta\right)\dif\theta}\propto\left[\mathrm{e}^{-\theta n}\theta^{\sum_{i=1}^{n}x_{i}}\right]\left[\theta^{\alpha-1}\mathrm{e}^{-\beta\theta}\right]=\theta^{\alpha+\sum_{i=1}^{n}x_{i}-1}\mathrm{e}^{-\theta\left(\beta+n\right)},
\]

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\text{Gamma}\left(\alpha+\sum_{i=1}^{n}x_{i},\beta+n\right)$
\end_inset

 random variable.
 The prior mean is 
\begin_inset Formula $\alpha/\beta$
\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:mle-poisson"

\end_inset

 implies that the maximum likelihood estimate is 
\begin_inset Formula $\hat{\theta}=\bar{x}$
\end_inset

.
 The posterior mean is then
\begin_inset Formula 
\begin{flalign*}
\E\left[\theta|\mathbf{x}\right] & =\frac{\alpha+\sum_{i=1}^{n}x_{i}}{\beta+n}\tag{\ensuremath{\theta|\mathbf{x}\sim\text{Gamma}\left(\alpha+\sum_{i=1}^{n}x_{i},\beta+n\right)}}\\
 & =\frac{\alpha}{\beta+n}+\frac{\sum_{i=1}^{n}x_{i}}{\beta+n}\\
 & =\frac{\beta}{\beta+n}\frac{\alpha}{\beta}+\frac{n}{\beta+n}\bar{x},
\end{flalign*}

\end_inset

which we see is the average of the prior mean and MLE with weights 
\begin_inset Formula $\beta/\left(\beta+n\right)$
\end_inset

 and 
\begin_inset Formula $n/\left(\beta+n\right)$
\end_inset

, respectively.
\end_layout

\begin_layout Section
Noninformative priors
\end_layout

\begin_layout Standard
A prior 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

 is noninformative (vague, flat) if it has minimal impact on the posterior
 distribution 
\begin_inset Formula $f\left(\theta|y\right)$
\end_inset

.
 Roughly speaking, a prior distribution is noninformative if the prior is
 
\begin_inset Quotes eld
\end_inset

flat
\begin_inset Quotes erd
\end_inset

 relative to the likelihood function.
 For example, if 
\begin_inset Formula $\theta\sim\mathcal{N}\left(\mu_{0},\tau^{2}\right)$
\end_inset

 and 
\begin_inset Formula $\tau^{2}\rightarrow\infty$
\end_inset

, then we get a noninformative prior.
 That is, we can pick 
\begin_inset Formula $\tau^{2}$
\end_inset

 large enough to obtain a noninformative prior.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:binomial-prior-alt-param"

\end_inset

Consider 
\begin_inset Formula $X\sim\text{Binomial}\left(n,\theta\right)$
\end_inset

.
 We know that 
\begin_inset Formula $\theta\in\left[0,1\right]$
\end_inset

, and the flat prior on 
\begin_inset Formula $\theta$
\end_inset

 is the uniform distribution, 
\begin_inset Formula $\pi\left(\theta\right)=1$
\end_inset

.
 Consider the parameterization using the log-odds, 
\begin_inset Formula 
\[
\rho=\log\frac{\theta}{1-\theta}\implies\mathrm{e}^{\rho}=\frac{\theta}{1-\theta}\implies\theta=\mathrm{e}^{\rho}-\mathrm{e}^{\rho}\theta\implies\theta\left(1+\mathrm{e}^{\rho}\right)=\mathrm{e}^{\rho}\implies\theta=\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}},
\]

\end_inset

which maps 
\begin_inset Formula $\theta$
\end_inset

 to the real numbers.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:pdf-of-function-of-rv"

\end_inset

 implies that a pdf of 
\begin_inset Formula $\rho$
\end_inset

 is
\begin_inset Formula 
\[
\pi_{\rho}\left(\rho\right)=\pi_{\theta}\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)\left|\frac{\dif}{\dif\rho}\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right|=1\left|\frac{\mathrm{e}^{\rho}\left(1+\mathrm{e}^{\rho}\right)-\mathrm{e}^{\rho}\left(0+\mathrm{e}^{p}\right)}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right|=\left|\frac{\mathrm{e}^{\rho}+\mathrm{e}^{2\rho}-\mathrm{e}^{2\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right|=\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}.
\]

\end_inset

Under this parameterization, the prior distribution 
\begin_inset Formula $\pi\left(\rho\right)$
\end_inset

 is no longer flat.
 This example shows a prior that is noninformative is one parameterization,
 but becomes informative through a change of variables.
\end_layout

\begin_layout Section
Improper priors
\end_layout

\begin_layout Definition
A prior is said to be 
\shape italic
improper
\shape default
 if
\begin_inset Formula 
\[
\int_{\Theta}\pi\left(\theta\right)\dif\theta=\infty.
\]

\end_inset


\end_layout

\begin_layout Standard
If a prior integrates to any finite constant, then it may be normalized
 to yield a proper prior.
 Improper priors yield noninformative priors.
 The posterior obtained from an improper prior may be either proper or improper
 (though inference cannot be made with improper posterior distributions).
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal mean, known variance]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:posterior-normal-known-variance"

\end_inset

Let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 be iid samples from a 
\begin_inset Formula $\mathcal{N}\left(\theta,1^{2}\right)$
\end_inset

 distribution, and suppose 
\begin_inset Formula $\pi\left(\theta\right)\propto1$
\end_inset

 for 
\begin_inset Formula $\theta\in\mathbb{R}$
\end_inset

.
 We see that 
\begin_inset Formula 
\begin{flalign*}
\int_{\Theta}\pi\left(\theta\right)\dif\theta & =\int_{-\infty}^{\infty}1\dif\theta\\
 & =\lim_{c\rightarrow\infty}\lim_{k\rightarrow-\infty}\int_{k}^{c}1\dif\theta\\
 & =\lim_{c\rightarrow\infty}\lim_{k\rightarrow-\infty}\theta\rvert_{k}^{c}\\
 & =\lim_{c\rightarrow\infty}\lim_{k\rightarrow-\infty}\left(c-k\right)\\
 & =\lim_{c\rightarrow\infty}\left(c-\left(-\infty\right)\right)\\
 & =\infty+\infty\\
 & =\infty,
\end{flalign*}

\end_inset

i.e., the prior distribution is improper.
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:suff-stat-normal"

\end_inset

 implies that the joint density of the samples is given by
\begin_inset Formula 
\[
p\left(\mathbf{y}|\theta\right)=\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} ,
\]

\end_inset

so that the posterior distribution is
\begin_inset Formula 
\begin{flalign*}
p\left(\theta|\mathbf{y}\right) & \propto\frac{\left[\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} \right]\cdot1}{\int_{\Theta}\left[\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} \right]\cdot1\dif\theta}\\
 & =\frac{\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} }{\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} \dif\theta}.
\end{flalign*}

\end_inset

The denominator becomes
\begin_inset Formula 
\begin{flalign*}
 & \quad\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{1}{2\sigma^{2}}\left(n\theta^{2}-2\theta n\bar{y}\right)\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{n}{2\sigma^{2}}\left(\theta^{2}-2\theta\bar{y}\right)\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{n}{2\sigma^{2}}\left(\theta^{2}-2\theta\bar{y}+\bar{y}^{2}-\bar{y}^{2}\right)\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{n}{2\sigma^{2}}\left(-\bar{y}^{2}\right)-\frac{n}{2\sigma^{2}}\left(\theta^{2}-2\theta\bar{y}+\bar{y}^{2}\right)\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)-\frac{1}{2\sigma^{2}/n}\left(\theta-\bar{y}\right)^{2}\right\} \dif\theta\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} \exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\theta\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} \int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\theta\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} n^{-n/2}\int_{-\infty}^{\infty}\frac{\left(2\pi\sigma^{2}\right)^{-n/2}}{n^{-n/2}}\exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\theta\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} n^{-n/2}\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}/n\right)^{-n/2}\exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\theta.
\end{flalign*}

\end_inset

We recognize the integrand as the pdf of a 
\begin_inset Formula $\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)$
\end_inset

 random variable, and it follows that
\begin_inset Formula 
\begin{flalign*}
 & \quad\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} n^{-n/2}\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}/n\right)^{-n/2}\exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\theta\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} n^{-n/2}\cdot1\\
 & =n^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} .
\end{flalign*}

\end_inset

Then, the posterior distribution becomes
\begin_inset Formula 
\begin{flalign*}
p\left(\theta|\mathbf{y}\right) & \propto\frac{\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} }{n^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} }\\
 & =\left(\frac{2\pi\sigma^{2}}{n}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)\right\} \exp\left\{ -\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right]\right\} \\
 & =\left(\frac{2\pi\sigma^{2}}{n}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\theta\sum_{i=1}^{n}y_{i}+n\theta^{2}\right)+\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}\right)\right\} \\
 & =\left(\frac{2\pi\sigma^{2}}{n}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{1}{2\sigma^{2}}\left(n\theta^{2}-2\theta n\bar{y}\right)+\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}y_{i}^{2}-\frac{1}{2\sigma^{2}}n\bar{y}^{2}\right\} \\
 & =\left(\frac{2\pi\sigma^{2}}{n}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n\theta^{2}-2\theta n\bar{y}+n\bar{y}^{2}\right)\right\} \\
 & =\left(\frac{2\pi\sigma^{2}}{n}\right)^{-n/2}\exp\left\{ -\frac{\left(\theta-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} ,
\end{flalign*}

\end_inset

i.e., the posterior distribution of 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula $\theta|\mathbf{y}\sim\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)$
\end_inset

, which is proper.
 We have 
\begin_inset Formula $\sigma^{2}=1$
\end_inset

, so that 
\begin_inset Formula $\theta|\mathbf{y}\sim\mathcal{N}\left(\bar{y},1/n\right)$
\end_inset

.
\end_layout

\begin_layout Section
Jeffreys prior
\end_layout

\begin_layout Standard
Jeffreys (1961) suggested a default procedure for specifying a prior distributio
n for 
\begin_inset Formula $\theta$
\end_inset

 that is invariant under transformation, 
\begin_inset Formula 
\[
\pi\left(\theta\right)\propto\left[I\left(\theta\right)\right]^{1/2},
\]

\end_inset

where 
\begin_inset Formula $I\left(\theta\right)$
\end_inset

 is the expected Fisher information as given in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:cramér-rao"

\end_inset

 (the expectation is taken with respect to the sampling distribution of
 
\begin_inset Formula $Y$
\end_inset

 given 
\begin_inset Formula $\theta$
\end_inset

).
 Jeffreys prior gives an automated scheme for finding a noninformative prior
 for any parametric model 
\begin_inset Formula $f\left(y|\theta\right)$
\end_inset

.
 Jeffreys prior is improper for many models, although it may be proper for
 certain models.
 Jeffreys prior generally works well for one-parameter models, though it
 is not as straightforward for multiparameter models.
\end_layout

\begin_layout Standard
Jeffreys general principle is that any rule for determining the prior density
 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

 should yield an equivalent result if applied to the transformed parameter.
 Consider a one-to-one transformation of the parameter 
\begin_inset Formula $\phi=h\left(\theta\right)$
\end_inset

.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:pdf-of-function-of-rv"

\end_inset

 implies that a pdf of the transformed parameter is
\begin_inset Formula 
\[
\pi_{\Phi}\left(\phi\right)=\pi_{\Theta}\left(h^{-1}\left(\phi\right)\right)\left|\frac{\dif}{\dif\phi}h^{-1}\left(\phi\right)\right|=\pi_{\Theta}\left(\theta\right)\left|\frac{\dif\theta}{\dif\phi}\right|,
\]

\end_inset

where the final equality holds for the Jeffreys prior.
 Using 
\begin_inset Formula $I\left(\phi\right)$
\end_inset

, we have 
\begin_inset Formula 
\[
I\left(\phi\right)=-\E\left[\frac{\partial^{2}}{\partial\phi^{2}}\log p\left(y|\phi\right)\right]=-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log p\left(y|\theta=h^{-1}\left(\phi\right)\right)\left|\frac{\dif\theta}{\dif\phi}\right|^{2}\right]=I\left(\theta\right)\left|\frac{\dif\theta}{\dif\phi}\right|^{2},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\pi_{\Phi}\left(\phi\right)=\left[I\left(\phi\right)\right]^{1/2}=\left[I\left(\theta\right)\left|\frac{\dif\theta}{\dif\phi}\right|^{2}\right]^{1/2}=\left[I\left(\theta\right)\right]^{1/2}\left|\frac{\dif\theta}{\dif\phi}\right|=\pi_{\Theta}\left(\theta\right)\left|\frac{\dif\theta}{\dif\phi}\right|,
\]

\end_inset

as obtained with the change of variable formula.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Jeffreys prior for normal mean]
\end_layout

\end_inset

Suppose 
\begin_inset Formula $Y\sim\mathcal{N}\left(\mu,1\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Find Jeffreys prior for 
\begin_inset Formula $\mu$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The log-likelihood is
\begin_inset Formula 
\[
\ell\left(\theta|\mathbf{y}\right)=\log\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left(\frac{y-\theta}{\sigma}\right)^{2}\right\} =-\frac{1}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2}\left(\frac{y-\theta}{\sigma}\right)^{2}.
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\theta}\ell\left(\theta|\mathbf{y}\right) & =0-\frac{1}{2}\cdot2\left(\frac{y-\theta}{\sigma}\right)\left(-\frac{1}{\sigma}\right)=\frac{y-\theta}{\sigma^{2}}.
\end{flalign*}

\end_inset

The second derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 is then
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta^{2}}\ell\left(\theta|\mathbf{y}\right)=\frac{\partial}{\partial\theta}\frac{y-\theta}{\sigma^{2}}=0-\frac{1}{\sigma^{2}}=-1.\tag{\ensuremath{\sigma^{2}=1}}
\]

\end_inset

Then, the Fisher information is
\begin_inset Formula 
\[
I\left(\theta\right)=-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\ell\left(\theta|\mathbf{y}\right)\right]=-\E\left[-1\right]=1,
\]

\end_inset

so that the Jeffreys prior for 
\begin_inset Formula $\mu$
\end_inset

 is 
\begin_inset Formula $\pi\left(\theta\right)\propto\sqrt{I\left(\theta\right)}=\sqrt{1}=1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $\psi=\exp\left(\mu\right)$
\end_inset

.
 Find Jeffreys prior for 
\begin_inset Formula $\psi$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We have 
\begin_inset Formula $\psi=\mathrm{e}^{\mu}\implies\mu=\log\psi$
\end_inset

.
 Then, a pdf for 
\begin_inset Formula $\psi$
\end_inset

 is
\begin_inset Formula 
\[
\pi_{\Psi}\left(\psi\right)=\pi_{\mu}\left(\log\psi\right)\left|\frac{\dif}{\dif\psi}\log\psi\right|\propto1\cdot\left|\frac{1}{\psi}\right|=\frac{1}{\psi}.
\]

\end_inset

Noting that 
\begin_inset Formula $\mu\in\mathbb{R}\implies\psi\in\left(0,\infty\right)$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\int_{0}^{\infty}\frac{1}{\psi}\dif\psi & =\int_{0}^{x}\frac{1}{\psi}\dif\psi+\int_{x}^{\infty}\frac{1}{\psi}\dif\psi\\
 & =\lim_{c\rightarrow0^{+}}\int_{c}^{x}\frac{1}{\psi}\dif\psi+\lim_{c\rightarrow\infty}\int_{x}^{c}\frac{1}{\psi}\dif\psi\\
 & =\lim_{c\rightarrow0^{+}}\left[\log\psi\right|_{c}^{x}+\lim_{c\rightarrow\infty}\left[\log\psi\right|_{x}^{c}\tag{\ensuremath{\psi>0\implies\left|\psi\right|=\psi}}\\
 & =\lim_{c\rightarrow0^{+}}\left(\log x-\log c\right)+\lim_{c\rightarrow\infty}\left(\log c-\log x\right)\\
 & =\log x-\lim_{c\rightarrow0^{+}}\log c+\lim_{c\rightarrow\infty}\log c-\log x\\
 & =-\left(-\infty\right)+\infty\\
 & =\infty,
\end{flalign*}

\end_inset

i.e., 
\begin_inset Formula $\pi_{\Psi}\left(\psi\right)$
\end_inset

 is improper.
 Now, the density of 
\begin_inset Formula $Y$
\end_inset

 (parameterized in terms of 
\begin_inset Formula $\psi$
\end_inset

) is
\begin_inset Formula 
\[
p\left(y|\psi\right)=\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left(\frac{y-\log\psi}{\sigma}\right)^{2}\right\} ,
\]

\end_inset

so that the log-likelihood is
\begin_inset Formula 
\[
\ell\left(\psi|\mathbf{y}\right)=\log\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left(\frac{y-\log\psi}{\sigma}\right)^{2}\right\} =-\frac{1}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2}\left(\frac{y-\log\psi}{\sigma}\right)^{2}.
\]

\end_inset

Taking the second derivative with respect to 
\begin_inset Formula $\psi$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\psi^{2}}\ell\left(\psi|\mathbf{y}\right) & =\frac{\partial}{\partial\psi}\left[\frac{\partial}{\partial\psi}-\frac{1}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2}\left(\frac{y-\log\psi}{\sigma}\right)^{2}\right]\\
 & =\frac{\partial}{\partial\psi}\left[0-\frac{1}{2}\cdot2\left(\frac{y-\log\psi}{\sigma}\right)\left(-\frac{1}{\sigma\psi}\right)\right]\\
 & =\frac{\partial}{\partial\psi}\frac{y-\log\psi}{\sigma^{2}\psi}\\
 & =\frac{\left(0-1/\psi\right)\left(\sigma^{2}\psi\right)-\left(y-\log\psi\right)\left(\sigma^{2}\right)}{\left(\sigma^{2}\psi\right)^{2}}\\
 & =\frac{-\sigma^{2}-\sigma^{2}\left(y-\log\psi\right)}{\left(\sigma^{2}\psi\right)^{2}}\\
 & =\frac{-1-\left(y-\log\psi\right)}{\psi^{2}}.\tag{\ensuremath{\sigma^{2}=1}}
\end{flalign*}

\end_inset

Then, the Fisher information is 
\begin_inset Formula 
\begin{flalign*}
I\left(\psi\right) & =-\E\left[\frac{\partial^{2}}{\partial\psi^{2}}\log p\left(Y|\psi\right)\right]\\
 & =-\E\left[-\frac{1}{\psi^{2}}-\frac{1}{\psi^{2}}\left(Y-\log\psi\right)\right]\\
 & =-\left(\E\left[-\frac{1}{\psi^{2}}\right]-\E\left[\frac{1}{\psi^{2}}\left(Y-\log\psi\right)\right]\right)\\
 & =-\left(-\frac{1}{\psi^{2}}-\frac{1}{\psi^{2}}\E\left[Y-\log\psi\right]\right)\\
 & =\frac{1}{\psi^{2}}+\frac{1}{\psi^{2}}\left(\E\left[Y\right]-\E\left[\log\psi\right]\right)\\
 & =\frac{1}{\psi^{2}}\left(1+\E\left[Y\right]-\log\psi\right)\\
 & =\frac{1}{\psi^{2}}\left(1+\log\psi-\log\psi\right)\tag{\ensuremath{\E\left[Y\right]=\log\psi}}\\
 & =\frac{1}{\psi^{2}}\left(1+0\right)\\
 & =\frac{1}{\psi^{2}},
\end{flalign*}

\end_inset

so that Jeffreys prior for 
\begin_inset Formula $\psi$
\end_inset

 is
\begin_inset Formula 
\[
\pi\left(\psi\right)\propto\sqrt{I\left(\psi\right)}=\sqrt{\frac{1}{\psi^{2}}}=\frac{1}{\psi},
\]

\end_inset

which agrees with the result obtained from the change of variable formula.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Binomial Jeffreys prior]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:jeffreys-prior-binomial"

\end_inset

Suppose 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 are iid 
\begin_inset Formula $\text{Binomial}\left(1,\theta\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Compute Jeffreys prior for 
\begin_inset Formula $\theta$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
We have 
\begin_inset Formula $Y_{i}\sim\text{Bernoulli}\left(\theta\right)$
\end_inset

, so that 
\begin_inset Formula 
\[
\log p\left(y|\theta\right)=\log\theta^{y}\left(1-\theta\right)^{1-y}=y\log\theta+\left(1-y\right)\log\left(1-\theta\right).
\]

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{\partial}{\partial\theta}p\left(y|\theta\right)=\frac{\partial}{\partial\theta}\left[y\log\theta+\left(1-y\right)\log\left(1-\theta\right)\right]=\frac{y}{\theta}+\frac{1-y}{1-\theta}\left(-1\right)=\frac{y}{\theta}-\frac{1-y}{1-\theta}.
\]

\end_inset

Then, the second derivative with respect to 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\theta^{2}}p\left(y|\theta\right)=\frac{\partial}{\partial\theta}\left[\frac{y}{\theta}-\frac{1-y}{1-\theta}\right]=-\frac{y}{\theta^{2}}-\frac{1-y}{\left(1-\theta\right)^{2}}\left(-1\right)\left(-1\right)=-\frac{y}{\theta^{2}}-\frac{1-y}{\left(1-\theta\right)^{2}},
\]

\end_inset

so that the Fisher information is
\begin_inset Formula 
\begin{flalign*}
I\left(\theta\right) & =-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log p\left(Y|\theta\right)\right]\\
 & =-\E\left[-\frac{Y}{\theta^{2}}-\frac{1-Y}{\left(1-\theta\right)^{2}}\right]\\
 & =-\left(\E\left[-\frac{Y}{\theta^{2}}\right]-\E\left[\frac{1-Y}{\left(1-\theta\right)^{2}}\right]\right)\\
 & =\frac{1}{\theta^{2}}\E\left[Y\right]-\frac{1}{\left(1-\theta\right)^{2}}\left(\E\left[1\right]-\E\left[Y\right]\right)\\
 & =\frac{1}{\theta^{2}}\theta-\frac{1-\theta}{\left(1-\theta\right)^{2}}\tag{\ensuremath{\E\left[Y\right]=\theta}}\\
 & =\frac{1}{\theta}-\frac{1}{1-\theta}\\
 & =\frac{1-\theta-\theta}{\theta\left(1-\theta\right)}\\
 & =\frac{1}{\theta\left(1-\theta\right)}.
\end{flalign*}

\end_inset

Then, the Jeffreys prior for 
\begin_inset Formula $\theta$
\end_inset

 is 
\begin_inset Formula 
\[
\pi\left(\theta\right)\propto\sqrt{I\left(\theta\right)}=\sqrt{\frac{1}{\theta\left(1-\theta\right)}}=\theta^{-1/2}\left(1-\theta\right)^{-1/2}=\theta^{1/2-1}\left(1-\theta\right)^{1/2-1},
\]

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\text{Beta}\left(1/2,1/2\right)$
\end_inset

 random variable, i.e., 
\begin_inset Formula $\theta\sim\text{Beta}\left(1/2,1/2\right)$
\end_inset

, which is proper.
\end_layout

\end_deeper
\begin_layout Enumerate
Show that Jeffreys prior is invariant to the transformation 
\begin_inset Formula $\rho=\log\left(\theta/\left(1-\theta\right)\right)$
\end_inset

.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:binomial-prior-alt-param"

\end_inset

, we have 
\begin_inset Formula 
\[
\theta=\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}.
\]

\end_inset

Thus, to show that Jeffreys prior is invariant to this transformation, we
 must show that 
\begin_inset Formula 
\[
\left|I\left(\rho\right)\right|^{1/2}=\left|I\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)\right|^{1/2}\left|\frac{\dif}{\dif\rho}\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right|.
\]

\end_inset

The density of 
\begin_inset Formula $Y$
\end_inset

 (parameterized in terms of 
\begin_inset Formula $\rho$
\end_inset

) is
\begin_inset Formula 
\[
p\left(y|\rho\right)=\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{y}\left(1-\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{1-y}.
\]

\end_inset

Then,
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\rho^{2}}\log p\left(y|\rho\right) & =\frac{\partial^{2}}{\partial\rho^{2}}\log\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{y}\left(1-\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{1-y}\\
 & =\frac{\partial^{2}}{\partial\rho^{2}}\left[y\log\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}+\left(1-y\right)\log\left(\frac{1+\mathrm{e}^{\rho}-\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)\right]\\
 & =\frac{\partial^{2}}{\partial\rho^{2}}\left[y\left(\log\mathrm{e}^{\rho}-\log\left(1+\mathrm{e}^{\rho}\right)\right)+\left(1-y\right)\left(\log\left(1\right)-\log\left(1+\mathrm{e}^{\rho}\right)\right)\right]\\
 & =\frac{\partial^{2}}{\partial\rho^{2}}\left[y\rho-y\log\left(1+\mathrm{e}^{\rho}\right)+\left(1-y\right)\left(0-\log\left(1+\mathrm{e}^{\rho}\right)\right)\right]\\
 & =\frac{\partial^{2}}{\partial\rho^{2}}\left[y\rho-y\log\left(1+\mathrm{e}^{\rho}\right)-\log\left(1+\mathrm{e}^{\rho}\right)+y\log\left(1+\mathrm{e}^{\rho}\right)\right]\\
 & =\frac{\partial^{2}}{\partial\rho^{2}}\left[y\rho-\log\left(1+\mathrm{e}^{\rho}\right)\right]\\
 & =\frac{\partial}{\partial\rho}\left[y-\frac{1}{1+\mathrm{e}^{\rho}}\left(\mathrm{e}^{\rho}\right)\right]\\
 & =0-\frac{\mathrm{e}^{\rho}\left(1+\mathrm{e}^{\rho}\right)-\mathrm{e}^{\rho}\left(\mathrm{e}^{\rho}\right)}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\\
 & =-\frac{\mathrm{e}^{\rho}+\mathrm{e}^{2\rho}-\mathrm{e}^{2\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\\
 & =-\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}},
\end{flalign*}

\end_inset

so that the Fisher information is
\begin_inset Formula 
\[
I\left(\rho\right)=-\E\left[\frac{\partial^{2}}{\partial\rho^{2}}\log p\left(Y|\rho\right)\right]=-\E\left[-\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right]=\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}.
\]

\end_inset

Then, the Jeffreys prior is
\begin_inset Formula 
\begin{equation}
\pi_{\rho}\left(\rho\right)\propto\sqrt{I\left(\rho\right)}=\sqrt{\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}}.\label{eq:jeffreys-prior-binomial}
\end{equation}

\end_inset

Now, 
\begin_inset Formula 
\begin{flalign*}
\left|I\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)\right|^{1/2}\left|\frac{\partial}{\partial\rho}\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right| & =\left[\left(\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{-1/2}\left(1-\frac{\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{-1/2}\right]\left|\frac{\mathrm{e}^{\rho}\left(1+\mathrm{e}^{\rho}\right)-\mathrm{e}^{\rho}\left(\mathrm{e}^{\rho}\right)}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right|\\
 & =\frac{\left(1+\mathrm{e}^{\rho}\right)^{1/2}}{\mathrm{e}^{\rho/2}}\left(\frac{1+\mathrm{e}^{\rho}-\mathrm{e}^{\rho}}{1+\mathrm{e}^{\rho}}\right)^{-1/2}\left|\frac{\mathrm{e}^{\rho}+\mathrm{e}^{2\rho}-\mathrm{e}^{2\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right|\\
 & =\frac{\left(1+\mathrm{e}^{\rho}\right)^{1/2}}{\mathrm{e}^{\rho/2}}\frac{\left(1+\mathrm{e}^{\rho}\right)^{1/2}}{1}\left|\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right|\\
 & =\frac{\left(1+\mathrm{e}^{\rho}\right)}{\mathrm{e}^{\rho/2}}\left(\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}\right)\tag{\ensuremath{\mathrm{e}^{\rho}>0}}\\
 & =\frac{\mathrm{e}^{\rho}\mathrm{e}^{-\rho/2}}{1+\mathrm{e}^{\rho}}\\
 & =\frac{\mathrm{e}^{\rho/2}}{1+\mathrm{e}^{\rho}}\\
 & =\frac{\sqrt{\mathrm{e}^{\rho}}}{\mathrm{\sqrt{\left(1+\mathrm{e}^{\rho}\right)^{2}}}}\\
 & =\sqrt{\frac{\mathrm{e}^{\rho}}{\left(1+\mathrm{e}^{\rho}\right)^{2}}},
\end{flalign*}

\end_inset

which agrees with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:jeffreys-prior-binomial"

\end_inset

, and the result has been shown.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Likelihood principle and experimental design
\end_layout

\begin_layout Standard
Experimental design refers to the method used to collect the data.
 The likelihood principle refers to the concept that all the information
 carried in a sample is contained in the likelihood function.
 Jeffreys prior accounts for the experimental design, thereby violating
 the likelihood principle.
 The following examples illustrate this view.
\end_layout

\begin_layout Example
Consider the scenario in which we toss a coin 
\begin_inset Formula $n$
\end_inset

 times and observe 
\begin_inset Formula $r$
\end_inset

 heads.
 Let 
\begin_inset Formula $X\sim\text{Binomial}\left(n,\theta\right)$
\end_inset

, so that the likelihood function is
\begin_inset Formula 
\[
p\left(X=r|\theta\right)=\mathcal{L}\left(\theta\right)=\binom{n}{r}\theta^{r}\left(1-\theta\right)^{n-r}.
\]

\end_inset

In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:jeffreys-prior-binomial"

\end_inset

, we showed that the Jeffreys prior is given by
\begin_inset Formula 
\[
\pi\left(\theta\right)\propto\theta^{-1/2}\left(1-\theta\right)^{-1/2},
\]

\end_inset

and the corresponding posterior is
\begin_inset Formula 
\[
\theta|X=r\sim\text{Beta}\left(r+\frac{1}{2},n-r+\frac{1}{2}\right).
\]

\end_inset

Now consider the scenario in which we toss a coin until we see 
\begin_inset Formula $r$
\end_inset

 heads and end up tossing it 
\begin_inset Formula $n=y+r$
\end_inset

 times in total.
 Then, 
\begin_inset Formula $Y\sim\mathcal{NB}\left(r,\theta\right)$
\end_inset

, with likelihood function 
\begin_inset Formula 
\[
p\left(Y=y|\theta\right)=\mathcal{L}\left(\theta\right)=\binom{y+r-1}{r-1}\theta^{r}\left(1-\theta\right)^{y}=\binom{n-1}{r-1}\theta^{r}\left(1-\theta\right)^{n-r}.
\]

\end_inset

The difference between the two scenarios is captured in the constant terms
 
\begin_inset Formula $\binom{n-1}{r-1}$
\end_inset

 versus 
\begin_inset Formula $\binom{n}{r}$
\end_inset

, which are considered to be exclusively part of the experimental design
 since they do not affect the shape of the likelihood.
 In the negative binomial case, the second derivative of the log-likelihood
 with respect to 
\begin_inset Formula $\theta$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\theta^{2}}\ell\left(\theta\right) & =\frac{\partial^{2}}{\partial\theta^{2}}\log\binom{n-1}{r-1}\theta^{r}\left(1-\theta\right)^{n-r}\\
 & =\frac{\partial^{2}}{\partial\theta^{2}}\left[\log\binom{n-1}{r-1}+r\log\theta+\left(n-r\right)\log\left(1-\theta\right)\right]\\
 & =\frac{\partial}{\partial\theta}\left[0+\frac{r}{\theta}+\frac{n-r}{1-\theta}\left(-1\right)\right]\\
 & =-\frac{r}{\theta^{2}}-\frac{n-r}{\left(1-\theta\right)^{2}}\left(-1\right)\left(-1\right)\\
 & =-\frac{r}{\theta^{2}}-\frac{n-r}{\left(1-\theta\right)^{2}}\\
 & =-\frac{r}{\theta^{2}}-\frac{y}{\left(1-\theta\right)^{2}},\tag{\ensuremath{y=n-r}}
\end{flalign*}

\end_inset

so that the Fisher information is
\begin_inset Formula 
\begin{flalign*}
I\left(\theta\right) & =-\E\left[\frac{\partial^{2}}{\partial\theta^{2}}\log p\left(Y|\theta\right)\right]\\
 & =-\E\left[-\frac{r}{\theta^{2}}-\frac{Y}{\left(1-\theta\right)^{2}}\right]\\
 & =\frac{r}{\theta^{2}}+\frac{1}{\left(1-\theta\right)^{2}}\E\left[Y\right]\\
 & =\frac{r}{\theta^{2}}+\frac{1}{\left(1-\theta\right)^{2}}\frac{r\left(1-\theta\right)}{\theta}\\
 & =\frac{r}{\theta^{2}}+\frac{r}{\theta\left(1-\theta\right)}\\
 & =\frac{r\left(1-\theta\right)+\theta r}{\theta^{2}\left(1-\theta\right)}\\
 & =\frac{r}{\theta^{2}\left(1-\theta\right)}.
\end{flalign*}

\end_inset

Then, the Jeffreys prior is
\begin_inset Formula 
\[
\pi\left(\theta\right)\propto\sqrt{I\left(\theta\right)}=\sqrt{\frac{r}{\theta^{2}\left(1-\theta\right)}}=\frac{\sqrt{r}}{\sqrt{\theta^{2}\left(1-\theta\right)}}\propto\frac{1}{\theta\left(1-\theta\right)^{1/2}}=\theta^{-1}\left(1-\theta\right)^{-1/2}.
\]

\end_inset

While this resembles the kernel of a beta random variable, we cannot have
 the shape parameter 
\begin_inset Formula $\alpha=0$
\end_inset

, and it follows that this is an improper prior.
 The corresponding posterior is
\begin_inset Formula 
\[
\theta|X=r\sim\text{Beta}\left(r,n-r+\frac{1}{2}\right).
\]

\end_inset

In the negative binomial case 
\begin_inset Formula $r\neq0$
\end_inset

 (one cannot toss a coin until zero heads are observed), whereas in the
 binomial case 
\begin_inset Formula $r$
\end_inset

 can be zero.
 Jeffreys prior differs in the two cases, even though the likelihood functions
 are proportional.
 We see that Jeffreys prior is affected by the experimental design and can
 violate the likelihood principle.
\end_layout

\begin_layout Chapter
Multiparameter models
\end_layout

\begin_layout Standard
A model may include several parameters, but often interest lies in making
 inference about one or a few parameters.
 The parameters that are not of direct interest are called 
\shape italic
nuisance parameters
\shape default
.
 The marginal posterior distribution of the parameters of interest can be
 obtained by integrating the joint posterior density over the parameters
 that are not of immediate interest.
\end_layout

\begin_layout Section
Joint and marginal posterior distributions
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\theta_{2}\right)$
\end_inset

 and we are only interested in inference for 
\begin_inset Formula $\theta_{1}$
\end_inset

, so 
\begin_inset Formula $\theta_{2}$
\end_inset

 may be considered a 
\begin_inset Quotes eld
\end_inset

nuisance
\begin_inset Quotes erd
\end_inset

 parameter.
 The joint posterior density is given by
\begin_inset Formula 
\[
p\left(\theta_{1},\theta_{2}|y\right)\propto p\left(y|\theta_{1},\theta_{2}\right)p\left(\theta_{1},\theta_{2}\right).
\]

\end_inset

The marginal posterior density of 
\begin_inset Formula $\theta_{1}$
\end_inset

 is derived by averaging over 
\begin_inset Formula $\theta_{2}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
p\left(\theta_{1}|y\right)=\int_{\Theta_{2}}p\left(\theta_{1},\theta_{2}|y\right)\dif\theta_{2}.
\]

\end_inset

Alternatively, the marginal posterior density of 
\begin_inset Formula $\theta_{1}$
\end_inset

 can by obtained from
\begin_inset Formula 
\[
p\left(\theta_{1}|y\right)=\int_{\Theta_{2}}p\left(\theta_{1}|\theta_{2},y\right)p\left(\theta_{2}|y\right)\dif\theta_{2}.
\]

\end_inset

This integral is not often evaluated explicitly, but it suggests an important
 practical strategy:
\end_layout

\begin_layout Enumerate
first draw 
\begin_inset Formula $\theta_{2}$
\end_inset

 from its posterior distribution, 
\begin_inset Formula $p\left(\theta_{2}|y\right)$
\end_inset

,
\end_layout

\begin_layout Enumerate
then draw 
\begin_inset Formula $\theta_{1}$
\end_inset

 from its conditional posterior distribution, 
\begin_inset Formula $p\left(\theta_{1}|\theta_{2},y\right)$
\end_inset

, given the drawn value of 
\begin_inset Formula $\theta_{2}$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Normal data with noninformative prior]
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "exa:posterior-normal-unknown-variance"

\end_inset

Suppose 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 are iid 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 where 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 are both unknown.
 We begin by computing Jeffreys prior for 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

.
 In the multiparameter case, the Fisher information is a square matrix whose
 entries are the negative expectations of the second partial derivatives
 of the log-likelihood, i.e.,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{2.0}
\end_layout

\end_inset

 
\begin_inset Formula 
\[
\mathbf{I}\left(\boldsymbol{\theta}\right)=-\E\begin{bmatrix}\dfrac{\partial^{2}}{\partial\mu^{2}}\ell\left(\boldsymbol{\theta}\right) & \dfrac{\partial^{2}}{\partial\mu\partial\left(\sigma^{2}\right)}\ell\left(\boldsymbol{\theta}\right)\\
\dfrac{\partial^{2}}{\partial\left(\sigma^{2}\right)\partial\mu}\ell\left(\boldsymbol{\theta}\right) & \dfrac{\partial^{2}}{\partial\left(\sigma^{2}\right)^{2}}\ell\left(\boldsymbol{\theta}\right)
\end{bmatrix},
\]

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

and Jeffreys prior is 
\begin_inset Formula $\pi\left(\boldsymbol{\theta}\right)\propto\left[\det\mathbf{I}\left(\boldsymbol{\theta}\right)\right]^{1/2}$
\end_inset

.
 For 
\begin_inset Formula $n=1$
\end_inset

 observation, the log-likelihood is
\begin_inset Formula 
\[
\ell\left(\boldsymbol{\theta}\right)=-\frac{1}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\left(y-\mu\right)^{2},
\]

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\mu}\ell\left(\boldsymbol{\theta}\right) & =0-\frac{1}{2\sigma^{2}}\cdot2\left(y-\mu\right)\left(-1\right)=\frac{y-\mu}{\sigma^{2}}\\
\frac{\partial}{\partial\left(\sigma^{2}\right)}\ell\left(\boldsymbol{\theta}\right) & =-\frac{1}{2}\frac{1}{2\pi\sigma^{2}}\left(2\pi\right)-\frac{1}{2\left(\sigma^{2}\right)^{2}}\left(-1\right)\left(y-\mu\right)^{2}=-\frac{1}{2\sigma^{2}}+\frac{\left(y-\mu\right)^{2}}{2\left(\sigma^{2}\right)^{2}}.
\end{flalign*}

\end_inset

Then, the second partial derivatives are
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\mu^{2}}\ell\left(\boldsymbol{\theta}\right) & =0-\frac{1}{\sigma^{2}}=-\frac{1}{\sigma^{2}}\\
\frac{\partial^{2}}{\partial\mu\partial\left(\sigma^{2}\right)} & =0+\frac{2\left(y-\mu\right)}{2\left(\sigma^{2}\right)^{2}}\left(-1\right)=-\frac{y-\mu}{\left(\sigma^{2}\right)^{2}}\\
\frac{\partial^{2}}{\partial\left(\sigma^{2}\right)\partial\mu}\ell\left(\boldsymbol{\theta}\right) & =-\frac{y-\mu}{\left(\sigma^{2}\right)^{2}}\\
\frac{\partial^{2}}{\partial\left(\sigma^{2}\right)^{2}}\ell\left(\boldsymbol{\theta}\right) & =\frac{1}{2\left(\sigma^{2}\right)^{2}}+\left(-2\right)\frac{\left(y-\mu\right)^{2}}{2\left(\sigma^{2}\right)^{3}}=\frac{1}{2\left(\sigma^{2}\right)^{2}}-\frac{\left(y-\mu\right)^{2}}{\left(\sigma^{2}\right)^{3}},
\end{flalign*}

\end_inset

so that the Fisher information is 
\begin_inset Formula 
\begin{flalign*}
\mathbf{I}\left(\boldsymbol{\theta}\right) & =-\E\begin{bmatrix}-1/\sigma^{2} & \left(Y-\mu\right)/\left(\sigma^{2}\right)^{2}\\
\left(Y-\mu\right)/\left(\sigma^{2}\right)^{2} & 1/2\left(\sigma^{2}\right)^{2}-\left(Y-\mu\right)^{2}/\left(\sigma^{2}\right)^{3}
\end{bmatrix}\\
 & =-\begin{bmatrix}\E\left[-1/\sigma^{2}\right] & \E\left[\left(Y-\mu\right)/\left(\sigma^{2}\right)^{2}\right]\\
\E\left[\left(Y-\mu\right)/\left(\sigma^{2}\right)^{2}\right] & \E\left[1/2\left(\sigma^{2}\right)^{2}-\left(Y-\mu\right)^{2}/\left(\sigma^{2}\right)^{3}\right]
\end{bmatrix}\\
 & =-\begin{bmatrix}-1/\sigma^{2} & \left(1/\left(\sigma^{2}\right)^{2}\right)\E\left[Y-\mu\right]\\
\left(1/\left(\sigma^{2}\right)^{2}\right)\E\left[Y-\mu\right] & \E\left[1/2\left(\sigma^{2}\right)^{2}\right]-\E\left[\left(Y-\mu\right)^{2}/\left(\sigma^{2}\right)^{3}\right]
\end{bmatrix}\\
 & =-\begin{bmatrix}-1/\sigma^{2} & \left(1/\left(\sigma^{2}\right)^{2}\right)\left(\E\left[Y\right]-\E\left[\mu\right]\right)\\
\left(1/\left(\sigma^{2}\right)^{2}\right)\left(\E\left[Y\right]-\E\left[\mu\right]\right) & 1/2\left(\sigma^{2}\right)^{2}-\left(1/\left(\sigma^{2}\right)^{3}\right)\E\left[\left(Y-\mu\right)^{2}\right]
\end{bmatrix}.
\end{flalign*}

\end_inset

Now, the expected value of 
\begin_inset Formula $Y$
\end_inset

 is just its mean 
\begin_inset Formula $\mu$
\end_inset

, so it follows that 
\begin_inset Formula $\E\left[Y\right]-\E\left[\mu\right]=\mu-\mu=0$
\end_inset

.
 Similarly, the expected value of 
\begin_inset Formula $\E\left[\left(Y-\mu\right)^{2}\right]$
\end_inset

 is just the variance of 
\begin_inset Formula $Y$
\end_inset

, i.e., 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Then,
\begin_inset Formula 
\begin{flalign*}
\mathbf{I}\left(\boldsymbol{\theta}\right) & =-\begin{bmatrix}-1/\sigma^{2} & 0\\
0 & 1/2\left(\sigma^{2}\right)^{2}-\left(1/\left(\sigma^{2}\right)^{3}\right)\sigma^{2}
\end{bmatrix}\\
 & =-\begin{bmatrix}-1/\sigma^{2} & 0\\
0 & 1/2\left(\sigma^{2}\right)^{2}-1/\left(\sigma^{2}\right)^{2}
\end{bmatrix}\\
 & =-\begin{bmatrix}-1/\sigma^{2} & 0\\
0 & -1/2\left(\sigma^{2}\right)^{2}
\end{bmatrix}\\
 & =\begin{bmatrix}1/\sigma^{2} & 0\\
0 & 1/2\left(\sigma^{2}\right)^{2}
\end{bmatrix},
\end{flalign*}

\end_inset

so that Jeffreys prior is
\begin_inset Formula 
\[
\pi\left(\boldsymbol{\theta}\right)\propto\left[\det\mathbf{I}\left(\boldsymbol{\theta}\right)\right]^{1/2}=\left[\frac{1}{\sigma^{2}}\left(\frac{1}{2\left(\sigma^{2}\right)^{2}}\right)-0\right]^{1/2}=\left[\frac{1}{2\left(\sigma^{2}\right)^{3}}\right]^{1/2}=\left[2\left(\sigma^{2}\right)^{3}\right]^{-1/2}\propto\left(\sigma^{2}\right)^{-3/2}.
\]

\end_inset

Now consider the prior distribution 
\begin_inset Formula $p\left(\mu,\sigma^{2}\right)\propto1/\sigma^{2}$
\end_inset

.
 The posterior distribution of 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
p\left(\mu,\sigma^{2}|\mathbf{y}\right) & \propto p\left(\mathbf{y}|\mu,\sigma^{2}\right)p\left(\mu,\sigma^{2}\right)\\
 & \propto\left[\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}}{2\sigma^{2}}\right\} \right]\frac{1}{\sigma^{2}}\\
 & =\left(2\pi\right)^{-n/2}\left(\sigma^{2}\right)^{-n/2}\left(\sigma^{2}\right)^{-1}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\mu\sum_{i=1}^{n}y_{i}+n\mu^{2}\right)\right\} \\
 & =\left(2\pi\right)^{-n/2}\left(\sigma^{2}\right)^{-n/2-1}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}y_{i}^{2}-2\mu n\bar{y}+n\mu^{2}\right)\right\} \\
 & \propto\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n\mu^{2}-2\mu n\bar{y}+n\bar{y}^{2}-n\bar{y}^{2}+\sum_{i=1}^{n}y_{i}^{2}\right)\right\} \\
 & =\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[n\left(\mu^{2}-2\mu\bar{y}+\bar{y}^{2}\right)-n\bar{y}^{2}+\sum_{i=1}^{n}y_{i}^{2}\right]\right\} \\
 & =\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[n\left(\mu-\bar{y}\right)^{2}-n\bar{y}^{2}+\sum_{i=1}^{n}y_{i}^{2}\right]\right\} .
\end{flalign*}

\end_inset

It follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:computing-sums-rand-samples"

\end_inset

 that 
\begin_inset Formula 
\[
\sum_{i=1}^{n}y_{i}^{2}-n\bar{y}^{2}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2},
\]

\end_inset

and therefore that
\begin_inset Formula 
\begin{flalign*}
p\left(\mu,\sigma^{2}|\mathbf{y}\right) & \propto\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[n\left(\mu-\bar{y}\right)^{2}+\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}\right]\right\} \\
 & =\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)\frac{1}{n-1}\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} \\
 & =\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $s^{2}$
\end_inset

 is the sample variance.
 The conditional posterior distribution 
\begin_inset Formula $p\left(\mu|\sigma^{2},\mathbf{y}\right)$
\end_inset

 is equivalent to deriving the posterior for 
\begin_inset Formula $\mu$
\end_inset

 when 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:posterior-normal-known-variance"

\end_inset

 implies that 
\begin_inset Formula 
\[
\mu|\sigma^{2},\mathbf{y}\sim\mathcal{N}\left(\bar{y},\frac{\sigma^{2}}{n}\right).
\]

\end_inset

The marginal posterior distribution of 
\begin_inset Formula $\sigma^{2}$
\end_inset

, 
\begin_inset Formula $p\left(\sigma^{2}|\mathbf{y}\right)$
\end_inset

, is obtained by integrating 
\begin_inset Formula $p\left(\mu,\sigma^{2}|\mathbf{y}\right)$
\end_inset

 over 
\begin_inset Formula $\mu$
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
p\left(\sigma^{2}|\mathbf{y}\right) & =\int_{\mu}p\left(\mu,\sigma^{2}|\mathbf{y}\right)\dif\mu\\
 & \propto\int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-\left[\left(n+2\right)/2\right]}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}-\frac{1}{2\sigma^{2}}n\left(\mu-\bar{y}\right)^{2}\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-\left[1/2+\left(n+1\right)/2\right]}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right\} \exp\left\{ -\frac{1}{2\sigma^{2}}n\left(\mu-\bar{y}\right)^{2}\right\} \dif\mu\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right\} \int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-1/2}\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{1}{2\sigma^{2}}n\left(\mu-\bar{y}\right)^{2}\right\} \dif\mu\\
 & =\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(n-1\right)s^{2}\right\} \int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\mu\\
 & =\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \left(\frac{2\pi/n}{2\pi/n}\right)^{1/2}\int_{-\infty}^{\infty}\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\mu\\
 & =\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \left(2\pi/n\right)^{1/2}\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}/n\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\mu.
\end{flalign*}

\end_inset

We recognize the integrand as the density of a 
\begin_inset Formula $\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)$
\end_inset

 random variable, and it follows that
\begin_inset Formula 
\begin{flalign*}
p\left(\sigma^{2}|\mathbf{y}\right) & \propto\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \left(2\pi/n\right)^{1/2}\cdot1\\
 & \propto\left(\sigma^{2}\right)^{-\left(n+1\right)/2}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \\
 & =\left(\sigma^{2}\right)^{-\left(n+1+2-2\right)/2}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \\
 & =\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1\right]}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} ,
\end{flalign*}

\end_inset

which we recognize as the kernel of an inverse-gamma density with shape
 parameter 
\begin_inset Formula $\alpha=\left(n-1\right)/2$
\end_inset

 and scale parameter 
\begin_inset Formula $\beta=\left(n-1\right)s^{2}/2$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\sigma^{2}|\mathbf{y}\sim\text{Inv-Gamma}\left(\frac{n-1}{2},\frac{\left(n-1\right)}{2}s^{2}\right).
\]

\end_inset

Note that we can also obtain the marginal posterior density of 
\begin_inset Formula $\mu$
\end_inset

 through an application of conditional probability, i.e., 
\begin_inset Formula 
\begin{flalign*}
p\left(\mu|\sigma^{2},\mathbf{y}\right) & =\frac{p\left(\mu,\sigma^{2}|\mathbf{y}\right)}{p\left(\sigma^{2}|\mathbf{y}\right)}\\
 & \propto\frac{\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} }{\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1\right]}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} }\\
 & =\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\left(\sigma^{2}\right)^{\left[\left(n-1\right)/2+1\right]}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} \exp\left\{ \frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \\
 & =\left(\sigma^{2}\right)^{-n/2-1+\left(n-1\right)/2+1}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}-\frac{n\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}}+\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \\
 & =\left(\sigma^{2}\right)^{-n/2+n/2-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \\
 & =\left(\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} ,
\end{flalign*}

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)$
\end_inset

 random variable.
\end_layout

\begin_layout Subsection
Sampling from the joint posterior distribution
\end_layout

\begin_layout Standard
Consider the joint posterior density from the preceding example.
 One can simulate a value of 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 from the joint posterior density by
\end_layout

\begin_layout Enumerate
simulating 
\begin_inset Formula $\sigma^{2}$
\end_inset

 from an 
\begin_inset Formula $\text{Inv-Gamma}\left(\left(n-1\right)/2,\left(n-1\right)s^{2}/2\right)$
\end_inset

 distribution, which can be done by taking the inverse of a random sample
 from a 
\begin_inset Formula $\text{Gamma}\left(\left(n-1\right)/2,\left(n-1\right)s^{2}/2\right)$
\end_inset

 distribution (recall that if 
\begin_inset Formula $X\sim\text{Gamma}\left(\alpha,\beta\right)$
\end_inset

, then 
\begin_inset Formula $Y=1/X\sim\text{Inv-Gamma}\left(\alpha,\beta\right)$
\end_inset

);
\end_layout

\begin_layout Enumerate
then simulating 
\begin_inset Formula $\mu$
\end_inset

 from a 
\begin_inset Formula $\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)$
\end_inset

 distribution.
\end_layout

\begin_layout Example
\begin_inset CommandInset label
LatexCommand label
name "exa:nyc-marathon"

\end_inset

Suppose we are interested in learning about the distribution of completion
 times for men between ages 20 and 29 running the New York marathon.
 We observe the time 
\begin_inset Formula $y_{1},\ldots,y_{n}$
\end_inset

 in minutes of 
\begin_inset Formula $n=20$
\end_inset

 runners, which are saved under the data 
\family typewriter
marathontimes.txt
\family default
.
 We assume the 
\begin_inset Formula $y_{i}\text{'s}$
\end_inset

 represent a random sample from a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution.
 Inference about the parameters or functions of the parameters can be obtained
 from the simulated samples.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# code goes here
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Analytical form of 
\begin_inset Formula $p\left(\mu|y\right)$
\end_inset


\end_layout

\begin_layout Standard
The population mean 
\begin_inset Formula $\mu$
\end_inset

 is typically the parameter of interest.
 So, the objective of the Bayesian analysis is the marginal posterior distributi
on of 
\begin_inset Formula $\mu$
\end_inset

.
 For a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution, we have 
\begin_inset Formula $\sigma^{2}>0$
\end_inset

, so that 
\begin_inset Formula 
\[
p\left(\mu|\mathbf{y}\right)=\int_{0}^{\infty}p\left(\mu,\sigma^{2}|\mathbf{y}\right)\dif\sigma^{2}\propto\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} \dif\sigma^{2}.
\]

\end_inset

Let 
\begin_inset Formula 
\[
\alpha=\frac{n}{2}\quad\text{and}\quad\beta=\frac{\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}}{2},
\]

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
p\left(\mu|\mathbf{y}\right) & \propto\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left(\alpha+1\right)}\exp\left\{ -\beta/\sigma^{2}\right\} \dif\sigma^{2}\\
 & =\frac{\Gamma\left(\alpha\right)}{\beta^{\alpha}}\int_{0}^{\infty}\frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}\left(\sigma^{2}\right)^{-\left(\alpha+1\right)}\exp\left\{ -\beta/\sigma^{2}\right\} \dif\sigma^{2}.
\end{flalign*}

\end_inset

We recognize the integrand as the density of an 
\begin_inset Formula $\text{Inv-Gamma}\left(\alpha,\beta\right)$
\end_inset

 random variable, and it follows that
\begin_inset Formula 
\begin{flalign*}
p\left(\mu|\mathbf{y}\right) & \propto\frac{\Gamma\left(\alpha\right)}{\beta^{\alpha}}\cdot1\\
 & =\Gamma\left(\frac{n}{2}\right)\left[\frac{\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}}{2}\right]^{-n/2}\\
 & =\Gamma\left(\frac{n}{2}\right)2^{n/2}\left[\left(n-1\right)s^{2}+n\left(\mu-\bar{y}\right)^{2}\right]^{-n/2}\\
 & =\Gamma\left(\frac{n}{2}\right)2^{n/2}\left[\left(n-1\right)s^{2}\left(1+\frac{n\left(\mu-\bar{y}\right)^{2}}{\left(n-1\right)s^{2}}\right)\right]^{-n/2}\\
 & =\Gamma\left(\frac{n}{2}\right)2^{n/2}\left[\left(n-1\right)s^{2}\right]^{-n/2}\left[1+\frac{n\left(\mu-\bar{y}\right)^{2}}{\left(n-1\right)s^{2}}\right]^{-n/2}\\
 & =\Gamma\left(\frac{n}{2}\right)2^{n/2}\left[\left(n-1\right)s^{2}\right]^{-n/2}\left[1+\frac{\left(\mu-\bar{y}\right)^{2}}{\left(n-1\right)s^{2}/n}\right]^{-n/2}\\
 & =\Gamma\left(\frac{n}{2}\right)2^{n/2}\left[\left(n-1\right)s^{2}\right]^{-n/2}\left[1+\frac{1}{n-1}\left(\frac{\mu-\bar{y}}{s/\sqrt{n}}\right)^{2}\right]^{-n/2}\\
 & \propto\left[1+\frac{1}{n-1}\left(\frac{\mu-\bar{y}}{s/\sqrt{n}}\right)^{2}\right]^{-\left[\left(n-1\right)+1\right]/2},
\end{flalign*}

\end_inset

which we recognize as the kernel of a 
\begin_inset Formula $t$
\end_inset

-distribution with 
\begin_inset Formula $\nu=n-1$
\end_inset

 degrees of freedom, location parameter 
\begin_inset Formula $\bar{y}$
\end_inset

, and scale parameter 
\begin_inset Formula $\left(s/\sqrt{n}\right)^{2}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\mu|\mathbf{y}\sim t\left(n-1,\bar{y},\left(s/\sqrt{n}\right)^{2}\right).
\]

\end_inset

We can equivalently standardize 
\begin_inset Formula $\mu|\mathbf{y}$
\end_inset

, i.e., let 
\begin_inset Formula 
\[
Z=\frac{\mu-\bar{y}}{s/\sqrt{n}}.
\]

\end_inset

Then, 
\begin_inset Formula $Z$
\end_inset

 has the central 
\begin_inset Formula $t$
\end_inset

-distribution with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom.
 To generate 
\begin_inset Formula $\mu|\mathbf{y}\sim t\left(n-1,\bar{y},\left(s/\sqrt{n}\right)^{2}\right)$
\end_inset

 in 
\family sans
R
\family default
, we can
\end_layout

\begin_layout Enumerate
draw 
\begin_inset Formula $m$
\end_inset

 samples from 
\begin_inset Formula $t_{n-1}$
\end_inset

 using 
\family typewriter
rt(m, df = n - 1, ncp = 0)
\family default
,
\end_layout

\begin_layout Enumerate
then transform the samples as 
\begin_inset Formula $\mu=\bar{y}+t_{n-1}s/\sqrt{n}$
\end_inset

.
\end_layout

\begin_layout Standard
We implement this approach for the marathon times from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:nyc-marathon"

\end_inset

 below.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# code goes here
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Posterior predictive distribution
\end_layout

\begin_layout Standard
As above, let 
\begin_inset Formula $Y_{1},\ldots,Y_{n}$
\end_inset

 be iid samples from a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution.
 Then, the posterior predictive distribution for a future observation 
\begin_inset Formula $z$
\end_inset

 can be written as
\begin_inset Formula 
\[
p\left(z|\mathbf{y}\right)=\int_{\sigma^{2}}\int_{\mu}p\left(z|\mu,\sigma^{2}\right)p\left(\mu,\sigma^{2}|\mathbf{y}\right)\dif\mu\dif\sigma^{2},
\]

\end_inset

or can be found more easily using the factorization of the joint density
\begin_inset Formula 
\[
p\left(z|\mathbf{y}\right)=\int_{\sigma^{2}}\int_{\mu}p\left(z|\mu,\sigma^{2}\right)p\left(\mu|\sigma^{2},\mathbf{y}\right)p\left(\sigma^{2}|\mathbf{y}\right)\dif\mu\dif\sigma^{2}.
\]

\end_inset

As in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:posterior-normal-unknown-variance"

\end_inset

, we will consider the noninformative Jeffreys prior 
\begin_inset Formula $p\left(\mu,\sigma^{2}\right)\propto1/\sigma^{2}$
\end_inset

, so that the conditional posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

 and marginal posterior distribution of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 are 
\begin_inset Formula 
\[
\mu|\sigma^{2},\mathbf{y}\sim\mathcal{N}\left(\bar{y},\sigma^{2}/n\right)\quad\text{and}\quad\sigma^{2}|\mathbf{y}\sim\text{Inv-Gamma}\left(\frac{n-1}{2},\frac{\left(n-1\right)}{2}s^{2}\right),
\]

\end_inset

respectively.
 Then, noting that 
\begin_inset Formula $Z$
\end_inset

 is similarly drawn from a 
\begin_inset Formula $\mathcal{N}\left(\mu,\sigma^{2}\right)$
\end_inset

 distribution, we have 
\begin_inset Formula 
\begin{flalign*}
p\left(z|\mathbf{y}\right) & =\int_{\sigma^{2}}p\left(\sigma^{2}|\mathbf{y}\right)\int_{\mu}p\left(z|\mu,\sigma^{2}\right)p\left(\mu|\sigma^{2},\mathbf{y}\right)\dif\mu\dif\sigma^{2}\\
 & =\int_{\sigma^{2}}p\left(\sigma^{2}|\mathbf{y}\right)\int_{-\infty}^{\infty}\left[\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(z-\mu\right)^{2}}{2\sigma^{2}}\right\} \right]\left[\left(\frac{2\pi\sigma^{2}}{n}\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \right]\dif\mu\dif\sigma^{2}\\
 & =\int_{\sigma^{2}}\left(\frac{2\pi\sigma^{2}}{n}\right)^{-1/2}p\left(\sigma^{2}|\mathbf{y}\right)\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(z-\mu\right)^{2}}{2\sigma^{2}}-\frac{\left(\mu-\bar{y}\right)^{2}}{2\sigma^{2}/n}\right\} \dif\mu\dif\sigma^{2}.
\end{flalign*}

\end_inset

The inner integral becomes
\begin_inset Formula 
\begin{flalign*}
 & \quad\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\left(z-\mu\right)^{2}+n\left(\mu-\bar{y}\right)^{2}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}-2\mu z+\mu^{2}+n\left(\mu^{2}-2\mu\bar{y}+\bar{y}^{2}\right)\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}-2\mu z+\mu^{2}+n\mu^{2}-2\mu n\bar{y}+n\bar{y}^{2}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\mu^{2}\left(n+1\right)-2\mu\left(z+n\bar{y}\right)+z^{2}+n\bar{y}^{2}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\mu^{2}\left(n+1\right)-2\mu\left(z+n\bar{y}\right)+z^{2}+n\bar{y}^{2}+\frac{\left(z+n\bar{y}\right)^{2}}{n+1}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]-\frac{1}{2\sigma^{2}}\left[\mu^{2}\left(n+1\right)-2\mu\left(z+n\bar{y}\right)+\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]-\frac{n+1}{2\sigma^{2}}\left[\mu^{2}-\frac{2\mu\left(z+n\bar{y}\right)}{n+1}+\frac{\left(z+n\bar{y}\right)^{2}}{\left(n+1\right)^{2}}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]-\frac{n+1}{2\sigma^{2}}\left(\mu-\frac{z+n\bar{y}}{n+1}\right)^{2}\right\} \dif\mu\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \int_{-\infty}^{\infty}\left(2\pi\sigma^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}/\left(n+1\right)}\left(\mu-\frac{z+n\bar{y}}{n+1}\right)^{2}\right\} \dif\mu\\
 & =\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \left(n+1\right)^{-1/2}\\
 & \quad\times\int_{-\infty}^{\infty}\left(\frac{2\pi\sigma^{2}}{n+1}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}/\left(n+1\right)}\left(\mu-\frac{z+n\bar{y}}{n+1}\right)^{2}\right\} \dif\mu.
\end{flalign*}

\end_inset

We recognize the integrand as the density of a 
\begin_inset Formula 
\[
\mathcal{N}\left(\frac{z+n\bar{y}}{n+1},\frac{\sigma^{2}}{n+1}\right)
\]

\end_inset

random variable, and it follows that
\begin_inset Formula 
\begin{flalign*}
p\left(z|\mathbf{y}\right) & =\int_{\sigma^{2}}\left(\frac{2\pi\sigma^{2}}{n}\right)^{-1/2}p\left(\sigma^{2}|\mathbf{y}\right)\left[\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \left(n+1\right)^{-1/2}\cdot1\right]\dif\sigma^{2}\\
 & =\int_{0}^{\infty}\left(\frac{2\pi\sigma^{2}}{n}\right)^{-1/2}\left[\frac{\left[\frac{n-1}{2}s^{2}\right]^{\left(n-1\right)/2}}{\Gamma\left(\left(n-1\right)/2\right)}\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1\right]}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}\right\} \right]\\
 & \quad\times\exp\left\{ -\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \left(n+1\right)^{-1/2}\dif\sigma^{2}\\
 & =\left(2\pi\frac{n+1}{n}\right)^{-1/2}\frac{\left[\frac{n-1}{2}s^{2}\right]^{\left(n-1\right)/2}}{\Gamma\left(\left(n-1\right)/2\right)}\\
 & \quad\times\int_{0}^{\infty}\left(\sigma^{2}\right)^{-1/2}\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1\right]}\exp\left\{ -\frac{\left(n-1\right)s^{2}}{2\sigma^{2}}-\frac{1}{2\sigma^{2}}\left[z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right\} \dif\sigma^{2}\\
 & =\left(2\pi\right)^{-1/2}\left(1+\frac{1}{n}\right)^{-1/2}\frac{\left[\frac{n-1}{2}s^{2}\right]^{\left(n-1\right)/2}}{\Gamma\left(\left(n-1\right)/2\right)}\\
 & \quad\times\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1/2+1\right]}\exp\left\{ -\frac{1}{\sigma^{2}}\left(\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)\right\} \dif\sigma^{2}.
\end{flalign*}

\end_inset

The integral becomes
\begin_inset Formula 
\begin{flalign*}
 & \quad\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left[\left(n-1\right)/2+1/2+1\right]}\exp\left\{ -\frac{1}{\sigma^{2}}\left(\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)\right\} \dif\sigma^{2}\\
 & =\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{\sigma^{2}}\left(\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)\right\} \dif\sigma^{2}\\
 & =\int_{0}^{\infty}\left(\sigma^{2}\right)^{-\left(n/2+1\right)}\exp\left\{ -\frac{1}{\sigma^{2}}\left(\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)\right\} \dif\sigma^{2},
\end{flalign*}

\end_inset

which we recognize as the kernel of an 
\begin_inset Formula 
\[
\text{Inv-Gamma}\left(\frac{n}{2},\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)
\]

\end_inset

random variable, and it follows that
\begin_inset Formula 
\begin{flalign*}
p\left(z|\mathbf{y}\right) & =\left(2\pi\right)^{-1/2}\left(1+\frac{1}{n}\right)^{-1/2}\frac{\left[\frac{n-1}{2}s^{2}\right]^{\left(n-1\right)/2}}{\Gamma\left(\left(n-1\right)/2\right)}\Gamma\left(\frac{n}{2}\right)\left(\frac{1}{2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]\right)^{-n/2}\cdot1\\
 & =\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\pi}}2^{-1/2}\left(1+\frac{1}{n}\right)^{-1/2}2^{-\left(n-1\right)/2}\left[\left(n-1\right)s^{2}\right]^{\left(n-1\right)/2}\\
 & \quad\times2^{n/2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\pi}}2^{-1/2-\left(n-1\right)/2+n/2}\left(1+\frac{1}{n}\right)^{-1/2}\left[\left(n-1\right)s^{2}\right]^{-1/2}\\
 & \quad\times\left[\left(n-1\right)s^{2}\right]^{n/2}\left[\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\pi}\sqrt{1+\frac{1}{n}}\sqrt{\left(n-1\right)s^{2}}}\left[\frac{1}{\left(n-1\right)s^{2}}\left(\left(n-1\right)s^{2}+z^{2}+n\bar{y}^{2}-\frac{\left(z+n\bar{y}\right)^{2}}{n+1}\right)\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{\left(n-1\right)s^{2}}\left(\frac{z^{2}\left(n+1\right)+n\bar{y}^{2}\left(n+1\right)-\left(z+n\bar{y}\right)^{2}}{n+1}\right)\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{\left(n-1\right)s^{2}}\left(\frac{1}{n+1}\left(nz^{2}+z^{2}+n^{2}\bar{y}^{2}+n\bar{y}^{2}-\left(z+n\bar{y}\right)^{2}\right)\right)\right]^{-n/2}.
\end{flalign*}

\end_inset

The rightmost expression in parentheses becomes
\begin_inset Formula 
\begin{flalign*}
nz^{2}+z^{2}+n^{2}\bar{y}^{2}+n\bar{y}^{2}-\left(z+n\bar{y}\right)^{2} & =nz^{2}+z^{2}+n^{2}\bar{y}^{2}+n\bar{y}^{2}-\left(z^{2}+2nz\bar{y}+n^{2}\bar{y}^{2}\right)\\
 & =nz^{2}+n\bar{y}^{2}-2nz\bar{y}\\
 & =n\left(z^{2}-2z\bar{y}+\bar{y}^{2}\right)\\
 & =n\left(z-\bar{y}\right)^{2},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
p\left(z|\mathbf{y}\right) & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{\left(n-1\right)s^{2}}\left(\frac{n}{n+1}\left(z-\bar{y}\right)^{2}\right)\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{n-1}\frac{\left(z-\bar{y}\right)^{2}}{s^{2}\left(n+1\right)/n}\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{n-1}\frac{\left(z-\bar{y}\right)^{2}}{s^{2}\left(1+\frac{1}{n}\right)}\right]^{-n/2}\\
 & =\frac{\Gamma\left(\frac{\left(n-1\right)+1}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)\sqrt{\left(n-1\right)\pi}s\sqrt{1+\frac{1}{n}}}\left[1+\frac{1}{n-1}\left(\frac{z-\bar{y}}{s\sqrt{1+\frac{1}{n}}}\right)^{2}\right]^{-\left[\left(n-1\right)+1\right]/2},
\end{flalign*}

\end_inset

which we recognize as the density of a 
\begin_inset Formula $t$
\end_inset

-distribution with 
\begin_inset Formula $\nu=n-1$
\end_inset

 degrees of freedom, location parameter 
\begin_inset Formula $\bar{y}$
\end_inset

, and scale parameter 
\begin_inset Formula $\sigma^{2}=s^{2}\left(1+1/n\right)$
\end_inset

, i.e., the posterior predictive distribution of 
\begin_inset Formula $z$
\end_inset

 is 
\begin_inset Formula 
\[
z|\mathbf{y}\sim t\left(n-1,\bar{y},\left(1+\frac{1}{n}\right)s^{2}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
The discussion above shows that the conjugate prior density must have the
 product form 
\begin_inset Formula $p\left(\mu|\sigma^{2}\right)p\left(\sigma^{2}\right)$
\end_inset

.
 For normal data, we have
\begin_inset Formula 
\[
\mu|\sigma^{2}\sim\mathcal{N}\left(\mu_{0},\sigma^{2}/\kappa_{0}\right)\quad\text{and}\quad\sigma^{2}\sim\text{Inv-Gamma}\left(\frac{\nu_{0}}{2},\frac{\nu_{0}\sigma_{0}^{2}}{2}\right),
\]

\end_inset

or equivalently 
\begin_inset Formula $1/\sigma^{2}\sim\text{Gamma}\left(\nu_{0}/2,\nu_{0}\sigma_{0}^{2}/2\right)$
\end_inset

, which corresponds to the joint prior density
\begin_inset Formula 
\begin{flalign*}
p\left(\mu,\sigma^{2}\right) & \propto\left(\sigma^{2}/\kappa_{0}\right)^{-1/2}\exp\left\{ -\frac{1}{2\sigma^{2}/\kappa_{0}}\left(\mu-\mu_{0}\right)^{2}\right\} \cdot\left(\sigma^{2}\right)^{-\left(\nu_{0}/2+1\right)}\exp\left\{ -\frac{\sigma_{0}^{2}}{2\sigma^{2}}\right\} \\
 & =\left(\sigma^{2}\right)^{-\left[\left(\nu_{0}+1\right)/2+1\right]}\exp\left\{ -\frac{\kappa_{0}}{2\sigma^{2}}\left(\frac{\sigma_{0}^{2}}{\kappa_{0}}+\left(\mu-\mu_{0}\right)^{2}\right)\right\} .
\end{flalign*}

\end_inset

We refer to this distribution as the Normal-Inverse Gamma
\begin_inset Formula $\left(\mu_{0},\sigma_{0}^{2}/\kappa_{0};\nu_{0}/2,\sigma_{0}^{2}/2\right)$
\end_inset

 density.
 Note that 
\begin_inset Formula $\kappa_{0}$
\end_inset

 controls how informative the prior is relative to the data.
 The joint posterior distribution of 
\begin_inset Formula $\left(\mu,\sigma^{2}\right)$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
p\left(\mu,\sigma^{2}|\mathbf{y}\right) & \propto\left(\sigma^{2}\right)^{-\left[\left(\nu_{0}+1\right)/2+1\right]}\exp\left\{ -\frac{1}{2\sigma^{2}}\left(\sigma_{0}^{2}+\kappa_{0}\left(\mu-\mu_{0}\right)^{2}\right)\right\} \left(\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right\} \\
 & \propto\left(\sigma^{2}\right)^{-\left[\left(\nu_{n}+1\right)/2+1\right]}\exp\left\{ -\frac{\kappa_{n}}{2\sigma^{2}}\left(\frac{\sigma_{n}^{2}}{\kappa_{n}}+\left(\mu-\mu_{n}\right)^{2}\right)\right\} .
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula $\mu,\sigma^{2}|\mathbf{y}\sim\text{Normal-Inverse Gamma}\left(\mu_{n},\sigma_{n}^{2}/\kappa_{n};\nu_{n}/2,\sigma_{n}^{2}/2\right)$
\end_inset

, where
\begin_inset Formula 
\begin{flalign*}
\mu_{n} & =\frac{\kappa_{0}}{\kappa_{0}+n}\mu_{0}+\frac{n}{\kappa_{0}+n}\bar{y}\\
\kappa_{n} & =\kappa_{0}+n\\
\nu_{n} & =\nu_{0}+n\\
\nu_{n}\sigma_{n}^{2} & =\nu_{0}\sigma_{0}^{2}+\left(n-1\right)s^{2}+\frac{\kappa_{0}n}{\kappa_{0}+n}\left(\bar{y}-\mu_{0}\right)^{2}.
\end{flalign*}

\end_inset

Note that 
\begin_inset Formula $\mu_{n}$
\end_inset

 is a weighted average of the prior and sample means; 
\begin_inset Formula $\nu_{n}$
\end_inset

 combines the prior degrees of freedom and the sample size; and 
\begin_inset Formula $\nu_{n}\sigma_{n}^{2}$
\end_inset

 combines prior variation, observed variation, and variation between the
 sample mean and prior mean.
 After some algebra, it can be shown that the conditional posterior distribution
 of 
\begin_inset Formula $\mu$
\end_inset

 is
\begin_inset Formula 
\[
\mu|\sigma^{2},\mathbf{y}\sim\mathcal{N}\left(\mu_{n},\frac{\sigma^{2}}{\kappa_{n}}\right),
\]

\end_inset

the marginal posterior distribution of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is
\begin_inset Formula 
\[
\sigma^{2}|\mathbf{y}\sim\text{Inv-Gamma}\left(\frac{\nu_{n}}{2},\frac{\nu_{n}}{2}\sigma_{n}^{2}\right),
\]

\end_inset

the marginal posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

 is
\begin_inset Formula 
\[
\mu|\mathbf{y}\sim t\left(\nu_{n},\mu_{n},\frac{\sigma_{n}^{2}}{\kappa_{n}}\right),
\]

\end_inset

and the posterior predictive distribution for a future observation is
\begin_inset Formula 
\[
z|\mathbf{y}\sim t\left(\nu_{n},\mu_{n},\sigma_{n}^{2}\left[1+\frac{1}{\kappa_{n}}\right]\right).
\]

\end_inset


\end_layout

\begin_layout Example
Sunscreens are assigned Sun Protection Factor (SPF) values by the US Food
 and Drug Administration (FDA).
 SPF is a number that refers to the suncreen product's ability to block
 UVB radiation.
 A sunscreen product with a SPF of 15 will protect your skin 15 times longer
 from UVB than if you did not have sunscreen applied.
 For instance, an individual who can tolerate 
\begin_inset Formula $Y$
\end_inset

 minutes of sunlight without any sunscreen can tolerate 
\begin_inset Formula $15Y$
\end_inset

 minutes with an SPF 15 sunscreen.
 The exact amount of time will of course vary from person to person.
 Data on 13 individuals' sun tolerance with and without a particular suncreen
 have been collected (see data 
\family typewriter
spf.txt
\family default
).
 The goal is to determine the SPF value for this sunscreen.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-spf, fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos = 'h',
 fig.cap = 'Tolerance in minutes by experimental group'>>=
\end_layout

\begin_layout Plain Layout

spf <- read.table("data/spf.txt", header = T, sep = "
\backslash
t")
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

boxplot(spf, ylab = "Tolerance (min)", cex.lab = 0.75, cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
This is matched pairs data and the analysis should take into account the
 pairing, which induces dependence between observations.
 We can use pairwise differences or ratios to account for the pairing.
 In this case, evaluating ratios makes sense since the goal of the analysis
 is to determine how much longer a person can be exposed to the sun relative
 to his/her baseline.
 It is reasonable to assume that the log-ratios are normally distributed.
 So, we are modeling
\begin_inset Formula 
\[
y=\log\left(\frac{\text{sunscreen}}{\text{control}}\right)=\log\left(\text{sunscreen}\right)-\log\left(\text{control}\right)\sim\mathcal{N}\left(\mu,\sigma^{2}\right).
\]

\end_inset

We are interested in the SPF, which corresponds to 
\begin_inset Formula $\exp\left(\mu\right)$
\end_inset

.
 Let us take conjugate priors 
\begin_inset Formula 
\[
\mu|\sigma^{2}\sim\mathcal{N}\left(\mu_{0},\frac{\sigma^{2}}{\kappa_{0}}\right)\quad\text{and}\quad\sigma^{2}\sim\text{Inv-Gamma}\left(\frac{\nu_{0}}{2},\frac{\sigma_{0}^{2}}{2}\right)
\]

\end_inset

with 
\begin_inset Formula $\mu_{0}=0$
\end_inset

, 
\begin_inset Formula $\kappa_{0}=0.1$
\end_inset

, 
\begin_inset Formula $\nu_{0}=10$
\end_inset

, and 
\begin_inset Formula $\sigma_{0}^{2}=4$
\end_inset

.
 Then, the marginal posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

 is
\begin_inset Formula 
\[
\mu|\mathbf{y}\sim t\left(\nu_{n},\mu_{n},\frac{\sigma_{n}^{2}}{\kappa_{n}}\right).
\]

\end_inset

To draw samples of SPF from the posterior distribution, we will
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
draw 
\begin_inset Formula $\mu$
\end_inset

 from its posterior distribution:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
draw 
\begin_inset Formula $\sigma^{2}$
\end_inset

 from 
\begin_inset Formula $p\left(\sigma^{2}|\mathbf{y}\right)$
\end_inset

, 
\begin_inset Formula $\text{Inv-Gamma}\left(\nu_{n}/2,\nu_{n}\sigma_{n}^{2}/2\right)$
\end_inset

, then
\end_layout

\begin_layout Enumerate
draw 
\begin_inset Formula $\mu$
\end_inset

 from 
\begin_inset Formula $p\left(\mu|\sigma^{2},\mathbf{y}\right)$
\end_inset

, 
\begin_inset Formula $\mathcal{N}\left(\mu_{n},\sigma^{2}/\kappa_{n}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
(alternatively) draw 
\begin_inset Formula $\mu$
\end_inset

 directly from its marginal posterior density 
\begin_inset Formula $p\left(\mu|\mathbf{y}\right)$
\end_inset

, 
\begin_inset Formula $t\left(\nu_{n},\mu_{n},\sigma_{n}^{2}/\kappa_{n}\right)$
\end_inset

;
\end_layout

\end_deeper
\begin_layout Enumerate
use the transformation 
\begin_inset Formula $\exp\left(\mu\right)$
\end_inset

 to draw samples of SPF values.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos = 'h'>>=
\end_layout

\begin_layout Plain Layout

y <- log(spf$SUNSCREEN / spf$CONTROL)
\end_layout

\begin_layout Plain Layout

ybar <- mean(y)
\end_layout

\begin_layout Plain Layout

n <- length(y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# prior hyperparameters
\end_layout

\begin_layout Plain Layout

m0 <- 0
\end_layout

\begin_layout Plain Layout

k0 <- 0.1
\end_layout

\begin_layout Plain Layout

v0 <- 10
\end_layout

\begin_layout Plain Layout

s0 <- 4
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# posterior hyperparameters
\end_layout

\begin_layout Plain Layout

kn <- k0 + n
\end_layout

\begin_layout Plain Layout

mn <- (k0 * m0 + n * ybar) / kn
\end_layout

\begin_layout Plain Layout

vn <- v0 + n
\end_layout

\begin_layout Plain Layout

sn <- (v0 * s0 + (n - 1) * var(y) + (k0 * n / kn) * (ybar - m0)^2) / vn
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# draw samples
\end_layout

\begin_layout Plain Layout

nsamp <- 1e5
\end_layout

\begin_layout Plain Layout

phi <- rgamma(n = nsamp, shape = vn / 2, rate = sn * vn / 2)
\end_layout

\begin_layout Plain Layout

sigma2 <- 1 / phi
\end_layout

\begin_layout Plain Layout

mu <- rnorm(nsamp, mn, sqrt(sigma2 / kn))
\end_layout

\begin_layout Plain Layout

mu.t <- rt(nsamp, vn) * sqrt(sn / kn) + mn
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# compare the distributions of the posterior samples
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

qqplot(mu, mu.t)
\end_layout

\begin_layout Plain Layout

abline(a = 0, b = 1, col = "red")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can obtain the 95% credible interval for the SPF:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# code goes here
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
or the 95% HPD interval.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can obtain samples from the posterior predictive distribution.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Normal data with semi-conjugate prior distribution
\end_layout

\begin_layout Standard
The prior distributions for 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 may be specified independently, i.e., 
\begin_inset Formula 
\[
\mu|\sigma^{2}\sim\mathcal{N}\left(\mu_{0},\tau_{0}^{2}\right)\quad\text{and}\quad\sigma^{2}\sim\text{Inv-Gamma}\left(\frac{\nu_{0}}{2},\frac{\sigma_{0}^{2}}{2}\right),
\]

\end_inset

where we see that 
\begin_inset Formula $\mu$
\end_inset

 is independent of 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 The joint prior distribution is not a conjugate family for the normal likelihoo
d.
 The posterior density (which can be shown to be proper) does not follow
 any standard parametric form, but we can obtain posterior samples by considerin
g 
\begin_inset Formula $p\left(\mu|\sigma^{2},\mathbf{y}\right)$
\end_inset

 and 
\begin_inset Formula $p\left(\sigma^{2}|\mathbf{y}\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Multivariate normal model
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\mathbf{Y}=\left(Y_{1},\ldots,Y_{p}\right)$
\end_inset

, let 
\begin_inset Formula $\boldsymbol{\mu}=\left(\mu_{1},\ldots,\mu_{p}\right)$
\end_inset

, and let 
\begin_inset Formula $\boldsymbol{\Sigma}$
\end_inset

 be a 
\begin_inset Formula $p\times p$
\end_inset

 symmetric and positive definite matrix, where
\begin_inset Formula 
\[
\mathbf{Y}|\boldsymbol{\mu},\boldsymbol{\Sigma}\sim\mathcal{N}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right).
\]

\end_inset

The sampling model for a single observation 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is given by 
\begin_inset Formula 
\[
p\left(\mathbf{y}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)=\left(2\pi\right)^{-p/2}\left|\boldsymbol{\Sigma}\right|^{-1/2}\exp\left\{ -\frac{1}{2}\left(\mathbf{y}-\boldsymbol{\mu}\right)^{\mathsf{T}}\boldsymbol{\Sigma}^{-1}\left(\mathbf{y}-\boldsymbol{\mu}\right)\right\} .
\]

\end_inset


\end_layout

\begin_layout Standard
For a sample of 
\begin_inset Formula $n$
\end_inset

 iid observations 
\begin_inset Formula $\mathbf{y}_{1},\ldots,\mathbf{y}_{n}$
\end_inset

, 
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{y}_{1},\ldots,\mathbf{y}_{n}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right) & =\left(2\pi\right)^{-np/2}\left|\boldsymbol{\Sigma}\right|^{-n/2}\exp\left\{ -\frac{1}{2}\sum_{i=1}^{n}\left(\mathbf{y}_{i}-\boldsymbol{\mu}\right)^{\mathsf{T}}\boldsymbol{\Sigma}^{-1}\left(\mathbf{y}_{i}-\boldsymbol{\mu}\right)\right\} \\
 & =\left(2\pi\right)^{-np/2}\left|\boldsymbol{\Sigma}\right|^{-n/2}\exp\left\{ -\frac{1}{2}\tr\left(\boldsymbol{\Sigma}^{-1}\sum_{i=1}^{n}\left(\mathbf{y}_{i}-\boldsymbol{\mu}\right)\left(\mathbf{y}_{i}-\boldsymbol{\mu}\right)^{\mathsf{T}}\right)\right\} ,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\tr\left(\mathbf{A}\right)$
\end_inset

 is the trace of a matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and given by the sum of its diagonal terms.
 Note that 
\begin_inset Formula $\boldsymbol{\Sigma}$
\end_inset

 is the covariance matrix with entries given by
\begin_inset Formula 
\[
\boldsymbol{\Sigma}=\begin{bmatrix}\Var\left(y_{i1}\right) & \cdots & \Cov\left(y_{i1},y_{ik}\right)\\
\ddots & \ddots & \ddots\\
\Cov\left(y_{ik},y_{ki}\right) &  & \Var\left(y_{ip}\right)
\end{bmatrix}.
\]

\end_inset

The inverse-Wishart, a multivariate generalization of the inverse-Gamma,
 can be used to describe the prior distribution of the matrix 
\begin_inset Formula $\boldsymbol{\Sigma}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\boldsymbol{\mu}|\boldsymbol{\Sigma}\sim\mathcal{N}\left(\boldsymbol{\mu}_{0},\kappa_{0}^{-1}\boldsymbol{\Sigma}\right)\quad\text{and}\quad\boldsymbol{\Sigma}\sim\text{Inv-Wishart}\left(\boldsymbol{\Omega}_{0}^{-1},\nu_{0}\right).
\]

\end_inset

The inverse-Wishart density is given by
\begin_inset Formula 
\[
p\left(\boldsymbol{\Sigma}\right)=\left[2^{p\nu_{0}/2}\pi^{p\left(p-1\right)/4}\prod_{i=1}^{p}\Gamma\left(\frac{\nu_{0}+1-i}{2}\right)\right]^{-1}\left|\boldsymbol{\Omega}_{0}\right|^{\nu_{0}/2}\left|\boldsymbol{\Sigma}\right|^{-\left(\nu_{0}+p+1\right)/2}\exp\left\{ -\frac{1}{2}\tr\left(\boldsymbol{\Omega}_{0}\boldsymbol{\Sigma}^{-1}\right)\right\} .
\]

\end_inset

The joint prior density is
\begin_inset Formula 
\[
p\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\propto\left|\boldsymbol{\Sigma}\right|^{-\left[\left(\nu_{0}+p\right)/2+1\right]}\exp\left\{ -\frac{1}{2}\tr\left(\boldsymbol{\Omega}_{0}\boldsymbol{\Sigma}^{-1}\right)-\frac{\kappa_{0}}{2}\left(\boldsymbol{\mu}-\boldsymbol{\mu}_{0}\right)^{\mathsf{T}}\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}-\boldsymbol{\mu}_{0}\right)\right\} .
\]

\end_inset

The results from the univariate normal distribution generalize to the multivaria
te case.
 Multiplying the joint prior density by the normal likelihood results in
 a posterior density of the same family with parameters
\begin_inset Formula 
\begin{flalign*}
\boldsymbol{\mu}_{n} & =\frac{\kappa_{0}}{\kappa_{0}+n}\boldsymbol{\mu}_{0}+\frac{n}{\kappa_{0}+n}\bar{\mathbf{y}}\\
\kappa_{n} & =\kappa_{0}+n\\
\nu_{n} & =\nu_{0}+n\\
\boldsymbol{\Omega}_{n} & =\boldsymbol{\Omega}_{0}+\sum_{i=1}^{n}\left(\mathbf{y}_{i}-\bar{\mathbf{y}}\right)\left(\mathbf{y}_{i}-\bar{\mathbf{y}}\right)^{\mathsf{T}}+\frac{\kappa_{0}}{\kappa_{0}+n}\left(\bar{\mathbf{y}}-\boldsymbol{\mu}_{0}\right)\left(\bar{\mathbf{y}}-\boldsymbol{\mu}_{0}\right)^{\mathsf{T}}.
\end{flalign*}

\end_inset

The marginal posterior distribution of 
\begin_inset Formula $\boldsymbol{\mu}$
\end_inset

 is multivariate 
\begin_inset Formula $t\left(\nu_{n}-p+1,\boldsymbol{\mu}_{n},\left[\kappa_{n}\left(\nu_{n}-p+1\right)\right]^{-1}\boldsymbol{\Omega}_{n}\right)$
\end_inset

.
 Samples from the joint posterior distribution 
\begin_inset Formula $\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)$
\end_inset

 are easily obtained by
\end_layout

\begin_layout Enumerate
drawing 
\begin_inset Formula $\boldsymbol{\Sigma}|\mathbf{y}\sim\text{Inv-Wishart}\left(\boldsymbol{\Omega}_{n}^{-1},\nu_{n}\right)$
\end_inset

,
\end_layout

\begin_layout Enumerate
then drawing 
\begin_inset Formula $\boldsymbol{\mu}|\boldsymbol{\Sigma},\mathbf{y}\sim\mathcal{N}\left(\boldsymbol{\mu}_{n},\kappa_{n}^{-1}\boldsymbol{\Sigma}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
A commonly proposed noninformative prior distribution is the multivariate
 Jeffreys prior density 
\begin_inset Formula 
\[
p\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\propto\left|\boldsymbol{\Sigma}\right|^{-\left(p+1\right)/2},
\]

\end_inset

which is the limit of the conjugate prior density as 
\begin_inset Formula $\kappa_{0}\rightarrow0$
\end_inset

, 
\begin_inset Formula $\nu_{0}\rightarrow-1$
\end_inset

, and 
\begin_inset Formula $\left|\boldsymbol{\Omega}_{0}\right|\rightarrow0$
\end_inset

.
 Results for the posterior distributions follow from the univariate discussions,
 assuming that the posterior distribution is proper.
 It is especially important to check that the posterior distribution is
 proper when using noninformative prior distributions in high dimensions.
\end_layout

\begin_layout Subsection
Multinomial model
\end_layout

\begin_layout Standard
The binomial distribution can be generalized to allow more than two possible
 outcomes.
 The multinomial sampling distribution is used to describe data for which
 each observation can take one of 
\begin_inset Formula $k$
\end_inset

 possible values.
 If 
\begin_inset Formula $\mathbf{y}=\left(y_{1},\ldots,y_{k}\right)$
\end_inset

 is the vector of counts of the number of observations in each of the 
\begin_inset Formula $k$
\end_inset

 categories for a sample of size 
\begin_inset Formula $n$
\end_inset

, then 
\begin_inset Formula $\mathbf{y}|\boldsymbol{\theta}\sim\text{Multinomial}\left(n;\theta_{1},\ldots,\theta_{k}\right)$
\end_inset

 with density given by
\begin_inset Formula 
\[
p\left(\mathbf{y}|\boldsymbol{\theta}\right)=\frac{n!}{\prod_{j=1}^{k}y_{j}!}\prod_{j=1}^{k}\theta_{j}^{y_{j}}=\frac{n!}{y_{1}!y_{2}!\cdots y_{k}!}\theta_{1}^{y_{1}}\theta_{2}^{y_{2}}\cdots\theta_{k}^{y_{k}}
\]

\end_inset

for 
\begin_inset Formula $y_{j}=0,\ldots,n$
\end_inset

 and 
\begin_inset Formula $\sum_{j=1}^{k}y_{j}=n$
\end_inset

; 
\begin_inset Formula $0\leq\theta_{j}\leq1$
\end_inset

 and 
\begin_inset Formula $\sum_{j}\theta_{j}=1$
\end_inset

.
 Let 
\begin_inset Formula $Z_{i}$
\end_inset

 be the 
\begin_inset Formula $i\text{th}$
\end_inset

 outcome, so that 
\begin_inset Formula $Z_{i}\in\left\{ 1,2,\ldots,k\right\} $
\end_inset

.
 Then, we can express 
\begin_inset Formula $Y_{k}$
\end_inset

 as 
\begin_inset Formula 
\[
Y_{k}=\sum_{i=1}^{n}I_{\left\{ Z_{i}=k\right\} }.
\]

\end_inset

The conjugate prior distribution is a multivariate generalization of the
 beta distribution known as the Dirichlet, which has density given by
\begin_inset Formula 
\[
p\left(\boldsymbol{\theta}\right)=\frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{k}\right)}{\Gamma\left(\alpha_{1}\right)\cdots\Gamma\left(\alpha_{k}\right)}\prod_{j=1}^{k}\theta_{j}^{\alpha_{j}-1}.
\]

\end_inset

The resulting posterior distribution for the 
\begin_inset Formula $\theta_{j}\text{'s}$
\end_inset

 is Dirichlet with parameters 
\begin_inset Formula $\alpha_{j}+y_{j}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\boldsymbol{\theta}|\mathbf{y}\sim\text{Dirichlet}\left(\alpha_{1}+y_{1},\ldots,\alpha_{k}+y_{k}\right),
\]

\end_inset

whose density we can derive as
\begin_inset Formula 
\begin{flalign*}
p\left(\boldsymbol{\theta}|\mathbf{y}\right) & \propto p\left(\mathbf{y}|\boldsymbol{\theta}\right)p\left(\boldsymbol{\theta}\right)\\
 & =\left[\frac{n!}{\prod_{j=1}^{k}y_{j}!}\prod_{j=1}^{k}\theta_{j}^{y_{j}}\right]\left[\frac{\Gamma\left(\sum_{j=1}^{k}\alpha_{j}\right)}{\prod_{j=1}^{k}\Gamma\left(\alpha_{j}\right)}\prod_{j=1}^{k}\theta_{j}^{\alpha_{j}-1}\right]\\
 & \propto\left(\prod_{j=1}^{k}\theta_{j}^{y_{j}}\right)\left(\prod_{j=1}^{k}\theta_{j}^{\alpha_{j}-1}\right)\\
 & =\prod_{j=1}^{k}\theta_{j}^{\alpha_{j}+y_{j}-1}.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Example
A survey of 1447 adults is conducted to determine preferences in an election:
 
\begin_inset Formula $y_{1}=727$
\end_inset

 supported candidate A, 
\begin_inset Formula $y_{2}=583$
\end_inset

 supported candidate B, and 
\begin_inset Formula $y_{3}=137$
\end_inset

 supported other candidates or expressed no opinion.
 If we assume random sampling, then the data 
\begin_inset Formula $\left(y_{1},y_{2},y_{3}\right)$
\end_inset

 follow a multinomial distribution with parameters 
\begin_inset Formula $\left(\theta_{1},\theta_{2},\theta_{3}\right)$
\end_inset

.
 A parameter of interest is 
\begin_inset Formula $\theta_{1}-\theta_{2}$
\end_inset

, the population difference in support of the two major candidates.
 With a noninformative uniform prior distribution on 
\begin_inset Formula $\theta$
\end_inset

, 
\begin_inset Formula $\alpha_{1}=\alpha_{2}=\alpha_{3}=1$
\end_inset

, the posterior distribution for 
\begin_inset Formula $\left(\theta_{1},\theta_{2},\theta_{3}\right)$
\end_inset

 is 
\begin_inset Formula $\boldsymbol{\theta}|\mathbf{y}\sim\text{Dirichlet}\left(728,584,138\right)$
\end_inset

.
 We could compute the posterior distribution of 
\begin_inset Formula $\theta_{1}-\theta_{2}$
\end_inset

.
 It is simpler to draw 
\begin_inset Formula $\left(\theta_{1},\theta_{2},\theta_{3}\right)$
\end_inset

 from the posterior Dirichlet distribution and compute 
\begin_inset Formula $\theta_{1}-\theta_{2}$
\end_inset

 for each drawn sample.
 You can use the function 
\family typewriter
rdirichlet
\family default
 in the package 
\family typewriter
\series bold
MCMCpack
\family default
\series default
 to sample from a Dirichlet distribution.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<message = F, fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos
 = 'h'>>=
\end_layout

\begin_layout Plain Layout

library(MCMCpack)
\end_layout

\begin_layout Plain Layout

set.seed(123)
\end_layout

\begin_layout Plain Layout

thetas <- rdirichlet(1e5, c(728,584,138))
\end_layout

\begin_layout Plain Layout

thdiff <- thetas[,1] - thetas[,2]
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,3,0.5,2))
\end_layout

\begin_layout Plain Layout

hist(thdiff, breaks = 50, xlab = expression(theta[1] - theta[2]),
\end_layout

\begin_layout Plain Layout

  prob = T, main = "", cex.lab = 0.75, cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

lines(density(thdiff), lwd = 2, col = "red")
\end_layout

\begin_layout Plain Layout

mean(thdiff > 0)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
The estimated posterior probability that candidate A has more support than
 candidate B in the survey population is 
\begin_inset Formula $P\left(\theta_{1}>\theta_{2}\right)=P\left(\theta_{1}-\theta_{2}>0\right)=99.99\%$
\end_inset

.
\end_layout

\begin_layout Chapter
Hypothesis testing and Bayes factor
\end_layout

\begin_layout Standard
The 
\shape italic
Bayes factor
\shape default
 is used to test hypotheses and compare models in the Bayesian framework.
 Suppose we have two candidate models, 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

, with respective parameter vectors 
\begin_inset Formula $\boldsymbol{\theta}_{1}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}_{2}$
\end_inset

.
 The Bayes factor is the ratio of the posterior odds of 
\begin_inset Formula $H_{1}$
\end_inset

 to the prior odds of 
\begin_inset Formula $H_{1}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(H_{1}|\mathbf{y}\right)/p\left(H_{2}|\mathbf{y}\right)}{p\left(H_{1}\right)/p\left(H_{2}\right)}.
\]

\end_inset

The marginal distribution of 
\begin_inset Formula $Y$
\end_inset

 under each model 
\begin_inset Formula $H_{i}$
\end_inset

 is
\begin_inset Formula 
\[
p\left(y|H_{i}\right)=\int p\left(y|\boldsymbol{\theta}_{i},H_{i}\right)\pi_{i}\left(\boldsymbol{\theta}_{i}\right)\dif\boldsymbol{\theta}_{i},\quad i\in\left\{ 1,2\right\} .
\]

\end_inset

The Bayes factor can also be written as the ratio of the observed marginal
 densities for the two models, i.e., 
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(H_{1}|\mathbf{y}\right)/p\left(H_{2}|\mathbf{y}\right)}{p\left(H_{1}\right)/p\left(H_{2}\right)}=\frac{\left[\frac{p\left(\mathbf{y}|H_{1}\right)p\left(H_{1}\right)}{p\left(\mathbf{y}\right)}\right]/\left[\frac{p\left(\mathbf{y}|H_{2}\right)p\left(H_{2}\right)}{p\left(\mathbf{y}\right)}\right]}{p\left(H_{1}\right)/p\left(H_{2}\right)}=\frac{p\left(\mathbf{y}|H_{1}\right)}{p\left(\mathbf{y}|H_{2}\right)}.
\]

\end_inset

The Bayes factor is only defined when the marginal density of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 under each model is proper.
 If 
\begin_inset Formula $\pi_{i}\left(\boldsymbol{\theta}_{i}\right)$
\end_inset

 is improper, then 
\begin_inset Formula $p\left(y|H_{i}\right)$
\end_inset

 will necessarily be improper, and the Bayes factor is not defined.
 The following interpretation of the Bayes factor was proposed by Jeffreys.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\log_{10}\text{BF}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bayes factor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Interpretation
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0-0.5$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1\leq\text{BF}\leq3.2$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
weak
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0.5-1.0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3.2<\text{BF}\leq10$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
substantial
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1.0-2.0$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10<\text{BF}\leq100$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
strong
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $>2$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{BF}>100$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
decisive
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Jeffreys' scale of evidence in favor of 
\begin_inset Formula $H_{1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The following alternative interpretation was proposed by Kass and Raftery.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2\ln\text{BF}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bayes factor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Interpretation
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0-2$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1\leq\text{BF}\leq3$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
weak
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2-6$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3<\text{BF}\leq20$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
positive
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $6-10$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $20<\text{BF}\leq150$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
strong
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $>10$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{BF}>150$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
very strong
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kass and Raftery scale of evidence in favor of 
\begin_inset Formula $H_{1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Comparison to frequentist hypothesis testing
\end_layout

\begin_layout Standard
Recall from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "chap:hypothesis-testing"

\end_inset

 that, in classical hypothesis testing, we proceed as follows:
\end_layout

\begin_layout Enumerate
state a null hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

 and an alternative hypothesis 
\begin_inset Formula $H_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
determine an appropriate test statistic 
\begin_inset Formula $T\left(\mathbf{Y}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
compute the 
\begin_inset Formula $p\text{-value}$
\end_inset

 of the test as
\begin_inset Formula 
\[
p\text{-value}=P\left(T\left(\mathbf{Y}\right)\text{ more "extreme" than }T\left(\mathbf{y}_{\text{obs}}\right)|\boldsymbol{\theta},H_{0}\right)
\]

\end_inset

where 
\begin_inset Quotes eld
\end_inset

extremeness
\begin_inset Quotes erd
\end_inset

 is in the direction of 
\begin_inset Formula $H_{1}$
\end_inset

;
\end_layout

\begin_layout Enumerate
if the 
\begin_inset Formula $p\text{-value}$
\end_inset

 is less than the pre-specified Type I error rate 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $H_{0}$
\end_inset

 is rejected.
\end_layout

\begin_layout Standard
Classical hypothesis testing is straightforward only when the two hypotheses
 are nested.
 In Bayesian hypothesis testing, we proceed as follows:
\end_layout

\begin_layout Enumerate
state the two hypotheses 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

;
\end_layout

\begin_layout Enumerate
assign priors to 
\begin_inset Formula $H_{1}$
\end_inset

 and 
\begin_inset Formula $H_{2}$
\end_inset

, and specific 
\begin_inset Formula $p\left(\boldsymbol{\theta}|H_{1}\right)$
\end_inset

 and 
\begin_inset Formula $p\left(\boldsymbol{\theta}|H_{2}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
evaluate 
\begin_inset Formula $p\left(H_{1}|\mathbf{y}\right)$
\end_inset

 and 
\begin_inset Formula $p\left(H_{2}|\mathbf{y}\right)$
\end_inset

 via Bayes' theorem;
\end_layout

\begin_layout Enumerate
compute the Bayes factor to assess the evidence in favor of 
\begin_inset Formula $H_{1}$
\end_inset

:
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(\mathbf{y}|H_{1}\right)}{p\left(\mathbf{y}|H_{2}\right)}.
\]

\end_inset


\end_layout

\begin_layout Standard
The Bayesian framework does not require the two models to be nested.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Test of proportion]
\end_layout

\end_inset

Suppose 16 customers have been recruited by a fast-food chain to compare
 two types of ground beef patty on the basis of flavor.
 All of the patties to be evaluated have been kept frozen for eight months.
 One set of 16 has been stored in a high-quality freezer that maintains
 a temperature that is consistently within 
\begin_inset Formula $\pm1$
\end_inset

°F.
 The other set of 16 has been stored in a freezer with temperature that
 varies anywhere between 0 and 
\begin_inset Formula $15$
\end_inset

°F.
 The food chain executives are interested in whether storage in the higher-quali
ty freezer translates into a substantial improvement in taste, thus justifying
 the extra effort and cost associated with equipping all of their stores
 with these freezers.
 Suppose that to be regarded as 
\begin_inset Quotes eld
\end_inset

substantial
\begin_inset Quotes erd
\end_inset

 improvement, more than 60% of consumers must prefer the more expensive
 option.
 13 of the 16 consumers state a preference for the more expensive patty.
 Let 
\begin_inset Formula $Y_{i}=1$
\end_inset

 if consumer 
\begin_inset Formula $i$
\end_inset

 states a preference for the more expensive patty and 
\begin_inset Formula $Y_{i}=0$
\end_inset

 otherwise, i.e., 
\begin_inset Formula $Y_{i}\sim\text{Bernoulli}\left(\theta\right)$
\end_inset

, so that 
\begin_inset Formula 
\[
X=\sum_{i=1}^{16}Y_{i}\sim\text{Binomial}\left(16,\theta\right)
\]

\end_inset

is the total number of consumers who state a preference for the more expensive
 patty.
 We want to test 
\begin_inset Formula 
\[
H_{1}:\theta>0.6\quad\text{vs}\quad H_{2}\leq0.6.
\]

\end_inset

(Observe that this test cannot be done in a frequentist setting, where a
 single value must be specified for the null hypothesis.) The executives
 may believe that the choice could go either way, i.e., 
\begin_inset Formula $p\left(H_{1}\right)=p\left(H_{2}\right)=0.5$
\end_inset

.
 Suppose we consider 
\begin_inset Quotes eld
\end_inset

minimally informative
\begin_inset Quotes erd
\end_inset

 priors 
\begin_inset Formula $\pi\left(\theta\right)$
\end_inset

: Jeffreys prior, 
\begin_inset Formula $\text{Beta}\left(0.5,0.5\right)$
\end_inset

; a prior that we think of as 
\begin_inset Quotes eld
\end_inset

noninformative,
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\text{Beta}\left(1,1\right)$
\end_inset

; and 
\begin_inset Formula $\text{Beta}\left(2,2\right)$
\end_inset

.
 Example 
\begin_inset CommandInset ref
LatexCommand ref
reference "exa:infectious-disease"

\end_inset

 implies that the posterior distribution is
\begin_inset Formula 
\[
\theta|x\sim\text{Beta}\left(\alpha+x,\beta+n-x\right).
\]

\end_inset

The Bayes factor in favor of 
\begin_inset Formula $H_{1}$
\end_inset

 is
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(H_{1}|x\right)/p\left(H_{2}|x\right)}{p\left(H_{1}\right)/p\left(H_{2}\right)}.
\]

\end_inset

We can calculate the Bayes factors as shown below.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

n <- 16
\end_layout

\begin_layout Plain Layout

x <- 13
\end_layout

\begin_layout Plain Layout

# for Beta(0.5, 0.5) prior
\end_layout

\begin_layout Plain Layout

a <- 0.5
\end_layout

\begin_layout Plain Layout

b <- 0.5
\end_layout

\begin_layout Plain Layout

post.ratio <- (1 - pbeta(0.6, a + x, b + n - x)) / pbeta(0.6, a + x, b + n
 - x)
\end_layout

\begin_layout Plain Layout

prior.ratio <- (1 - pbeta(0.6, a, b)) / pbeta(0.6, a, b)
\end_layout

\begin_layout Plain Layout

BF <- post.ratio / prior.ratio
\end_layout

\begin_layout Plain Layout

BF
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
Thus, under the 
\begin_inset Formula $\text{Beta}\left(0.5,0.5\right)$
\end_inset

 prior, the Bayes factor is
\begin_inset Formula 
\[
\text{BF}=\frac{P\left(\theta>0.6|x\right)/P\left(\theta\leq0.6|x\right)}{P\left(\theta>0.6\right)/P\left(\theta\leq0.6\right)}\approx\Sexpr{BF}.
\]

\end_inset

Since 
\begin_inset Formula $\log_{10}\Sexpr{BF}\approx\Sexpr{log10(BF)}$
\end_inset

 (or 
\begin_inset Formula $2\ln\Sexpr{BF}\approx\Sexpr{2*log(BF)}$
\end_inset

, there is strong evidence in favor of 
\begin_inset Formula $H_{1}$
\end_inset

.
 The prior distributions are shown with the resulting posterior quantiles
 and Bayes factors.
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Posterior quantile
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Prior
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.025
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.975
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $p\left(\theta>0.6|x\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bayes factor
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Beta}\left(0.5,0.5\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.579
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.806
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.944
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.964
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
34.432
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Beta}\left(1,1\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.566
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.788
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.932
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.954
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30.812
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\text{Beta}\left(2,2\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.544
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.758
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.909
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.930
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24.604
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Posterior quantiles and Bayes factors for selected prior distributions
\end_layout

\end_inset


\end_layout

\end_inset

We see that, under either the Jeffreys or Kass and Raftery interpretation,
 there is strong evidence in favor of 
\begin_inset Formula $H_{1}:\theta>0.6$
\end_inset

.
 Alternatively, we can calculate the Bayes factor as
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(x|H_{1}\right)}{p\left(x|H_{2}\right)},\quad\text{where}\quad p\left(x|H_{j}\right)=\int_{\theta}p\left(x|\theta,H_{j}\right)p\left(\theta|H_{j}\right)\dif\theta.
\]

\end_inset

We have 
\begin_inset Formula 
\begin{flalign*}
p\left(\theta|H_{1}\right) & =\frac{p\left(\theta,H_{1}\right)}{p\left(H_{1}\right)}\\
 & =\frac{\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{\int_{0.6}^{1}\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)\Gamma\left(\beta\right)}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}\\
 & =\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{\int_{0.6}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta},\quad\theta>0.6,
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
p\left(x|H_{1}\right) & =\int_{\theta}\binom{n}{x}\theta^{x}\left(1-\theta\right)^{n-x}\frac{\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}}{\int_{0.6}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}\dif\theta\\
 & =\binom{n}{x}\frac{\int_{0.6}^{1}\theta^{\alpha+x-1}\left(1-\theta\right)^{\beta+n-x-1}\dif\theta}{\int_{0.6}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}.
\end{flalign*}

\end_inset

Similarly, we have
\begin_inset Formula 
\[
p\left(\theta|H_{2}\right)=\frac{\theta^{\alpha-1}\left(1-\beta\right)^{\beta-1}}{\int_{0}^{0.6}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta},\quad\theta\leq0.6,
\]

\end_inset

so that 
\begin_inset Formula 
\[
p\left(x|H_{2}\right)=\binom{n}{x}\frac{\int_{0}^{0.6}\theta^{\alpha+x-1}\left(1-\theta\right)^{\beta+n-x-1}\dif\theta}{\int_{0}^{0.6}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}.
\]

\end_inset

Then, the Bayes factor is
\begin_inset Formula 
\[
\text{BF}=\frac{p\left(x|H_{1}\right)}{p\left(x|H_{2}\right)}=\frac{\int_{0.6}^{1}\theta^{\alpha+x-1}\left(1-\theta\right)^{\beta+n-x-1}\dif\theta}{\int_{0}^{0.6}\theta^{\alpha+x-1}\left(1-\theta\right)^{\beta+n-x-1}\dif\theta}\times\left[\frac{\int_{0.6}^{1}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}{\int_{0}^{0.6}\theta^{\alpha-1}\left(1-\theta\right)^{\beta-1}\dif\theta}\right]^{-1},
\]

\end_inset

i.e., the Bayes factor is the product of the ratio of posteriors and the ratio
 of priors.
 Let us analyze these data using frequentist approaches.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

prop.test(13, 16, p = 0.6, alternative = "greater")
\end_layout

\begin_layout Plain Layout

binom.test(13, 16, p = 0.6, alternative = "greater")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
We see that we would not reject the null hypothesis at 
\begin_inset Formula $\alpha=0.05$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

[Two-sided test of normal mean]
\end_layout

\end_inset

John weighed 170 pounds last year and he is wondering if he still weighs
 the same.
 For simplicity, assume he knows the accuracy of the scale and 
\begin_inset Formula $\sigma=3$
\end_inset

 pounds.
 We wish to test 
\begin_inset Formula 
\[
H_{1}:\mu\neq170\quad\text{vs}\quad H_{2}:\mu=170.
\]

\end_inset

He weighs himself 10 times and obtains the following measurements 
\begin_inset Formula $y_{i}\sim\mathcal{N}\left(\mu_{0},\sigma^{2}\right)$
\end_inset

.
\begin_inset Formula 
\[
\begin{array}{cccccccccc}
182 & 172 & 173 & 176 & 176 & 180 & 173 & 174 & 179 & 175\end{array}
\]

\end_inset

John seems to believe his weight could have remained the same or could have
 changed, i.e., 
\begin_inset Formula $p\left(H_{1}\right)=p\left(H_{2}\right)=0.5$
\end_inset

.
 Under 
\begin_inset Formula $H_{1}$
\end_inset

, he may think that it's more likely that 
\begin_inset Formula $\mu$
\end_inset

 is close to 170 than far from it and can take as a prior a normal distribution
 with mean 170 and standard deviation 
\begin_inset Formula $\tau$
\end_inset

, i.e., 
\begin_inset Formula $\pi\left(\mu\right)\sim\mathcal{N}\left(\mu_{0},\tau^{2}\right)$
\end_inset

, so that 
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{y}|H_{1}\right) & =\int_{\mu}p\left(\mathbf{y}|\mu,H_{1}\right)p\left(\mu|H_{1}\right)\dif\mu\\
 & =\int_{-\infty}^{\infty}\left[\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right\} \right]\left[\left(2\pi\tau^{2}\right)^{-1/2}\exp\left\{ -\frac{\left(\mu-\mu_{0}\right)^{2}}{2\tau^{2}}\right\} \right]\dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}-\frac{1}{2\tau^{2}}\left(\mu-\mu_{0}\right)^{2}\right\} \dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\frac{\sum_{i=1}^{n}\left(y_{i}^{2}-2\mu y_{i}+\mu^{2}\right)}{\sigma^{2}}+\frac{\mu^{2}-2\mu\mu_{0}+\mu_{0}^{2}}{\tau^{2}}\right]\right\} \dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\\
 & \quad\times\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\frac{\sum_{i=1}^{n}y_{i}^{2}-\sum_{i=1}^{n}2\mu y_{i}+\sum_{i=1}^{n}\mu^{2}}{\sigma^{2}}+\frac{\mu^{2}-2\mu\mu_{0}+\mu_{0}^{2}}{\tau^{2}}\right]\right\} \dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\\
 & \quad\times\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}-\frac{2\mu\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{n\mu^{2}}{\sigma^{2}}+\frac{\mu^{2}}{\tau^{2}}-\frac{2\mu\mu_{0}}{\tau^{2}}+\frac{\mu_{0}^{2}}{\tau^{2}}\right]\right\} \dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\\
 & \quad\times\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\mu^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)-2\mu\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)+\frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}+\frac{\mu_{0}^{2}}{\tau^{2}}\right]\right\} \dif\mu\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}+\frac{\mu_{0}^{2}}{\tau^{2}}\right]\right\} \\
 & \quad\times\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\mu^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)-2\mu\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} \dif\mu\\
 & \propto\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\mu^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)-2\mu\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\mu^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)-2\mu\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right.\right.\\
 & \quad\left.\left.+\frac{\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)^{2}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}-\frac{\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)^{2}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\right]\right\} \dif\mu\\
 & =\exp\left\{ -\frac{1}{2}\left[-\frac{\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)^{2}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\right]\right\} \\
 & \quad\times\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left[\mu^{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)-2\mu\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)+\frac{\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)^{2}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\right]\right\} \dif\mu\\
 & \propto\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)\left[\mu^{2}-2\mu\frac{\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}+\frac{\left(\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)^{2}}{\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)^{2}}\right]\right\} \dif\mu\\
 & =\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)\left(\mu-\frac{\frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\right)^{2}\right\} \dif\mu.
\end{flalign*}

\end_inset

Let 
\begin_inset Formula 
\[
\mu_{n}=\frac{\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\quad\text{and}\quad\frac{1}{\tau_{n}^{2}}=\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}},
\]

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{y}|H_{1}\right) & \propto\int_{-\infty}^{\infty}\exp\left\{ -\frac{1}{2\tau_{n}^{2}}\left(\mu-\mu_{n}\right)^{2}\right\} \dif\mu\\
 & =\left(2\pi\tau_{n}^{2}\right)^{1/2}\int_{-\infty}^{\infty}\left(2\pi\tau_{n}^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\tau_{n}^{2}}\left(\mu-\mu_{n}\right)^{2}\right\} \dif\mu.
\end{flalign*}

\end_inset

We recognize the integrand as the density of a 
\begin_inset Formula $\mathcal{N}\left(\mu_{n},\tau_{n}^{2}\right)$
\end_inset

 random variable, so that 
\begin_inset Formula 
\[
p\left(y|H_{1}\right)\propto\left(2\pi\tau_{n}^{2}\right)^{1/2}\cdot1.
\]

\end_inset

Adding back the constants we dropped gives
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{y}|H_{1}\right) & =\left(2\pi\sigma^{2}\right)^{-n/2}\left(2\pi\tau^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}+\frac{\mu_{0}^{2}}{\tau^{2}}\right]\right\} \\
 & \quad\times\exp\left\{ -\frac{1}{2}\left[-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} \left(2\pi\tau_{n}^{2}\right)^{1/2}\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left[\frac{2\pi\tau_{n}^{2}}{2\pi\tau^{2}}\right]^{1/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\left[\frac{\tau_{n}^{2}}{\tau^{2}}\right]^{1/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} .
\end{flalign*}

\end_inset

 Now, under 
\begin_inset Formula $H_{2}$
\end_inset

, 
\begin_inset Formula 
\[
\pi\left(\mu\right)=I_{\left\{ \mu=\mu_{0}\right\} }=\begin{cases}
1, & \text{if }\mu=\mu_{0}\\
0, & \text{otherwise}
\end{cases},
\]

\end_inset

i.e., 
\begin_inset Formula $\pi\left(\mu\right)$
\end_inset

 is a point mass at 
\begin_inset Formula $\mu_{0}$
\end_inset

, so that
\begin_inset Formula 
\begin{flalign*}
p\left(\mathbf{y}|H_{2}\right) & =\int_{\mu}p\left(y|\mu,H_{2}\right)p\left(\mu|H_{2}\right)\dif\mu\\
 & =p\left(y|\mu_{0}\right)\\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\mu_{0}\right)^{2}\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}^{2}-2\mu_{0}y_{i}+\mu_{0}^{2}\right)\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\sum_{i=1}^{n}y_{i}^{2}-\sum_{i=1}^{n}2\mu_{0}y_{i}+\sum_{i=1}^{n}\mu_{0}^{2}\right]\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{1}{2\sigma^{2}}\left[\sum_{i=1}^{n}y_{i}^{2}-2\mu_{0}n\bar{y}+n\mu_{0}^{2}\right]\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2\sigma^{2}}\left[n\mu_{0}^{2}-2\mu_{0}n\bar{y}\right]\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2\sigma^{2}}\left[n\mu_{0}^{2}-2\mu_{0}n\bar{y}+n\bar{y}^{2}-n\bar{y}^{2}\right]\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2\sigma^{2}}\left(n\mu_{0}^{2}-2\mu_{0}n\bar{y}+n\bar{y}^{2}\right)\right\} \exp\left\{ \frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} \\
 & =\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} \exp\left\{ \frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} .
\end{flalign*}

\end_inset

Then, the Bayes factor in support of 
\begin_inset Formula $H_{1}$
\end_inset

 is
\begin_inset Formula 
\begin{flalign*}
\text{BF} & =\frac{\left(2\pi\sigma^{2}\right)^{-n/2}\left[\frac{\tau_{n}^{2}}{\tau^{2}}\right]^{1/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} }{\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{ -\frac{\sum_{i=1}^{n}y_{i}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} \exp\left\{ \frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} }\\
 & =\frac{\left(\tau^{2}/\tau_{n}^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} }{\exp\left\{ \frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} \exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} }\\
 & =\frac{\left(\tau^{2}/\tau_{n}^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]\right\} \exp\left\{ -\frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} }{\exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} }\\
 & =\frac{\left(\tau^{2}/\tau_{n}^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)\right]-\frac{n\bar{y}^{2}}{2\sigma^{2}}\right\} }{\exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} }\\
 & =\frac{\left(\tau^{2}/\tau_{n}^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2}\left[\frac{\mu_{0}^{2}}{\tau^{2}}-\mu_{n}\left(\frac{n\bar{y}}{\sigma^{2}}+\frac{\mu_{0}}{\tau^{2}}\right)+\frac{n\bar{y}^{2}}{\sigma^{2}}\right]\right\} }{\exp\left\{ -\frac{n}{2\sigma^{2}}\left(\mu_{0}-\bar{y}\right)^{2}\right\} }
\end{flalign*}

\end_inset


\end_layout

\begin_layout Standard
ALGEBRAIC ISSUES, FIX LATER, FINAL RESULT SHOULD BE
\begin_inset Formula 
\[
\text{BF}=\frac{\left(\sigma^{2}/n+\tau^{2}\right)^{-1/2}\exp\left\{ -\frac{1}{2\left(\sigma^{2}/n+\tau^{2}\right)}\left(\bar{y}-\mu_{0}\right)^{2}\right\} }{\sqrt{n}/\sigma\exp\left\{ -\frac{n}{2\sigma^{2}}\left(\bar{y}-\mu_{0}\right)^{2}\right\} }.
\]

\end_inset


\end_layout

\begin_layout Standard
Let us consider different values of 
\begin_inset Formula $\tau$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

weights <- c(182,172,173,176,176,180,173,174,179,175)
\end_layout

\begin_layout Plain Layout

ybar <- mean(weights)
\end_layout

\begin_layout Plain Layout

n <- length(weights)
\end_layout

\begin_layout Plain Layout

sigma <- 3
\end_layout

\begin_layout Plain Layout

mu0 <- 170
\end_layout

\begin_layout Plain Layout

tau <- c(0.5,1,2,5,10)
\end_layout

\begin_layout Plain Layout

dnorm(ybar, mu0, sqrt(sigma^2 / n + tau^2)) / dnorm(ybar, mu0, sigma / sqrt(n))
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We see that there is very strong evidence in favor of 
\begin_inset Formula $H_{1}$
\end_inset

; his current weight is substantially different from 170 lbs.
\end_layout

\begin_layout Subsection
Relationship to model choice criteria
\end_layout

\begin_layout Standard
For large sample sizes 
\begin_inset Formula $n$
\end_inset

, Schwarz (1978) showed that an approximation to 
\begin_inset Formula $-2\log\text{BF}$
\end_inset

 is given by
\begin_inset Formula 
\[
\text{BIC}=-2\log\frac{\sup_{H_{1}}f\left(y|\hat{\theta}\right)}{\sup_{H_{2}}f\left(y|\theta\right)}-\left(p_{2}-p_{1}\right)\log n,
\]

\end_inset

where 
\begin_inset Formula 
\[
-2\log\frac{\sup_{H_{1}}f\left(y|\hat{\theta}\right)}{\sup_{H_{2}}f\left(y|\theta\right)}
\]

\end_inset

is the usual likelihood ratio rest statistic and 
\begin_inset Formula $p_{i}$
\end_inset

 is the number of parameters in model 
\begin_inset Formula $H_{i}$
\end_inset

.
 BIC stands for Bayesian Information Criterion (also known as the Schwartz
 Criterion).
 The second term in BIC acts as a penalty term which corrects for differences
 in size between the models.
\end_layout

\begin_layout Chapter
Monte Carlo methods
\end_layout

\begin_layout Standard
In simple models, especially if conjugate prior distributions are assumed,
 it is often easy to draw from the posterior distribution without difficulty.
 When the posterior density does not have a recognizable form, it might
 be possible to factor the distribution analytically and simulate in parts,
 as we have done in previous chapters.
 For more complicated problems, it is not possible to directly generate
 samples from the target distribution and indirect sampling schemes are
 used.
\end_layout

\begin_layout Section
Direct sampling
\end_layout

\begin_layout Standard
Monte Carlo sampling is often used in two kinds of related problems:
\end_layout

\begin_layout Itemize
Sampling from a distribution 
\begin_inset Formula $f\left(\theta\right)$
\end_inset

 (this will often be a posterior distribution for this course).
\end_layout

\begin_layout Itemize
Computing approximate integrals of the form 
\begin_inset Formula $\int h\left(\theta\right)f\left(\theta\right)\dif\theta$
\end_inset

, i.e., computing the expectation of 
\begin_inset Formula $h\left(\theta\right)$
\end_inset

 using the density 
\begin_inset Formula $f\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The above problems are related because if we can sample from 
\begin_inset Formula $f\left(\theta\right)$
\end_inset

, then we can also solve the problem of computing integrals.
 Suppose we can draw samples 
\begin_inset Formula $\theta^{\left(1\right)},\ldots,\theta^{\left(K\right)}$
\end_inset

 from 
\begin_inset Formula $f\left(\theta|y\right)$
\end_inset

.
 If we want to evaluate 
\begin_inset Formula $\E\left[h\left(\theta|y\right)\right]=\int_{\theta}h\left(\theta\right)f\left(\theta|y\right)\dif\theta$
\end_inset

, then 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:strong-lln"

\end_inset

 implies that
\begin_inset Formula 
\[
\frac{1}{K}\sum_{k=1}^{K}h\left(\theta^{\left(k\right)}\right)\rightarrow\E\left[h\left(\theta|y\right)\right]\quad\text{as }K\rightarrow\infty.
\]

\end_inset

This general procedure is called Monte Carlo integration.
 Other methods for direct sampling include
\end_layout

\begin_layout Enumerate
cumulative ordered values, which approximate the cdf 
\begin_inset Formula $F\left(\theta|Y\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
the empirical distribution of the samples 
\begin_inset Formula $\theta^{\left(1\right)},\ldots,\theta^{\left(K\right)}$
\end_inset

, which approximates 
\begin_inset Formula $f\left(\theta|Y\right)$
\end_inset

 (construct a histogram or kernel density estimator);
\end_layout

\begin_layout Enumerate
the proportion of samples where the event 
\begin_inset Formula $h\left(\theta^{\left(k\right)}\right)>g$
\end_inset

 occurs, which approximates 
\begin_inset Formula $P\left(\left\{ h\left(\theta\right)>c\right\} \right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
and sample moments/quantiles/functions, which approximate true moments/quantiles
/functions.
\end_layout

\begin_layout Standard
These approaches extend easily to higher dimensions.
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $f\left(\theta|Y\right)\sim\text{Beta}\left(7,82\right)$
\end_inset

.
 The exact posterior mean is 
\begin_inset Formula $7/\left(7+82\right)=0.0786$
\end_inset

.
 The median and 95% credible intervals are obtained below.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

qbeta(c(0.025,0.5,0.975), 7, 82)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
We can also estimate the parameters through simulation.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

set.seed(123)
\end_layout

\begin_layout Plain Layout

theta <- rbeta(1e4, 7, 82)
\end_layout

\begin_layout Plain Layout

mean(theta)
\end_layout

\begin_layout Plain Layout

quantile(theta, c(0.025,0.5,0.725))
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
As the number of draws 
\begin_inset Formula $K$
\end_inset

 increases, the estimate will approach the true value.
 We can also obtain HPD intervals for 
\begin_inset Formula $\theta$
\end_inset

 and the odds ratio 
\begin_inset Formula $\theta/\left(1-\theta\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library(coda)
\end_layout

\begin_layout Plain Layout

theta.post <- as.mcmc(theta)
\end_layout

\begin_layout Plain Layout

HPDinterval(theta.post)
\end_layout

\begin_layout Plain Layout

odds.post <- as.mcmc(theta / (1 - theta))
\end_layout

\begin_layout Plain Layout

HPDinterval(odds.post)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
Observe that HPD intervals are not invariant under transformation.
\end_layout

\begin_layout Section
Inverse transformation method
\end_layout

\begin_layout Part
Numerical methods
\end_layout

\begin_layout Chapter
Regression
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $y_{i}$
\end_inset

 be the 
\begin_inset Formula $i\text{th}$
\end_inset

 observation of some response variable, and let 
\begin_inset Formula $\mathbf{x}^{\left(i\right)}\in\mathbb{R}^{n}$
\end_inset

 be the corresponding vector of predictors, so that the data are 
\begin_inset Formula $\left(y_{i},\mathbf{x}^{\left(i\right)}\right)$
\end_inset

 for 
\begin_inset Formula $i\in\left\{ 1,2,\ldots,N\right\} $
\end_inset

.
 Then, the regression model is 
\begin_inset Formula 
\[
y_{i}\sim\alpha_{0}+\alpha_{1}x_{1}^{\left(i\right)}+\cdots+\alpha_{n}x_{n}^{\left(i\right)}.
\]

\end_inset

Suppose that we wish to examine a hypothesized linear relationship between
 height and weight in some population of interest.
 Let 
\begin_inset Formula $y_{i}$
\end_inset

 and 
\begin_inset Formula $x_{i}$
\end_inset

 be the height and weight, respectively, of the 
\begin_inset Formula $i\text{th}$
\end_inset

 person, so that the regression model is 
\begin_inset Formula $y_{i}\sim\alpha_{0}+\alpha_{1}x_{i}$
\end_inset

.
 There are infinitely many unique choices of 
\begin_inset Formula $\boldsymbol{\alpha}=\left(\alpha_{0},\alpha_{1}\right)$
\end_inset

, so that there are infinitely many lines that 
\begin_inset Quotes eld
\end_inset

fit
\begin_inset Quotes erd
\end_inset

 the data.
 The difference between the observed and predicted values for the 
\begin_inset Formula $i\text{th}$
\end_inset

 person, i.e., the 
\begin_inset Formula $i\text{th}$
\end_inset

 residual, is then 
\begin_inset Formula $r_{i}=y_{i}-\left(\alpha_{0}+\alpha_{1}x_{i}\right)$
\end_inset

.
 Then, we define the line that 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 fits the data as that whose coefficients 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

 minimize the least-squares loss function 
\begin_inset Formula 
\[
L\left(\boldsymbol{\alpha}\right)=\sum_{i=1}^{N}r_{i}^{2}=\sum_{i=1}^{N}\left(y_{i}-\alpha_{0}-\alpha_{1}x_{i}\right)^{2},
\]

\end_inset

i.e., we must solve the optimization problem
\begin_inset Formula 
\[
\min_{\boldsymbol{\alpha}\in\mathbb{R}^{2}}\sum_{i=1}^{N}\left(y_{i}-\alpha_{0}-\alpha_{1}x_{i}\right)^{2}.
\]

\end_inset

Suppose we wish to optimize the function 
\begin_inset Formula $f\left(x\right)=2x^{2}+3$
\end_inset

, where 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

.
 We might choose to solve 
\begin_inset Formula $f'\left(x\right)=0$
\end_inset

, or we might choose to lay down a grid over some domain, graph the function,
 and perform an exhaustive search.
 Each evaluation of 
\begin_inset Formula $f$
\end_inset

 requires 2 multiplications and one addition.
 For a grid whose elements are the integers between -10 and 10, the exhaustive
 search method will thus require 
\begin_inset Formula $2\cdot21=42$
\end_inset

 multiplications, which is a nearly trivial computational task.
 Now suppose that 
\begin_inset Formula $f\left(\mathbf{x}\right)=3x_{1}^{2}+2x_{2}^{2}-x_{1}x_{2}+5$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{2}$
\end_inset

.
 We might now lay down a (2-dimensional) grid given by 
\begin_inset Formula 
\[
\left\{ \mathbf{x}\,|\,x_{1}\in\left\{ -10,-9,\ldots,9,10\right\} ,x_{2}\in\left\{ -10,-9,\ldots,9,10\right\} \right\} .
\]

\end_inset

Each evaluation of 
\begin_inset Formula $f$
\end_inset

 requires 5 multiplications and 3 additions, so that the exhaustive search
 method requires 
\begin_inset Formula $5\cdot21^{2}$
\end_inset

 multiplications.
 Now suppose that 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

, and consider a real-valued function 
\begin_inset Formula $f\left(\mathbf{x}\right)$
\end_inset

.
 Suppose further that for the 
\begin_inset Formula $i\text{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{x}$
\end_inset

, we lay down a grid such that 
\begin_inset Formula $x_{i}\in\left\{ -10,-9,\ldots,9,10\right\} $
\end_inset

.
 The number of multiplications required by the exhaustive search method
 is now on the order of 
\begin_inset Formula $21^{n}$
\end_inset

.
 If we assume that we can perform 
\begin_inset Formula $10^{6}$
\end_inset

 multiplications per second, then for 
\begin_inset Formula $n=5$
\end_inset

, we must evaluate 
\begin_inset Formula $f$
\end_inset

 
\begin_inset Formula 
\[
21^{5}\approx\left(2\cdot10\right)^{5}=2^{5}10^{5}=32\cdot10^{5}\approx3\cdot10\cdot10^{5}=3\cdot10^{6}
\]

\end_inset

total times, so that the optimization will require roughly 3 seconds.
 If 
\begin_inset Formula $n=10$
\end_inset

, then we must perform 
\begin_inset Formula $21^{10}\approx2^{10}\cdot10^{10}$
\end_inset

 function evaluations, so that the optimization will require roughly 
\begin_inset Formula $2^{10}\cdot10^{4}=1024\cdot10^{4}\approx10^{3}\cdot10^{4}=10^{7}$
\end_inset

 seconds, or equivalently, 116 days.
 We see that the exhaustive search method will not in general be feasible,
 particularly in high dimensions.
\end_layout

\begin_layout Standard
We now consider the alternative method, i.e., solving 
\begin_inset Formula $f'\left(x\right)=0$
\end_inset

.
 For 
\begin_inset Formula $f\left(x\right)=2x^{2}+3$
\end_inset

, we have 
\begin_inset Formula $f'\left(x\right)=4x\implies x=0$
\end_inset

, so that 
\begin_inset Formula $f$
\end_inset

 is minimized by 
\begin_inset Formula $x=0$
\end_inset

.
 But 
\begin_inset Formula $f'\left(x\right)=0$
\end_inset

 cannot usually be solved analytically, e.g., suppose that 
\begin_inset Formula $f\left(x\right)=\mathrm{e}^{x}+x^{2}\implies f'\left(x\right)=\mathrm{e}^{x}+2x$
\end_inset

.
\end_layout

\begin_layout Section
Optimization of a quadratic function
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $f\left(x\right)=ax^{2}+bx+c$
\end_inset

, so that 
\begin_inset Formula $f'\left(x\right)=2ax+b$
\end_inset

.
 Then, a critical point of 
\begin_inset Formula $f$
\end_inset

 occurs at 
\begin_inset Formula $x=-b/\left(2a\right)$
\end_inset

.
 Now suppose that 
\begin_inset Formula $f\left(\mathbf{x}\right)=2x_{1}^{2}+3x_{2}^{2}+x_{1}x_{2}-2x_{1}-5x_{2}+5$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{2}$
\end_inset

.
 To find the critical point of this function, we will set the gradient of
 
\begin_inset Formula $f$
\end_inset

 equal to zero.
 We have
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=\begin{bmatrix}4x_{1}+x_{2}-2\\
6x_{2}+x_{1}-5
\end{bmatrix},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=\mathbf{0}\implies\begin{bmatrix}4x_{1}+x_{2}-2\\
x_{1}+6x_{2}-5
\end{bmatrix}=\begin{bmatrix}4 & 1\\
1 & 6
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}-\begin{bmatrix}2\\
5
\end{bmatrix}=\begin{bmatrix}0\\
0
\end{bmatrix}\implies\begin{bmatrix}4 & 1\\
1 & 6
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}=\begin{bmatrix}2\\
5
\end{bmatrix}.
\]

\end_inset

Let 
\begin_inset Formula 
\[
\mathbf{A}=\begin{bmatrix}4 & 1\\
1 & 6
\end{bmatrix}\quad\text{\text{and}}\quad\mathbf{b}=\begin{bmatrix}2\\
5
\end{bmatrix},
\]

\end_inset

so that 
\begin_inset Formula $\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}$
\end_inset

.
 Now suppose that 
\begin_inset Formula $n>1$
\end_inset

, i.e., 
\begin_inset Formula $f$
\end_inset

 has the form of an 
\begin_inset Formula $n$
\end_inset

-dimensional paraboloid.
 
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:gen-quadratic-form"

\end_inset

Any quadratic 
\begin_inset Formula $f\left(\mathbf{x}\right):\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 can be written as 
\begin_inset Formula $f\left(\mathbf{x}\right)=\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c$
\end_inset

, where 
\begin_inset Formula $\mathbf{M}$
\end_inset

 is a symmetric 
\begin_inset Formula $n\times n$
\end_inset

 matrix, 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is an 
\begin_inset Formula $n$
\end_inset

-dimensional vector, and 
\begin_inset Formula $c$
\end_inset

 is a scalar.
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $f\left(\mathbf{x}\right)=5x_{1}^{2}+3x_{1}x_{2}+5x_{2}+3$
\end_inset

.
 We can write 
\begin_inset Formula $f$
\end_inset

 as
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=\begin{bmatrix}x_{1} & x_{2}\end{bmatrix}\begin{bmatrix}5 & 3\\
0 & 0
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}+\begin{bmatrix}0 & 5\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}+3=\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c,
\]

\end_inset

where 
\begin_inset Formula 
\[
\mathbf{M}=\begin{bmatrix}5 & 3\\
0 & 0
\end{bmatrix},\quad\mathbf{b}=\begin{bmatrix}0\\
5
\end{bmatrix},\quad\text{and}\quad c=3.
\]

\end_inset

Observe also that 
\begin_inset Formula 
\[
\begin{bmatrix}x_{1} & x_{2}\end{bmatrix}\begin{bmatrix}5 & 3/2\\
3/2 & 0
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}=\begin{bmatrix}x_{1} & x_{2}\end{bmatrix}\begin{bmatrix}5x_{1}+3x_{2}/2\\
3x_{1}/2
\end{bmatrix}=5x_{1}^{2}+\frac{3}{2}x_{1}x_{2}+\frac{3}{2}x_{1}x_{2}=5x_{1}^{2}+3x_{1}x_{2},
\]

\end_inset

i.e., we can take 
\begin_inset Formula $\mathbf{M}$
\end_inset

 as symmetric.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:vector-matrix-product"

\end_inset

Let 
\begin_inset Formula $\mathbf{u}\in\mathbb{R}^{n}$
\end_inset

, let 
\begin_inset Formula $\mathbf{v}\in\mathbb{R}^{m}$
\end_inset

, and let 
\begin_inset Formula $\mathbf{M}$
\end_inset

 be an 
\begin_inset Formula $n\times m$
\end_inset

 matrix.
 Then,
\begin_inset Formula 
\[
\mathbf{u}^{\mathsf{T}}\mathbf{M}\mathbf{v}=\sum_{j=1}^{m}\sum_{i=1}^{n}u_{i}M_{ij}v_{j}.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $M_{ij}$
\end_inset

 be the 
\begin_inset Formula $ij\text{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{M}$
\end_inset

, and let 
\begin_inset Formula $\mathbf{m}_{j}$
\end_inset

 be the 
\begin_inset Formula $j\text{th}$
\end_inset

 column of 
\begin_inset Formula $\mathbf{M}$
\end_inset

, so that 
\begin_inset Formula 
\[
\mathbf{M}\mathbf{v}=\begin{bmatrix}\mathbf{m}_{1} & \mathbf{m}_{2} & \cdots & \mathbf{m}_{m}\end{bmatrix}\begin{bmatrix}v_{1}\\
v_{2}\\
\vdots\\
v_{m}
\end{bmatrix}=\mathbf{m}_{1}v_{1}+\mathbf{m}_{2}v_{2}+\cdots+\mathbf{m}_{m}v_{m}=\sum_{j=1}^{m}\mathbf{m}_{j}v_{j},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\mathbf{u}^{\mathsf{T}}\mathbf{M}\mathbf{v}=\mathbf{u}\cdot\mathbf{M}\mathbf{v}=\mathbf{u}\cdot\sum_{j=1}^{m}\mathbf{m}_{j}v_{j}=\sum_{j=1}^{m}\mathbf{u}\cdot\mathbf{m}_{j}v_{j}=\sum_{j=1}^{m}\left(u_{1}M_{1j}+u_{2}M_{2j}+\cdots+u_{n}M_{nj}\right)v_{j}=\sum_{j=1}^{m}\sum_{i=1}^{n}u_{i}M_{ij}v_{j}.
\]

\end_inset


\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:gen-quadratic-gradient"

\end_inset

The gradient of an 
\begin_inset Formula $n$
\end_inset

-dimensional quadratic 
\begin_inset Formula $f\left(\mathbf{x}\right)=\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c$
\end_inset

 is
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=2\mathbf{M}\mathbf{x}+\mathbf{b},
\]

\end_inset

where 
\begin_inset Formula $\mathbf{M}$
\end_inset

 and 
\begin_inset Formula $\mathbf{b}$
\end_inset

 are as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:vector-matrix-product"

\end_inset

.
\end_layout

\begin_layout Proof
We have
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c\right)=\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}\right)+\nabla\left(\mathbf{b}^{\mathsf{T}}\mathbf{x}\right)+\nabla c=\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}\right)+\nabla\left(\mathbf{b}^{\mathsf{T}}\mathbf{x}\right).
\]

\end_inset

Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:vector-matrix-product"

\end_inset

 implies that 
\begin_inset Formula 
\[
\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}\right)=\nabla\left(\sum_{j=1}^{n}\sum_{i=1}^{n}x_{i}M_{ij}x_{j}\right).
\]

\end_inset

The derivative with respect to 
\begin_inset Formula $x_{k}$
\end_inset

 of any term that does not include 
\begin_inset Formula $x_{k}$
\end_inset

 is zero.
 For 
\begin_inset Formula $j\neq k$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial x_{k}}\sum_{\substack{j=1\\
j\neq k
}
}^{n}\sum_{i=1}^{n}x_{i}M_{ij}x_{j} & =\sum_{\substack{j=1\\
j\neq k
}
}^{n}\frac{\partial}{\partial x_{k}}\left(x_{1}M_{1j}x_{j}+\cdots+x_{k}M_{kj}x_{j}+\cdots+x_{n}M_{nj}x_{j}\right)\\
 & =\sum_{\substack{j=1\\
j\neq k
}
}^{n}\left(0+\cdots+M_{kj}x_{j}+\cdots+0\right)\\
 & =\sum_{\substack{j=1\\
j\neq k
}
}^{n}M_{kj}x_{j},
\end{flalign*}

\end_inset

and for 
\begin_inset Formula $j=k$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial x_{k}}\sum_{i=1}^{n}x_{i}M_{ik}x_{k} & =\frac{\partial}{\partial x_{k}}\left(x_{1}M_{1k}x_{k}+\cdots+x_{k}M_{kk}x_{k}+\cdots+x_{n}M_{nk}x_{k}\right)\\
 & =\frac{\partial}{\partial x_{k}}x_{1}M_{1k}x_{k}+\cdots+\frac{\partial}{\partial x_{k}}x_{k}^{2}M_{kk}+\cdots+\frac{\partial}{\partial x_{k}}x_{n}M_{nk}x_{k}\\
 & =x_{1}M_{1k}+\cdots+2x_{k}M_{kk}+\cdots+x_{n}M_{nk}\\
 & =2x_{k}M_{kk}+\sum_{\substack{i=1\\
i\neq k
}
}^{n}x_{i}M_{ik},
\end{flalign*}

\end_inset

so that 
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial x_{k}}\sum_{j=1}^{n}\sum_{i=1}^{n}x_{i}M_{ij}x_{j} & =\sum_{\substack{j=1\\
j\neq k
}
}^{n}M_{kj}x_{j}+2x_{k}M_{kk}+\sum_{\substack{i=1\\
i\neq k
}
}^{n}x_{i}M_{ik}\\
 & =2x_{k}M_{kk}+\sum_{\substack{i=1\\
i\neq k
}
}^{n}\left(M_{ki}x_{i}+x_{i}M_{ik}\right)\\
 & =\left(M_{kk}+M_{kk}\right)x_{k}+\sum_{\substack{i=1\\
i\neq k
}
}^{n}\left(M_{ki}+M_{ik}\right)x_{i}\\
 & =\sum_{i=1}^{n}\left(M_{ki}+M_{ik}\right)x_{i}.
\end{flalign*}

\end_inset

Thus,
\begin_inset Formula 
\begin{flalign*}
\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}\right) & =\begin{bmatrix}\sum_{i=1}^{n}\left(M_{1i}+M_{i1}\right)x_{i}\\
\sum_{i=1}^{n}\left(M_{2i}+M_{i2}\right)x_{i}\\
\vdots\\
\sum_{i=1}^{n}\left(M_{ni}+M_{in}\right)x_{i}
\end{bmatrix}\\
 & =\begin{bmatrix}\left(M_{11}+M_{11}\right)x_{1}+\left(M_{12}+M_{21}\right)x_{2}+\cdots+\left(M_{1n}+M_{n1}\right)x_{n}\\
\left(M_{21}+M_{12}\right)x_{1}+\left(M_{22}+M_{22}\right)x_{2}+\cdots+\left(M_{2n}+M_{n2}\right)x_{n}\\
\vdots\\
\left(M_{n1}+M_{1n}\right)x_{1}+\left(M_{n2}+M_{2n}\right)x_{n}+\cdots+\left(M_{nn}+M_{nn}\right)x_{n}
\end{bmatrix}\\
 & =\begin{bmatrix}M_{11}+M_{11} & M_{12}+M_{21} & \cdots & M_{1n}+M_{n1}\\
M_{21}+M_{12} & M_{22}+M_{22} & \cdots & M_{2n}+M_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
M_{n1}+M_{1n} & M_{n2}+M_{2n} & \cdots & M_{nn}+M_{nn}
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix}\\
 & =\left(\begin{bmatrix}M_{11} & M_{12} & \cdots & M_{1n}\\
M_{21} & M_{22} & \cdots & M_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
M_{n1} & M_{n2} & \cdots & M_{nn}
\end{bmatrix}+\begin{bmatrix}M_{11} & M_{21} & \cdots & M_{n1}\\
M_{12} & M_{22} & \cdots & M_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
M_{1n} & M_{2n} & \cdots & M_{nn}
\end{bmatrix}\right)\mathbf{x}\\
 & =\left(\mathbf{M}+\mathbf{M}^{\mathsf{T}}\right)\mathbf{x}\\
 & =2\mathbf{M}\mathbf{x}.\tag{\ensuremath{\mathbf{M}} is symmetric}
\end{flalign*}

\end_inset

Noting that 
\begin_inset Formula 
\[
\frac{\partial}{\partial x_{i}}\left(b_{1}x_{1}+b_{2}x_{2}+\cdots+b_{i}x_{i}+\cdots+b_{n}x_{n}\right)=b_{i},
\]

\end_inset

we have 
\begin_inset Formula 
\[
\nabla\left(\mathbf{b}^{\mathsf{T}}\mathbf{x}\right)=\nabla\left(b_{1}x_{1}+b_{2}x_{2}+\cdots+b_{n}x_{n}\right)=\mathbf{b},
\]

\end_inset

so that 
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=\nabla\left(\mathbf{x}^{\mathsf{T}}\mathbf{M}\mathbf{x}\right)+\nabla\left(\mathbf{b}^{\mathsf{T}}\mathbf{x}\right)=2\mathbf{M}\mathbf{x}+\mathbf{b}.
\]

\end_inset


\end_layout

\begin_layout Standard
The critical point of an 
\begin_inset Formula $n$
\end_inset

-dimensional quadratic occurs when 
\begin_inset Formula $\nabla f\left(\mathbf{x}\right)=\mathbf{0}$
\end_inset

.
 We now solve for the critical point.
\begin_inset Formula 
\begin{equation}
\nabla f\left(\mathbf{x}\right)=2\mathbf{M}\mathbf{x}+\mathbf{b}=\mathbf{0}\implies2\mathbf{M}\mathbf{x}=-\mathbf{b}\implies\mathbf{M}\mathbf{x}=-\frac{1}{2}\mathbf{b}\implies\mathbf{x}=\mathbf{M}^{-1}\left(-\frac{1}{2}\mathbf{b}\right)=-\frac{1}{2}\mathbf{M}^{-1}\mathbf{b}.\label{eq:gen-quadratic-optimum}
\end{equation}

\end_inset


\end_layout

\begin_layout Remark
The class of quadratic functions is the only class that can be analyzed
 in any number of dimensions.
\end_layout

\begin_layout Standard
Consider again the least-squares loss function, which is easily seen to
 be quadratic.
 We will express 
\begin_inset Formula $L\left(\boldsymbol{\alpha}\right)$
\end_inset

 in the form of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:gen-quadratic-form"

\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
L\left(\boldsymbol{\alpha}\right) & =\sum_{i=1}^{N}\left(y_{i}-\alpha_{0}-\alpha_{1}x_{i}\right)^{2}\\
 & =\sum_{i=1}^{N}\left(y_{i}^{2}-2y_{i}\left(\alpha_{0}+\alpha_{1}x_{i}\right)+\left(\alpha_{0}+\alpha_{1}x_{i}\right)^{2}\right)\\
 & =\sum_{i=1}^{N}\left(y_{i}^{2}-2\alpha_{0}y_{i}-2\alpha_{1}x_{i}y_{i}+\alpha_{0}^{2}+2\alpha_{0}\alpha_{1}x_{i}+\alpha_{1}^{2}x_{i}^{2}\right)\\
 & =\sum_{i=1}^{N}\alpha_{0}^{2}+\sum_{i=1}^{N}\alpha_{1}^{2}x_{i}^{2}+\sum_{i=1}^{N}2\alpha_{0}\alpha_{1}x_{i}-\sum_{i=1}^{N}2\alpha_{0}y_{i}-\sum_{i=1}^{N}2\alpha_{1}x_{i}y_{i}+\sum_{i=1}^{N}y_{i}^{2}\\
 & =N\alpha_{0}^{2}+\alpha_{1}^{2}\sum_{i=1}^{N}x_{i}^{2}+\alpha_{0}\alpha_{1}\left(2\sum_{i=1}^{N}x_{i}\right)-\alpha_{0}\left(2\sum_{i=1}^{N}y_{i}\right)-\alpha_{1}\left(2\sum_{i=1}^{N}x_{i}y_{i}\right)+\sum_{i=1}^{N}y_{i}^{2}\\
 & =\begin{bmatrix}\alpha_{0} & \alpha_{1}\end{bmatrix}\begin{bmatrix}N & \sum_{i=1}^{N}x_{i}\\
\sum_{i=1}^{N}x_{i} & \sum_{i=1}^{N}x_{i}^{2}
\end{bmatrix}\begin{bmatrix}\alpha_{0}\\
\alpha_{1}
\end{bmatrix}+\begin{bmatrix}-2\sum_{i=1}^{N}y_{i} & -2\sum_{i=1}^{N}x_{i}y_{i}\end{bmatrix}\begin{bmatrix}\alpha_{0}\\
\alpha_{1}
\end{bmatrix}+\sum_{i=1}^{N}y_{i}^{2}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}+\mathbf{b}^{\mathsf{T}}\boldsymbol{\alpha}+c,
\end{flalign*}

\end_inset

where 
\begin_inset Formula 
\[
\mathbf{M}=\begin{bmatrix}N & \sum_{i=1}^{N}x_{i}\\
\sum_{i=1}^{N}x_{i} & \sum_{i=1}^{N}x_{i}^{2}
\end{bmatrix},\quad\mathbf{b}=\begin{bmatrix}-2\sum_{i=1}^{N}y_{i}\\
-2\sum_{i=1}^{N}x_{i}y_{i}
\end{bmatrix},\quad\text{and}\quad c=\sum_{i=1}^{N}y_{i}^{2}.
\]

\end_inset

The critical point of 
\begin_inset Formula $L\left(\boldsymbol{\alpha}\right)$
\end_inset

 therefore occurs at 
\begin_inset Formula $\boldsymbol{\alpha}=\mathbf{M}^{-1}\mathbf{b}/2$
\end_inset

.
 Now consider 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

, so that 
\begin_inset Formula 
\[
L\left(\boldsymbol{\alpha}\right)=\sum_{i=1}^{N}\left(y_{i}-\alpha_{0}-\alpha_{1}x_{1}^{\left(i\right)}-\alpha_{2}x_{2}^{\left(i\right)}-\cdots-\alpha_{n}x_{n}^{\left(i\right)}\right)^{2}=\sum_{i=1}^{N}r_{i}^{2}=\left\Vert \mathbf{r}\right\Vert ^{2},
\]

\end_inset

where 
\begin_inset Formula 
\begin{flalign*}
\mathbf{r} & =\begin{bmatrix}r_{1}\\
r_{2}\\
\vdots\\
r_{N}
\end{bmatrix}\\
 & =\begin{bmatrix}y_{1}-\alpha_{0}-\alpha_{1}x_{1}^{\left(1\right)}-\cdots-\alpha_{n}x_{n}^{\left(1\right)}\\
y_{2}-\alpha_{0}-\alpha_{1}x_{1}^{\left(2\right)}-\cdots-\alpha_{n}x_{n}^{\left(2\right)}\\
\vdots\\
y_{N}-\alpha_{0}-\alpha_{1}x_{1}^{\left(N\right)}-\cdots-\alpha_{n}x_{n}^{\left(N\right)}
\end{bmatrix}\\
 & =\begin{bmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{N}
\end{bmatrix}-\begin{bmatrix}\alpha_{0}+\alpha_{1}x_{1}^{\left(1\right)}+\cdots+\alpha_{n}x_{n}^{\left(1\right)}\\
\alpha_{0}+\alpha_{1}x_{1}^{\left(2\right)}+\cdots+\alpha_{n}x_{n}^{\left(2\right)}\\
\vdots\\
\alpha_{0}+\alpha_{1}x_{1}^{\left(N\right)}+\cdots+\alpha_{n}x_{n}^{\left(N\right)}
\end{bmatrix}\\
 & =\mathbf{y}-\begin{bmatrix}1 & x_{1}^{\left(1\right)} & \cdots & x_{n}^{\left(1\right)}\\
1 & x_{1}^{\left(2\right)} & \cdots & x_{n}^{\left(2\right)}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{1}^{\left(N\right)} & \cdots & x_{n}^{\left(N\right)}
\end{bmatrix}\begin{bmatrix}\alpha_{0}\\
\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{bmatrix}\\
 & =\mathbf{y}-\mathbf{B}\boldsymbol{\alpha},
\end{flalign*}

\end_inset

and 
\begin_inset Formula $\mathbf{B}$
\end_inset

 is referred to as the 
\emph on
model matrix
\emph default
.
 Thus,
\begin_inset Formula 
\begin{flalign*}
L\left(\boldsymbol{\alpha}\right) & =\left\Vert \mathbf{r}\right\Vert ^{2}\\
 & =\left\Vert \mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right\Vert ^{2}\\
 & =\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\cdot\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\\
 & =\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)^{\mathsf{T}}\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\\
 & =\left[\mathbf{y}^{\mathsf{T}}-\left(\mathbf{B}\boldsymbol{\alpha}\right)^{\mathsf{T}}\right]\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\\
 & =\left(\mathbf{y}^{\mathsf{T}}-\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\right)\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\\
 & =\mathbf{y}^{\mathsf{T}}\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)-\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\left(\mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right)\\
 & =\mathbf{y}^{\mathsf{T}}\mathbf{y}-\mathbf{y}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}-\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\mathbf{y}+\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}.
\end{flalign*}

\end_inset

Now, 
\begin_inset Formula $\boldsymbol{\alpha}^{\mathsf{T}}$
\end_inset

 is 
\begin_inset Formula $1\times\left(n+1\right)$
\end_inset

, 
\begin_inset Formula $\mathbf{B}^{\mathsf{T}}$
\end_inset

 is 
\begin_inset Formula $\left(n+1\right)\times N$
\end_inset

, and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is 
\begin_inset Formula $N\times1$
\end_inset

, so that 
\begin_inset Formula $\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\mathbf{y}$
\end_inset

 is 
\begin_inset Formula $1\times1$
\end_inset

, i.e., a scalar, hence equal to its transpose.
 Then, letting 
\begin_inset Formula $\mathbf{M}=\mathbf{B}^{\mathsf{T}}\mathbf{B}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
L\left(\boldsymbol{\alpha}\right) & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}-\mathbf{y}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}-\left(\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{B}^{\mathsf{T}}\mathbf{y}\right)^{\mathsf{T}}+\mathbf{y}^{\mathsf{T}}\mathbf{y}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}-\mathbf{y}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}-\mathbf{y}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}+\mathbf{y}^{\mathsf{T}}\mathbf{y}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}-2\mathbf{y}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}+\mathbf{y}^{\mathsf{T}}\mathbf{y}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}+\left(\left(-2\mathbf{y}^{\mathsf{T}}\mathbf{B}\right)^{\mathsf{T}}\right)^{\mathsf{T}}\boldsymbol{\alpha}+\mathbf{y}^{\mathsf{T}}\mathbf{y}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}+\left(-2\mathbf{B}^{\mathsf{T}}\mathbf{y}\right)^{\mathsf{T}}\boldsymbol{\alpha}+\mathbf{y}^{\mathsf{T}}\mathbf{y}\\
 & =\boldsymbol{\alpha}^{\mathsf{T}}\mathbf{M}\boldsymbol{\alpha}+\mathbf{b}^{\mathsf{T}}\boldsymbol{\alpha}+c,
\end{flalign*}

\end_inset

where 
\begin_inset Formula $\mathbf{b}=-2\mathbf{B}^{\mathsf{T}}\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $c=\mathbf{y}^{\mathsf{T}}\mathbf{y}$
\end_inset

.
 We recognize this expression as a general quadratic, so 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:gen-quadratic-gradient"

\end_inset

 implies that its gradient is given by 
\begin_inset Formula $\nabla L\left(\boldsymbol{\alpha}\right)=2\mathbf{M}\boldsymbol{\alpha}+\mathbf{b}$
\end_inset

, and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:gen-quadratic-optimum"

\end_inset

 implies that the critical point of 
\begin_inset Formula $L\left(\boldsymbol{\alpha}\right)$
\end_inset

 occurs at 
\begin_inset Formula 
\begin{equation}
\boldsymbol{\alpha}=-\frac{1}{2}\mathbf{M}^{-1}\mathbf{b}=-\frac{1}{2}\left(\mathbf{B}^{\mathsf{T}}\mathbf{B}\right)^{-1}\left(-2\mathbf{B}^{\mathsf{T}}\mathbf{y}\right)=\left(\mathbf{B}^{\mathsf{T}}\mathbf{B}\right)^{-1}\mathbf{B}^{\mathsf{T}}\mathbf{y},\label{eq:normal-eqns}
\end{equation}

\end_inset

where 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:normal-eqns"

\end_inset

 are referred to as the 
\emph on
normal equations
\emph default
.
 Hence, least-squares regression is simply quadratic optimization.
\end_layout

\begin_layout Section
Optimization
\end_layout

\begin_layout Standard
Consider minimizing the function 
\begin_inset Formula $f\left(\mathbf{x}\right):\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

.
 In the case that 
\begin_inset Formula $f$
\end_inset

 is quadratic, then 
\begin_inset Formula $\nabla f\left(\mathbf{x}\right)=\mathbf{0}$
\end_inset

 can be solved analytically.
 But in the case that 
\begin_inset Formula $f$
\end_inset

 is not quadratic, or more generally when 
\begin_inset Formula $\nabla f\left(\mathbf{x}\right)=\mathbf{0}$
\end_inset

 does not have an analytic solution, then iterative methods must be applied.
 Recall that the gradient of 
\begin_inset Formula $f$
\end_inset

 at some point 
\begin_inset Formula $\mathbf{x}$
\end_inset

 gives the direction at 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in which 
\begin_inset Formula $f$
\end_inset

 increases most rapidly, or the direction of 
\emph on
steepest ascent
\emph default
.
 Equivalently, 
\begin_inset Formula $-\nabla f\left(\mathbf{x}\right)$
\end_inset

 gives the direction of 
\emph on
steepest descent
\emph default
 at 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Given some initial point 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}$
\end_inset

, we can therefore find a critical point of 
\begin_inset Formula $f$
\end_inset

 by taking a step of length 
\begin_inset Formula $s$
\end_inset

 from 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}$
\end_inset

 in the direction of 
\begin_inset Formula $\nabla f\left(\mathbf{x}^{\left(0\right)}\right)$
\end_inset

, resulting in a new point 
\begin_inset Formula $\mathbf{x}^{\left(1\right)}$
\end_inset

.
 We can again take a step of length 
\begin_inset Formula $s$
\end_inset

 in the direction of 
\begin_inset Formula $\nabla f\left(\mathbf{x}^{\left(1\right)}\right)$
\end_inset

, repeating this procedure until the gradient of 
\begin_inset Formula $f$
\end_inset

 is zero, indicating that we have reached a critical point.
 When we implement this approach, we will normalize the direction, so that
 the step length is controlled by the step length parameter 
\begin_inset Formula $s$
\end_inset

.
 If we wish to minimize 
\begin_inset Formula $f$
\end_inset

, we will set the direction at 
\begin_inset Formula $\mathbf{x}^{\left(i\right)}$
\end_inset

 as
\begin_inset Formula 
\[
\mathbf{d}^{\left(i\right)}=-\frac{\nabla f\left(\mathbf{x}^{\left(i\right)}\right)}{\left\Vert \nabla f\left(\mathbf{x}^{\left(i\right)}\right)\right\Vert }.
\]

\end_inset

Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:steepest-descent"

\end_inset

 gives the general optimization iteration.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n},s\in\mathbb{R}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left
\backslash
Vert 
\backslash
nabla f
\backslash
left(
\backslash
mathbf{x}
\backslash
right)
\backslash
right
\backslash
Vert >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{d}\gets-\nabla f\left(\mathbf{x}\right)/\left\Vert \nabla f\left(\mathbf{x}\right)\right\Vert $
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}\gets\mathbf{x}+s\mathbf{d}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:steepest-descent"

\end_inset

Steepest Descent
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now consider the step length parameter 
\begin_inset Formula $s$
\end_inset

.
 For constant 
\begin_inset Formula $s$
\end_inset

, we may take steps that are too long, in which case we may overshoot the
 minimum, or we may take steps that are too short, in which case the algorithm
 may converge slowly.
 Rather, we would like to choose a step size 
\begin_inset Formula $s$
\end_inset

 such that 
\begin_inset Formula $s$
\end_inset

 is the distance from 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to the lowest point on the surface of 
\begin_inset Formula $f$
\end_inset

 in the direction of 
\begin_inset Formula $\mathbf{d}$
\end_inset

.
 The value of 
\begin_inset Formula $f$
\end_inset

 at that point is 
\begin_inset Formula $g\left(s\right)=f\left(\mathbf{x}+s\mathbf{d}\right),$
\end_inset

so to select 
\begin_inset Formula $s$
\end_inset

, we must minimize 
\begin_inset Formula $g$
\end_inset

.
 Taking the derivative of 
\begin_inset Formula $g$
\end_inset

 gives 
\begin_inset Formula 
\begin{flalign*}
g'\left(s\right) & =\frac{\dif}{\dif s}f\left(\mathbf{x}+s\mathbf{d}\right)\\
 & =\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\frac{\dif}{\dif s}\left(\mathbf{x}+s\mathbf{d}\right)\tag{Chain Rule}\\
 & =\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\left(\frac{\dif}{\dif s}\mathbf{x}+\frac{\dif}{\dif s}s\mathbf{d}\right)\\
 & =\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\left(\mathbf{0}+\mathbf{d}\right)\\
 & =\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\mathbf{d}.
\end{flalign*}

\end_inset

The quantity 
\begin_inset Formula $\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\mathbf{d}$
\end_inset

 is a scalar, so that to minimize 
\begin_inset Formula $g$
\end_inset

, we must solve
\begin_inset Formula 
\[
g'\left(s\right)=\nabla f\left(\mathbf{x}+s\mathbf{d}\right)\cdot\mathbf{d}=0,
\]

\end_inset

which is a one-dimensional root-finding problem.
 The solution to this equation satisfies the conditions described above
 for 
\begin_inset Formula $s$
\end_inset

.
\end_layout

\begin_layout Subsection
Root-finding
\end_layout

\begin_layout Standard
We now consider the problem of root-finding.
 Let 
\begin_inset Formula $h$
\end_inset

 be a real-valued function of 
\begin_inset Formula $s\in\mathbb{R}$
\end_inset

.
 We wish to find 
\begin_inset Formula $s^{*}$
\end_inset

 such that 
\begin_inset Formula $h\left(s^{*}\right)=0$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Bisection method
\end_layout

\begin_layout Standard
Suppose that 
\begin_inset Formula $h$
\end_inset

 has a root at 
\begin_inset Formula $s^{*}$
\end_inset

.
 Assuming that 
\begin_inset Formula $h$
\end_inset

 is continuous, it follows that there exist values 
\begin_inset Formula $s_{L}$
\end_inset

 and 
\begin_inset Formula $s_{R}$
\end_inset

 such that 
\begin_inset Formula $h\left(s_{L}\right)h\left(s_{R}\right)<0$
\end_inset

, i.e., 
\begin_inset Formula $h\left(s_{L}\right)$
\end_inset

 and 
\begin_inset Formula $h\left(s_{R}\right)$
\end_inset

 have opposite signs.
 Then, 
\begin_inset Formula $s_{L}$
\end_inset

 and 
\begin_inset Formula $s_{R}$
\end_inset

 
\emph on
bracket
\emph default
 the root at 
\begin_inset Formula $s^{*}$
\end_inset

.
 We then calculate the midpoint of the interval 
\begin_inset Formula $\left[s_{L},s_{R}\right]$
\end_inset

 as 
\begin_inset Formula $s_{M}=\left(s_{L}+s_{R}\right)/2$
\end_inset

.
 Now the root will be bracketed either by 
\begin_inset Formula $\left[s_{L},s_{M}\right]$
\end_inset

 or by 
\begin_inset Formula $\left[s_{M},s_{R}\right]$
\end_inset

; we can determine which by calculating 
\begin_inset Formula $h\left(s_{L}\right)h\left(s_{M}\right)$
\end_inset

 and 
\begin_inset Formula $h\left(s_{M}\right)h\left(s_{R}\right)$
\end_inset

.
 Taking the new bracket, we repeat the procedure until the interval is sufficien
tly small, i.e., until 
\begin_inset Formula $\left|s_{L}-s_{R}\right|<\varepsilon$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\left\{ s_{L},s_{R}:h\left(s_{L}\right)h\left(s_{R}\right)<0\right\} $
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left| s_{L}-s_{R} 
\backslash
right| >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{M}\gets\left(s_{L}+s_{R}\right)/2$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
If{$h
\backslash
left( s_{L}
\backslash
right) h
\backslash
left( s_{M}
\backslash
right) <0$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{R}\gets s_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Else
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{L}\gets s_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:root-bisection"

\end_inset

Bisection Method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observe that the bisection method requires knowledge only of 
\begin_inset Formula $h$
\end_inset

, and further that once a root is bracketed, the algorithm always converges
 (to a root).
 Bracketing a root is not in general straightforward, and convergence of
 the algorithm is slow.
\end_layout

\begin_layout Subsubsection
Newton's method
\end_layout

\begin_layout Standard
If in addition to 
\begin_inset Formula $h$
\end_inset

, we also have its derivative 
\begin_inset Formula $h'$
\end_inset

, we can overcome the difficulties of the bisection method: the idea is
 to replace 
\begin_inset Formula $h\left(s\right)$
\end_inset

 by its linear approximation.
 The equation of the line tangent to 
\begin_inset Formula $h$
\end_inset

 at 
\begin_inset Formula $s_{0}$
\end_inset

 is given by the point-slope formula, i.e., 
\begin_inset Formula $h\left(s\right)-h\left(s_{0}\right)=h'\left(s_{0}\right)\left(s-s_{0}\right)$
\end_inset

.
 Provided that 
\begin_inset Formula $s_{0}$
\end_inset

 is not a critical point of 
\begin_inset Formula $h$
\end_inset

, i.e., provided the slope of the tangent line at 
\begin_inset Formula $s_{0}$
\end_inset

 is non-zero, then the tangent line will intersect the 
\begin_inset Formula $s$
\end_inset

-axis.
 Denote this point of intersection as 
\begin_inset Formula $s_{1}$
\end_inset

, and observe that
\begin_inset Formula 
\begin{flalign*}
h\left(s_{1}\right)-h\left(s_{0}\right) & =h'\left(s_{0}\right)\left(s_{1}-s_{0}\right)\\
\implies-h\left(s_{0}\right) & =s_{1}h'\left(s_{0}\right)-s_{0}h'\left(s_{0}\right)\tag{\ensuremath{h\left(s_{1}\right)=0}}\\
\implies s_{1}h'\left(s_{0}\right) & =s_{0}h'\left(s_{0}\right)-h\left(s_{0}\right)\\
\implies s_{1} & =s_{0}-\frac{h\left(s_{0}\right)}{h'\left(s_{0}\right)}.
\end{flalign*}

\end_inset

Newton's method (also known as the Newton-Raphson method) can also be derived
 using a Taylor series expansion, as in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:newton-raphson"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $s$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left
\backslash
Vert h
\backslash
left( s
\backslash
right) 
\backslash
right
\backslash
Vert >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s\gets h\left(s\right)/h'\left(s\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Newton's Method (root-finding)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observe that Newton's method eliminates the need to first bracket the root,
 as is required by the bisection method.
 Newton's method is also fast: if 
\begin_inset Formula $s_{0}$
\end_inset

 is sufficiently close to 
\begin_inset Formula $s^{*}$
\end_inset

, then 
\begin_inset Formula $\left|s_{i+1}-s^{*}\right|<c\left|s_{i}-s^{*}\right|^{2}$
\end_inset

, i.e., Newton's method exhibits quadratic convergence.
 (By comparison, the bisection method exhibits linear convergence.) Newton's
 method presents certain challenges: it is not guaranteed to converge (it
 can 
\begin_inset Quotes eld
\end_inset

chase
\begin_inset Quotes erd
\end_inset

 roots to infinity); if 
\begin_inset Formula $h$
\end_inset

 has multiple roots, there is no control over which root is found; and the
 derivative 
\begin_inset Formula $h'$
\end_inset

 is required.
\end_layout

\begin_layout Subsubsection
Hybrid methods
\end_layout

\begin_layout Standard
Additional methods have been developed to combine the best attributes of
 both the bisection method and Newton's method while avoiding their respective
 difficulties.
 Given an initial bracket 
\begin_inset Formula $\left[s_{L},s_{R}\right]$
\end_inset

 such that 
\begin_inset Formula $s^{*}\in\left[s_{L},s_{R}\right]$
\end_inset

, 
\emph on
Brent's method
\emph default
 attempts to use Newton's method with the starting point taken as the midpoint
 of the bracket, and applies the bisection method if Newton's method fails.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\left\{ s_{L},s_{R}:h\left(s_{L}\right)h\left(s_{R}\right)<0\right\} $
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left| s_{L}-s_{R} 
\backslash
right| >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{M}\gets\left(s_{L}+s_{R}\right)/2$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\tilde{s}_{M}\gets s_{M}+h\left(s_{M}\right)/h'\left(s_{M}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
If{$
\backslash
tilde{s}_{M}
\backslash
in 
\backslash
left[ s_{L},s_{R}
\backslash
right]$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
If{$h
\backslash
left( s_{L}
\backslash
right) h
\backslash
left( 
\backslash
tilde{s}_{M}
\backslash
right) <0$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{R}\gets\tilde{s}_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Else
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{L}\gets\tilde{s}_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Else
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
If{$h
\backslash
left( s_{L}
\backslash
right) h
\backslash
left( s_{M}
\backslash
right) <0$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{R}\gets s_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Else
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s_{L}\gets s_{M}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndIf
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Brent's method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Because the derivative of 
\begin_inset Formula $h$
\end_inset

 may be difficult to compute, the 
\emph on
secant method
\emph default
 approximates the tangent line at 
\begin_inset Formula $s$
\end_inset

 (whose slope is given by 
\begin_inset Formula $h'\left(s\right)$
\end_inset

) by instead constructing a secant line.
\end_layout

\begin_layout Subsection
Newton's Method for optimization
\end_layout

\begin_layout Standard
Consider again the problem of minimizing 
\begin_inset Formula $f\left(\mathbf{x}\right):\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 by Steepest Descent.
 Suppose that 
\begin_inset Formula $f\left(\mathbf{x}\right)=10^{5}x_{1}^{2}+x_{2}^{2}$
\end_inset

, whose level curves are shown in figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-steepest-descent}
\end_layout

\end_inset

.
 The minimum of 
\begin_inset Formula $f$
\end_inset

 is easily seen to occur at 
\begin_inset Formula $\mathbf{x}^{\star}=\left(0,0\right)$
\end_inset

.
 Thus, the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 direction given some starting point 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}$
\end_inset

 is one that points toward the origin.
 Suppose that we begin the optimization with 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}=\left(0.01,-1\right)$
\end_inset

.
 The best direction is depicted as a dashed blue arrow.
 Noting that 
\begin_inset Formula $\nabla f\left(\mathbf{x}^{\left(0\right)}\right)=\left(2\cdot10^{3},-2\right)$
\end_inset

, we see that the (unnormalized) steepest descent direction 
\begin_inset Formula $\mathbf{d}^{\left(0\right)}=\left(-2\cdot10^{3},2\right)$
\end_inset

, depicted as a red arrow, is quite poor.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-steepest-descent, fig.height = 3, fig.width = 5, fig.align = 'center',
 fig.pos = 'h', fig.cap = 'Level curves of $f
\backslash

\backslash
left(
\backslash

\backslash
mathbf{x}
\backslash

\backslash
right)=10^{5}x_{1}^{2}+x_{2}^{2}$'>>=
\end_layout

\begin_layout Plain Layout

f <- function(x, y) 10^5 * x^2 + y^2
\end_layout

\begin_layout Plain Layout

x <- seq(-1e-2, 1e-2, 1e-4)
\end_layout

\begin_layout Plain Layout

y <- seq(-1, 1, 1e-2)
\end_layout

\begin_layout Plain Layout

z <- outer(x, y, f)
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,2,0.5,1))
\end_layout

\begin_layout Plain Layout

contour(x = x, y = y, z = z, levels = c(0.01, 0.1, 0.25, 0.5, 1, 2.5, 5, 10),
 
\end_layout

\begin_layout Plain Layout

        drawlabels = F, cex.lab = 0.75, cex.axis = 0.75)
\end_layout

\begin_layout Plain Layout

x0 <- c(1e-2, -1)
\end_layout

\begin_layout Plain Layout

points(x0[1], x0[2])
\end_layout

\begin_layout Plain Layout

grad.f <- function(x, y) c(2 * 10^5 * x, 2 * y)
\end_layout

\begin_layout Plain Layout

grad.x0 <- grad.f(x0[1], x0[2])
\end_layout

\begin_layout Plain Layout

x1 <- x0 + 5e-6 * -grad.x0
\end_layout

\begin_layout Plain Layout

arrows(x0 = x0[1], y0 = x0[2], x1 = 0, y1 = 0, col = "blue", lty = 2,
\end_layout

\begin_layout Plain Layout

       length = 0.1)
\end_layout

\begin_layout Plain Layout

arrows(x0 = x0[1], y0 = x0[2], x1 = x1[1], y1 = x1[2], col = "red",
\end_layout

\begin_layout Plain Layout

       length = 0.1)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Steepest Descent algorithm converges slowly in part because it gives
 poor descent directions, and in part because finding the 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

 step length 
\begin_inset Formula $s$
\end_inset

 requires finding the root of 
\begin_inset Formula $g'\left(s\right)$
\end_inset

, which is also slow.
 We would prefer to choose step size more quickly, which we can do via 
\emph on
backtracking
\emph default
.
 At each iteration, we will set the initial step length 
\begin_inset Formula $\tilde{s}$
\end_inset

 to some user-chosen constant 
\begin_inset Formula $k$
\end_inset

.
 If 
\begin_inset Formula $f\left(\mathbf{x}+\tilde{s}\mathbf{d}\right)<f\left(\mathbf{x}\right)$
\end_inset

, i.e., if taking a step from 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the direction of 
\begin_inset Formula $\mathbf{d}$
\end_inset

 and of length 
\begin_inset Formula $\tilde{s}$
\end_inset

 decreases value of 
\begin_inset Formula $f$
\end_inset

, then set 
\begin_inset Formula $s=\tilde{s}$
\end_inset

.
 If instead 
\begin_inset Formula $f\left(\mathbf{x}+\tilde{s}\mathbf{d}\right)\geq f\left(\mathbf{x}\right)$
\end_inset

, i.e., we did not descend, then divide 
\begin_inset Formula $\tilde{s}$
\end_inset

 by 2 and check whether this produces a descent step.
 Repeat this procedure until 
\begin_inset Formula $\tilde{s}$
\end_inset

 is such that a descent step is obtained.
 Although it is possible to backtrack too little or too much, in general
 backtracking outperforms the bisection method.
\end_layout

\begin_layout Standard
Having improved our choice of step length, we now turn our attention to
 improving our choice of direction.
 The general idea is to provide a 
\emph on
replacement function
\emph default
 
\begin_inset Formula $r\left(\mathbf{x}\right)$
\end_inset

 that is easy to optimize.
 A first-order Taylor series expansion for 
\begin_inset Formula $f$
\end_inset

 around the base point 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}$
\end_inset

 is 
\begin_inset Formula $f\left(\mathbf{x}\right)\approx f\left(\mathbf{x}^{\left(0\right)}\right)+\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\cdot\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)$
\end_inset

.
 Suppose that 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}=\left(0,3\right)$
\end_inset

, and that 
\begin_inset Formula $f$
\end_inset

 is such that 
\begin_inset Formula $f\left(\mathbf{x}^{\left(0\right)}\right)=6$
\end_inset

 and 
\begin_inset Formula $\nabla f\left(\mathbf{x}^{\left(0\right)}\right)=\left(1,1\right)$
\end_inset

.
 Then, 
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)\approx6+\begin{bmatrix}1\\
1
\end{bmatrix}\cdot\left(\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}-\begin{bmatrix}0\\
3
\end{bmatrix}\right)=6+\begin{bmatrix}1\\
1
\end{bmatrix}\cdot\begin{bmatrix}x_{1}\\
x_{2}
\end{bmatrix}-\begin{bmatrix}1\\
1
\end{bmatrix}\cdot\begin{bmatrix}0\\
3
\end{bmatrix}=6+x_{1}+x_{2}-3,
\]

\end_inset

which is the equation of a plane, hence has neither a maximum nor a minimum.
 To make the problem tractable, we can add a term to produce the second-order
 Taylor series expansion
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)\approx f\left(\mathbf{x}^{\left(0\right)}\right)+\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\cdot\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)^{\mathsf{T}}\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right),
\]

\end_inset

where 
\begin_inset Formula $\mathbf{H}_{f}$
\end_inset

 is the Hessian of 
\begin_inset Formula $f$
\end_inset

.
 Define the replacement function 
\begin_inset Formula 
\[
r\left(\mathbf{x}\right)=\frac{1}{2}\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)^{\mathsf{T}}\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)+\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\cdot\left(\mathbf{x}-\mathbf{x}^{\left(0\right)}\right)+f\left(\mathbf{x}^{\left(0\right)}\right)
\]

\end_inset

and observe that 
\begin_inset Formula $\mathbf{H}_{f}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 matrix, and that 
\begin_inset Formula $\nabla f$
\end_inset

 is an 
\begin_inset Formula $n$
\end_inset

-dimensional vector.
 Setting
\begin_inset Formula 
\[
\tilde{\mathbf{x}}=\mathbf{x}-\mathbf{x}^{\left(0\right)},\quad\mathbf{M}=\frac{1}{2}\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right),\quad\mathbf{b}=\nabla f\left(\mathbf{x}^{\left(0\right)}\right),\quad\text{and}\quad c=f\left(\mathbf{x}^{\left(0\right)}\right),
\]

\end_inset

we have
\begin_inset Formula 
\[
r\left(\tilde{\mathbf{x}}\right)=\tilde{\mathbf{x}}^{\mathsf{T}}\mathbf{M}\tilde{\mathbf{x}}+\mathbf{b}^{\mathsf{T}}\tilde{\mathbf{x}}+c,
\]

\end_inset

which we recognize as a general quadratic.
 Assuming that 
\begin_inset Formula $f$
\end_inset

 has continuous second derivatives, i.e., 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f\left(\mathbf{x}\right)=\frac{\partial^{2}}{\partial x_{j}\partial x_{i}},
\]

\end_inset

so that 
\begin_inset Formula $\mathbf{H}_{f}$
\end_inset

 is symmetric, it follows from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:gen-quadratic-gradient"

\end_inset

 that the gradient of 
\begin_inset Formula $r$
\end_inset

 is given by 
\begin_inset Formula $\nabla r\left(\tilde{\mathbf{x}}\right)=2\mathbf{M}\tilde{\mathbf{x}}+\mathbf{b}$
\end_inset

.
 Then, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:gen-quadratic-optimum"

\end_inset

 implies that the critical point of 
\begin_inset Formula $r$
\end_inset

 occurs at 
\begin_inset Formula 
\begin{flalign*}
\tilde{\mathbf{x}} & =-\frac{1}{2}\mathbf{M}^{-1}\mathbf{b}\\
 & =-\frac{1}{2}\left[\frac{1}{2}\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\\
 & =-\frac{1}{2}\left(2^{-1}\right)^{-1}\left[\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\tag{\ensuremath{\left(c\mathbf{A}\right)^{-1}=c^{-1}\mathbf{A}^{-1}}}\\
 & =-\left[\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\\
\implies\mathbf{x}-\mathbf{x}^{\left(0\right)} & =-\left[\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\\
\implies\mathbf{x} & =\mathbf{x}^{\left(0\right)}-\left[\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right).
\end{flalign*}

\end_inset

Observe that for 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, this provides the update 
\begin_inset Formula 
\[
x^{\left(i+1\right)}=x^{\left(i\right)}-\left[f''\left(x^{\left(i\right)}\right)\right]f'\left(x^{\left(i\right)}\right)=x^{\left(i\right)}-\frac{f'\left(x^{\left(i\right)}\right)}{f''\left(x^{\left(i\right)}\right)},
\]

\end_inset

precisely as obtained in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:newton-raphson-update"

\end_inset

.
 Thus, we see that Newton's method can be applied to optimization.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left
\backslash
Vert 
\backslash
nabla f
\backslash
left(
\backslash
mathbf{x}
\backslash
right)
\backslash
right
\backslash
Vert >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}\gets\mathbf{x}-\left[\mathbf{H}_{f}\left(\mathbf{x}\right)\right]^{-1}\nabla f\left(\mathbf{x}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Newton's method (optimization)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We see that there is no need in this case to choose a direction or step
 length.
 Although Newton's method may fail to converge altogther (as when used for
 root-finding), when it does converge, it converges rapidly.
 It is also easy to implement.
 In addition to possible non-convergence, Newton's method can converge to
 the 
\begin_inset Quotes eld
\end_inset

wrong
\begin_inset Quotes erd
\end_inset

 critical point (analogous to finding a different root than the one desired).
 Further, the Hessian may not be invertible, and even when it is, calculating
 
\begin_inset Formula $\left[\mathbf{H}_{f}\left(\mathbf{x}\right)\right]^{-1}\nabla f\left(\mathbf{x}\right)$
\end_inset

 can be time-consuming, especially in higher dimensions, and may even break
 down for sufficiently large 
\begin_inset Formula $n$
\end_inset

 (due to the requirement that the Hessian be inverted).
\end_layout

\begin_layout Example
Let 
\begin_inset Formula $f\left(\mathbf{x}\right)=3x_{1}^{2}+5x_{2}^{2}+x_{1}x_{2}+x_{1}+10x_{2}$
\end_inset

.
 Find 
\begin_inset Formula $\mathbf{x}^{*}$
\end_inset

 such that 
\begin_inset Formula $\mathbf{x}^{*}=\argmin_{\mathbf{x}}f\left(\mathbf{x}\right)$
\end_inset

.
\end_layout

\begin_layout Example
We have 
\begin_inset Formula 
\[
\nabla f\left(\mathbf{x}\right)=\begin{bmatrix}6x_{1}+x_{2}+1\\
10x_{2}+x_{1}+10
\end{bmatrix}\quad\text{and}\quad\mathbf{H}_{f}\left(\mathbf{x}\right)=\begin{bmatrix}6 & 1\\
1 & 10
\end{bmatrix}.
\]

\end_inset

Choose 
\begin_inset Formula $\mathbf{x}^{\left(0\right)}=\left(2,4\right)$
\end_inset

, so that 
\begin_inset Formula $\nabla f\left(\mathbf{x}^{\left(0\right)}\right)=\left(17,52\right)$
\end_inset

.
 Then, the Newton's method update is 
\begin_inset Formula 
\begin{flalign*}
\mathbf{x}^{\left(1\right)} & =\mathbf{x}^{\left(0\right)}-\left[\mathbf{H}_{f}\left(\mathbf{x}^{\left(0\right)}\right)\right]^{-1}\nabla f\left(\mathbf{x}^{\left(0\right)}\right)\\
 & =\begin{bmatrix}2\\
4
\end{bmatrix}-\begin{bmatrix}6 & 1\\
1 & 10
\end{bmatrix}^{-1}\begin{bmatrix}17\\
52
\end{bmatrix}\\
 & =\begin{bmatrix}2\\
4
\end{bmatrix}-\frac{1}{60-1}\begin{bmatrix}10 & -1\\
-1 & 6
\end{bmatrix}\begin{bmatrix}17\\
52
\end{bmatrix}\\
 & =\begin{bmatrix}2\\
4
\end{bmatrix}-\frac{1}{59}\begin{bmatrix}170-52\\
-17+312
\end{bmatrix}\\
 & =\begin{bmatrix}2\\
4
\end{bmatrix}-\begin{bmatrix}118/59\\
295/59
\end{bmatrix}\\
 & =\begin{bmatrix}118/59\\
236/59
\end{bmatrix}-\begin{bmatrix}118/59\\
295/59
\end{bmatrix}\\
 & =\begin{bmatrix}0\\
-1
\end{bmatrix}.
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula $\mathbf{x}^{*}=\left(0,-1\right)$
\end_inset

 minimizes 
\begin_inset Formula $f$
\end_inset

.
 Observe that Newton's Method finds the critical point in a single step
 because 
\begin_inset Formula $f$
\end_inset

 is quadratic.
\end_layout

\begin_layout Definition
Suppose that 
\begin_inset Formula $f\left(\mathbf{x}\right):\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

.
 An iterative optimization algorithm is a 
\emph on
descent method
\emph default
 if 
\begin_inset Formula $f\left(\mathbf{x}^{\left(0\right)}\right)\geq f\left(\mathbf{x}^{\left(1\right)}\right)\geq f\left(\mathbf{x}^{\left(2\right)}\right)\geq\cdots$
\end_inset

.
\end_layout

\begin_layout Standard
Descent algorithms always converge, i.e., the longer the algorithm runs, the
 closer it comes to the solution of the optimization problem.
 Observe that Steepest Descent with backtracking is a descent method, but
 that Steepest Descent with bisection is not.
 Because Newton's Method may, for example, find a maximum rather than a
 minimum, it is also not a descent method.
 To shape it into one, we must 
\begin_inset Quotes eld
\end_inset

fix
\begin_inset Quotes erd
\end_inset

 both the direction and the step length.
 Supposing for the moment that the direction is acceptable, we will fix
 the step length by 
\emph on
damping
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$
\backslash
left
\backslash
Vert 
\backslash
nabla f
\backslash
left(
\backslash
mathbf{x}
\backslash
right)
\backslash
right
\backslash
Vert >
\backslash
varepsilon$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{d}\gets-\left[\mathbf{H}_{f}\left(\mathbf{x}\right)\right]^{-1}\nabla f\left(\mathbf{x}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s\gets1$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
While{$f
\backslash
left(
\backslash
mathbf{x}
\backslash
right)<f
\backslash
left(
\backslash
mathbf{x}+s
\backslash
mathbf{d}
\backslash
right)$}
\end_layout

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $s\gets s/2$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{x}\gets\mathbf{x}+s\mathbf{d}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndWhile
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:damped-newtons-method"

\end_inset

Damped Newton's Method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Observe that the first attempt of algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:damped-newtons-method"

\end_inset

 is simply (undamped) Newton's Method, i.e., we check whether Newton's Method
 satisfies 
\begin_inset Formula $f\left(\mathbf{x}^{\left(i-i\right)}\right)\geq f\left(\mathbf{x}^{\left(i\right)}\right)$
\end_inset

, and damp (normalize) the direction only if it does not.
 Now, the direction might actually be wrong, so Damped Newton's Method is
 still not a descent algorithm.
\end_layout

\begin_layout Definition
For a function 
\begin_inset Formula $f$
\end_inset

 at a point 
\begin_inset Formula $\mathbf{x}$
\end_inset

, the direction 
\begin_inset Formula $\mathbf{d}$
\end_inset

 is a 
\emph on
descent direction
\emph default
 if there exists a value 
\begin_inset Formula $\tilde{s}$
\end_inset

 such that for all 
\begin_inset Formula $s<\tilde{s}$
\end_inset

, 
\begin_inset Formula $f\left(\mathbf{x}+s\mathbf{d}\right)<f\left(\mathbf{x}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Noting that the step length 
\begin_inset Formula $s$
\end_inset

 will be 
\begin_inset Quotes eld
\end_inset

small,
\begin_inset Quotes erd
\end_inset

 consider the Taylor series expansion of 
\begin_inset Formula $f\left(\mathbf{x}+s\mathbf{d}\right)$
\end_inset

 around the base point 
\begin_inset Formula $\mathbf{x}$
\end_inset

 
\begin_inset Formula 
\[
f\left(\mathbf{x}+s\mathbf{d}\right)\approx f\left(\mathbf{x}\right)+\nabla f\left(\mathbf{x}\right)\cdot\left(s\mathbf{d}\right)+\varepsilon,
\]

\end_inset

where the error term 
\begin_inset Formula $\varepsilon$
\end_inset

 will be small, on the order of 
\begin_inset Formula $\mathcal{O}\left(s^{2}\right)$
\end_inset

.
 Then, 
\begin_inset Formula $\mathbf{d}$
\end_inset

 will satisfy 
\begin_inset Formula $f\left(\mathbf{x}+s\mathbf{d}\right)<f\left(\mathbf{x}\right)$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{d}$
\end_inset

 will be a descent direction, if 
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)>f\left(\mathbf{x}+s\mathbf{d}\right)\approx f\left(\mathbf{x}\right)+\nabla f\left(\mathbf{x}\right)\cdot\left(s\mathbf{d}\right)=f\left(\mathbf{x}\right)+s\nabla f\left(\mathbf{x}\right)\cdot\mathbf{d}\implies s\nabla f\left(\mathbf{x}\right)\cdot\mathbf{d}<0.
\]

\end_inset

Now, the step length 
\begin_inset Formula $s$
\end_inset

 is strictly positive, so this inequality will hold if and only if 
\begin_inset Formula $\nabla f\left(\mathbf{x}\right)\cdot\mathbf{d}<0$
\end_inset

, i.e., if the dot product of the gradient of 
\begin_inset Formula $f$
\end_inset

 and the direction 
\begin_inset Formula $\mathbf{d}$
\end_inset

 is negative.
\end_layout

\begin_layout Subsection
Quadratic optimization
\end_layout

\begin_layout Standard
Recall that an 
\begin_inset Formula $n$
\end_inset

-dimensional quadratic has the form 
\begin_inset Formula $f\left(\mathbf{x}\right)=\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c$
\end_inset

, where 
\begin_inset Formula $\mathbf{A}$
\end_inset

 can be taken as symmetric.
 We would like to know under what circumstances 
\begin_inset Formula $f$
\end_inset

 will have a maximum, a minimum, or a saddle point, i.e., the characteristics
 of the critical point.
 Suppose that 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{2}$
\end_inset

, suppose for the moment that 
\begin_inset Formula $f$
\end_inset

 has no cross-term, and let 
\begin_inset Formula $f\left(\mathbf{x}\right)=x_{1}^{2}+2x_{2}^{2}$
\end_inset

.
 It is clear that 
\begin_inset Formula $f$
\end_inset

 is a paraboloid opening up, hence the critical point of 
\begin_inset Formula $f$
\end_inset

 is a minimum.
 If we instead let 
\begin_inset Formula $f\left(\mathbf{x}\right)=-x_{1}^{2}-2x_{2}^{2}$
\end_inset

, then 
\begin_inset Formula $f$
\end_inset

 is a paraboloid opening down, hence the critical point of 
\begin_inset Formula $f$
\end_inset

 is a maximum.
 If we let 
\begin_inset Formula $f\left(\mathbf{x}\right)=x_{1}^{2}-2x_{2}^{2}$
\end_inset

, then the critical point of 
\begin_inset Formula $f$
\end_inset

 is a saddle point.
 Now suppose that 
\begin_inset Formula $f\left(\mathbf{x}\right)=x_{1}^{2}+6x_{1}x_{2}+3x_{2}^{2}$
\end_inset

.
 
\begin_inset Formula $f$
\end_inset

 now has a cross-term, and it is not immediately clear how to characterize
 the critical point.
 We will see that 
\emph on
all
\emph default
 quadratics have no cross-term, given an appropriate transformation.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Spectral Theorem]
\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "thm:spectral"

\end_inset

Let 
\begin_inset Formula $\mathbf{A}$
\end_inset

 be an 
\begin_inset Formula $n\times n$
\end_inset

 real, symmetric matrix.
 Then, 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has an orthonormal basis of eigenvectors.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Corollary
Suppose that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 real, symmetric matrix, and let 
\begin_inset Formula $\mathbf{q}^{\left(1\right)},\mathbf{q}^{\left(2\right)},\ldots,\mathbf{q}^{\left(n\right)}$
\end_inset

 be the eigenvectors corresponding to the ordered eigenvalues 
\begin_inset Formula $\left|\lambda_{1}\right|>\left|\lambda_{2}\right|>\cdots>\left|\lambda_{n}\right|$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 be the matrix whose 
\begin_inset Formula $j\text{th}$
\end_inset

 column is given by 
\begin_inset Formula $\mathbf{q}^{\left(j\right)}$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{Q}=\begin{bmatrix}\mathbf{q}^{\left(1\right)} & \mathbf{q}^{\left(2\right)} & \cdots & \mathbf{q}^{\left(n\right)}\end{bmatrix}$
\end_inset

, and let 
\begin_inset Formula $\mathbf{D}$
\end_inset

 be the diagonal matrix whose 
\begin_inset Formula $i\text{th}$
\end_inset

 diagonal entry is 
\begin_inset Formula $\lambda_{i}$
\end_inset

.
 Then, 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has the decomposition 
\begin_inset Formula $\mathbf{A}=\mathbf{Q}\mathbf{D}\mathbf{Q}^{\mathsf{T}}$
\end_inset

.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\left\{ \mathbf{e}^{\left(i\right)}\right\} _{i=1}^{n}$
\end_inset

 be the standard basis vectors for 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

, i.e., the 
\begin_inset Formula $i\text{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{e}^{\left(i\right)}$
\end_inset

 is 1 and all other components are zero.
 Then, any 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 can be represented as a linear combination of the 
\begin_inset Formula $\mathbf{e}^{\left(i\right)}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{bmatrix}=x_{1}\mathbf{e}^{\left(1\right)}+x_{2}\mathbf{e}^{\left(2\right)}+\cdots+x_{n}\mathbf{e}^{\left(n\right)}=\sum_{i=1}^{n}x_{i}\mathbf{e}^{\left(i\right)}.
\]

\end_inset

Consider again the general quadratic, and observe that the matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 determines the behavior of 
\begin_inset Formula $f$
\end_inset

.
 For simplicity, we consider only the initial term, i.e., 
\begin_inset Formula $f\left(\mathbf{x}\right)=\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}$
\end_inset

.
 Then, we can express 
\begin_inset Formula $f$
\end_inset

 in terms of the 
\begin_inset Formula $\mathbf{e}^{\left(i\right)}$
\end_inset

, i.e., 
\begin_inset Formula 
\begin{flalign*}
f\left(\mathbf{x}\right) & =f\left(\sum_{i=1}^{n}x_{i}\mathbf{e}^{\left(i\right)}\right)\\
 & =\left(\sum_{i=1}^{n}x_{i}\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\left(\sum_{j=1}^{n}x_{j}\mathbf{e}^{\left(j\right)}\right)\\
 & =\left(\sum_{i=1}^{n}x_{i}\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\right)\mathbf{A}\left(\sum_{j=1}^{n}x_{j}\mathbf{e}^{\left(j\right)}\right)\tag{\ensuremath{\left(\mathbf{u}+\mathbf{v}\right)^{\mathsf{T}}=\mathbf{u}^{\mathsf{T}}+\mathbf{v}^{\mathsf{T}}}}\\
 & =\left(\sum_{i=1}^{n}x_{i}\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\right)\left(\sum_{j=1}^{n}x_{j}\mathbf{A}\mathbf{e}^{\left(j\right)}\right)\tag{\ensuremath{\mathbf{M}\left(\mathbf{u}+\mathbf{v}\right)=\mathbf{M}\mathbf{u}+\mathbf{M}\mathbf{v}}}\\
 & =\sum_{i=1}^{n}x_{i}\sum_{j=1}^{n}x_{j}\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{e}^{\left(j\right)}\\
 & =\sum_{i=1}^{n}\sum_{j=1}^{n}x_{i}x_{j}\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{e}^{\left(j\right)}.
\end{flalign*}

\end_inset

Now, right-multiplying 
\begin_inset Formula $\mathbf{A}$
\end_inset

 by 
\begin_inset Formula $\mathbf{e}^{\left(j\right)}$
\end_inset

 will extract the 
\begin_inset Formula $j\text{th}$
\end_inset

 column of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, so that each product 
\begin_inset Formula $\mathbf{A}\mathbf{e}^{\left(j\right)}$
\end_inset

 will be 
\begin_inset Formula $j\text{th}$
\end_inset

 column of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\mathbf{A}\mathbf{e}^{\left(j\right)}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}\begin{bmatrix}0\\
\vdots\\
1\\
\vdots\\
0
\end{bmatrix}=\begin{bmatrix}a_{1j}\\
a_{2j}\\
\vdots\\
a_{nj}
\end{bmatrix}.
\]

\end_inset

Then, left-multiplying 
\begin_inset Formula $\mathbf{A}\mathbf{e}^{\left(j\right)}$
\end_inset

 by 
\begin_inset Formula $\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}$
\end_inset

 will extract the 
\begin_inset Formula $i\text{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{A}\mathbf{e}^{\left(j\right)}$
\end_inset

, so that each product 
\begin_inset Formula $\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{e}^{\left(j\right)}$
\end_inset

 will be the 
\begin_inset Formula $\left(i,j\right)\text{th}$
\end_inset

 component of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, i.e.,
\begin_inset Formula 
\[
\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{e}^{\left(j\right)}=\begin{bmatrix}0 & \cdots & 1 & \cdots & 0\end{bmatrix}\begin{bmatrix}a_{1j}\\
a_{2j}\\
\vdots\\
a_{nj}
\end{bmatrix}=a_{ij}.
\]

\end_inset

Then, our expression for 
\begin_inset Formula $f\left(\mathbf{x}\right)$
\end_inset

 becomes
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}x_{i}x_{j}\left(\mathbf{e}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{e}^{\left(j\right)}=\sum_{i=1}^{n}\sum_{j=1}^{n}x_{i}x_{j}a_{ij}.
\]

\end_inset


\end_layout

\begin_layout Example
We now consider when cross-terms appear in 
\begin_inset Formula $f$
\end_inset

.
 Suppose that 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{2}$
\end_inset

, and let
\begin_inset Formula 
\[
\mathbf{A}=\begin{bmatrix}1 & 3\\
3 & 2
\end{bmatrix}\implies f\left(\mathbf{x}\right)=\sum_{i=1}^{2}\sum_{j=1}^{2}x_{i}x_{j}a_{ij}=x_{1}^{2}\cdot1+x_{1}x_{2}\cdot3+x_{2}x_{1}\cdot3+x_{2}^{2}\cdot2=x_{1}^{2}+6x_{1}x_{2}+2x_{2}^{2}.
\]

\end_inset

We see that 
\begin_inset Formula $f$
\end_inset

 contains the cross-term 
\begin_inset Formula $6x_{1}x_{2}$
\end_inset

.
 It is clear that 
\begin_inset Formula $f$
\end_inset

 would not have a cross-term if 
\begin_inset Formula $\mathbf{A}$
\end_inset

 were diagonal.
 Now, 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is an 
\begin_inset Formula $n\times n$
\end_inset

 real, symmetric matrix, so 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:spectral"

\end_inset

 implies that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has an orthonormal basis of eigenvectors, which we denote by 
\begin_inset Formula $\left\{ \mathbf{q}^{\left(i\right)}\right\} _{i=1}^{n}$
\end_inset

.
 Because the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 form a basis for 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

, it follows that we can express 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in terms of the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 with corresponding weights 
\begin_inset Formula $\left\{ w_{i}\right\} _{i=1}^{n}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\mathbf{x}=w_{1}\mathbf{q}^{\left(1\right)}+w_{2}\mathbf{q}^{\left(2\right)}+\cdots+w_{n}\mathbf{q}^{\left(n\right)}=\sum_{i=1}^{n}w_{i}\mathbf{q}^{\left(i\right)}.
\]

\end_inset

Then, our result above implies that 
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=f\left(\sum_{i=1}^{n}w_{i}\mathbf{q}^{\left(i\right)}\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}\left(\mathbf{q}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{A}\mathbf{q}^{\left(j\right)}.
\]

\end_inset

Now, 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 is an eigenvector of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 with corresponding eigenvalue 
\begin_inset Formula $\lambda_{i}$
\end_inset

, so that 
\begin_inset Formula $\mathbf{A}\mathbf{q}^{\left(j\right)}=\lambda_{j}\mathbf{q}^{\left(j\right)}$
\end_inset

, hence
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}\left(\mathbf{q}^{\left(i\right)}\right)^{\mathsf{T}}\lambda_{j}\mathbf{q}^{\left(j\right)}=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}\lambda_{j}\left(\mathbf{q}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{q}^{\left(j\right)}.
\]

\end_inset

Because the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 are orthonormal, we will have 
\begin_inset Formula $\left(\mathbf{q}^{\left(i\right)}\right)^{\mathsf{T}}\mathbf{q}^{\left(j\right)}=1$
\end_inset

 in the case that 
\begin_inset Formula $i=j$
\end_inset

 and zero otherwise.
 Thus,
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=w_{i}\cdot w_{i}\lambda_{i}\cdot1=w_{i}^{2}\lambda_{i}.
\]

\end_inset

The characteristic polynomial of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is 
\begin_inset Formula 
\begin{flalign*}
\det\left(\mathbf{A}-\lambda\mathbf{I}_{2}\right) & =\det\left(\begin{bmatrix}1 & 3\\
3 & 2
\end{bmatrix}-\begin{bmatrix}\lambda & 0\\
0 & \lambda
\end{bmatrix}\right)\\
 & =\det\left(\begin{bmatrix}1-\lambda & 3\\
3 & 2-\lambda
\end{bmatrix}\right)\\
 & =\left(1-\lambda\right)\left(2-\lambda\right)-9\\
 & =2-\lambda-2\lambda+\lambda^{2}-9\\
 & =\lambda^{2}-3\lambda-7.
\end{flalign*}

\end_inset

Solving for 
\begin_inset Formula $\lambda$
\end_inset

 gives
\begin_inset Formula 
\[
\lambda=\frac{3\pm\sqrt{9-4\left(-7\right)}}{2}=\frac{3\pm\sqrt{37}}{2}\implies\lambda_{1}=\frac{3+\sqrt{37}}{2}\approx\Sexpr{(3+sqrt(37))/2},\ \lambda_{2}=\frac{3-\sqrt{37}}{2}\approx\Sexpr{(3-sqrt(37))/2},
\]

\end_inset

so that expressing 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the 
\begin_inset Formula $\mathbf{q}$
\end_inset

-basis gives 
\begin_inset Formula 
\[
f\left(\mathbf{x}\right)=x_{1}^{2}+6x_{1}x_{2}+2x_{2}^{2}\iff f\left(\mathbf{w}\right)=\Sexpr{round((3+sqrt(37))/2,1)}w_{1}^{2}\Sexpr{round((3-sqrt(37))/2,1)}w_{2}^{2}.
\]

\end_inset

We see that when we express 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the 
\begin_inset Formula $\mathbf{q}$
\end_inset

-basis, our expression for 
\begin_inset Formula $f\left(\mathbf{w}\right)$
\end_inset

 does not contain a cross-term.
 Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-spectral}
\end_layout

\end_inset

 shows the standard and 
\begin_inset Formula $\mathbf{q}$
\end_inset

 bases for 
\begin_inset Formula $f\left(\mathbf{x}\right)$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-spectral, fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos
 = 'h', fig.cap = 'Standard and $
\backslash

\backslash
mathbf{q}$ bases for $f
\backslash

\backslash
left(
\backslash

\backslash
mathbf{x}
\backslash

\backslash
right)$'>>=
\end_layout

\begin_layout Plain Layout

lambda <- eigen(matrix(c(1,3,3,2), nr = 2))$vectors
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(0.5,1,0.5,1))
\end_layout

\begin_layout Plain Layout

plot.new()
\end_layout

\begin_layout Plain Layout

plot.window(xlim = c(-1, 1.1), ylim = c(0, 1.2), asp = 1)
\end_layout

\begin_layout Plain Layout

arrows(x0 = 0, y0 = 0, x1 = 1, y1 = 0, length = 0.1, col = "black")
\end_layout

\begin_layout Plain Layout

arrows(x0 = 0, y0 = 0, x1 = 0, y1 = 1, length = 0.1, col = "black")
\end_layout

\begin_layout Plain Layout

arrows(x0 = 0, y0 = 0, x1 = lambda[1,1], y1 = lambda[2,1], length = 0.1,
 
\end_layout

\begin_layout Plain Layout

       lty = 2, col = "blue")
\end_layout

\begin_layout Plain Layout

arrows(x0 = 0, y0 = 0, x1 = lambda[1,2], y1 = lambda[2,2], length = 0.1,
 
\end_layout

\begin_layout Plain Layout

       lty = 2, col = "blue")
\end_layout

\begin_layout Plain Layout

text(x = 1.05, y = 0.05, cex = 0.75, labels = expression(e^(1)))
\end_layout

\begin_layout Plain Layout

text(x = 0.05, y = 1.05, cex = 0.75, labels = expression(e^(2)))
\end_layout

\begin_layout Plain Layout

text(x = lambda[1,1] + 0.05, y = lambda[2,1] + 0.05, cex = 0.75, 
\end_layout

\begin_layout Plain Layout

     labels = expression(q^(1)), col = "blue")
\end_layout

\begin_layout Plain Layout

text(x = lambda[1,2] - 0.05, y = lambda[2,2] + 0.05, cex = 0.75, 
\end_layout

\begin_layout Plain Layout

     labels = expression(q^(2)), col = "blue")
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can apply the same idea to other quadratics, e.g., the regression least-squares
 loss function, where 
\begin_inset Formula $\mathbf{A}=\mathbf{B}^{\mathsf{T}}\mathbf{B}$
\end_inset

 for some design matrix 
\begin_inset Formula $\mathbf{B}$
\end_inset

.
 Computing the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 also provides information about the nature of the critical point, which
 we state in the following theorem.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f$
\end_inset

 be an 
\begin_inset Formula $n$
\end_inset

-dimensional quadratic, i.e., 
\begin_inset Formula $f\left(\mathbf{x}\right)=\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}+\mathbf{b}^{\mathsf{T}}\mathbf{x}+c$
\end_inset

, and let 
\begin_inset Formula $\left\{ \lambda_{i}\right\} _{i=1}^{n}$
\end_inset

 be the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

.
 Then, the critical point of 
\begin_inset Formula $f$
\end_inset

 is 
\end_layout

\begin_deeper
\begin_layout Enumerate
a maximum if all the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 are negative, i.e., if 
\begin_inset Formula $\left\{ \lambda_{i}:\lambda_{i}<0\ \forall i\right\} $
\end_inset

;
\end_layout

\begin_layout Enumerate
a minimum if all the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 are positive, i.e., if 
\begin_inset Formula $\left\{ \lambda_{i}:\lambda_{i}>0\ \forall i\right\} $
\end_inset

;
\end_layout

\begin_layout Enumerate
a saddle point if the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 are of mixed sign.
\end_layout

\end_deeper
\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Definition
An 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is 
\emph on
positive definite
\emph default
 if 
\begin_inset Formula $\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}>0$
\end_inset

 for all 
\begin_inset Formula $\left\{ \mathbf{x}:\mathbf{x}\in\mathbb{R}^{n},\mathbf{x}\neq\mathbf{0}\right\} $
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is positive definite, then for every nonzero 
\begin_inset Formula $\mathbf{x}$
\end_inset

, the quantity 
\begin_inset Formula $\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}$
\end_inset

 will be positive, which implies that the critical point of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is a minimum, hence all the eigenvalues of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 are positive.
\end_layout

\begin_layout Proposition
An 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is positive definite if and only if 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has all positive eigenvalues.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
Now consider the action of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 on a nonzero vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is positive definite, then 
\begin_inset Formula $0<\mathbf{x}^{\mathsf{T}}\mathbf{A}\mathbf{x}=\mathbf{x}\cdot\mathbf{A}\mathbf{x}$
\end_inset

.
 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is nonzero, so the dot product of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{A}\mathbf{x}$
\end_inset

 will be positive only in the case that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 does not 
\begin_inset Quotes eld
\end_inset

flip
\begin_inset Quotes erd
\end_inset

 the direction of 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Standard
Recall that the inner while loop in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:damped-newtons-method"

\end_inset

 will exit only when 
\begin_inset Formula $f\left(\mathbf{x}+s\mathbf{d}\right)$
\end_inset

 is less than or equal to 
\begin_inset Formula $f\left(\mathbf{x}\right)$
\end_inset

, i.e., when the step length 
\begin_inset Formula $s$
\end_inset

 results in 
\begin_inset Quotes eld
\end_inset

downhill
\begin_inset Quotes erd
\end_inset

 (or at least 
\begin_inset Quotes eld
\end_inset

flat
\begin_inset Quotes erd
\end_inset

) movement.
 This will occur only in the case that 
\begin_inset Formula $\mathbf{d}$
\end_inset

 is a descent direction, so it becomes natural to ask which choices of 
\begin_inset Formula $\mathbf{d}$
\end_inset

 will provide descent directions.
 Recall that the dot product of two vectors 
\begin_inset Formula $\mathbf{u}$
\end_inset

 and 
\begin_inset Formula $\mathbf{v}$
\end_inset

 can be written as 
\begin_inset Formula $\mathbf{u}\cdot\mathbf{v}=\left\Vert \mathbf{u}\right\Vert \left\Vert \mathbf{v}\right\Vert \cos\theta$
\end_inset

, where 
\begin_inset Formula $\theta$
\end_inset

 is the angle formed by 
\begin_inset Formula $\mathbf{u}$
\end_inset

 and 
\begin_inset Formula $\mathbf{v}$
\end_inset

.
 Noting that the gradient of 
\begin_inset Formula $f$
\end_inset

 evaluated at some point 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

 points in the direction of steepest increase of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

, it follows that a descent direction will be a direction 
\begin_inset Quotes eld
\end_inset

opposite
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\nabla f\left(\tilde{\mathbf{x}}\right)$
\end_inset

, i.e., a direction for which 
\begin_inset Formula $\mathbf{d}\cdot\nabla f\left(\tilde{\mathbf{x}}\right)<0$
\end_inset

, which will occur when 
\begin_inset Formula $\theta\in\left(\pi/2,3\pi/2\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Now, Damped Newton's Method defines the direction at a point 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

 to be 
\begin_inset Formula $\mathbf{d}=-\left[\mathbf{H}_{f}\left(\tilde{\mathbf{x}}\right)\right]^{-1}\nabla f\left(\tilde{\mathbf{x}}\right)$
\end_inset

.
 The Hessian of a general quadratic 
\begin_inset Formula $f$
\end_inset

 can be shown to be equal (possibly up to a constant, depending on the exact
 form of 
\begin_inset Formula $f$
\end_inset

) to the matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

, where 
\begin_inset Formula $\mathbf{A}$
\end_inset

 does not depend on 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Thus, 
\begin_inset Formula $\mathbf{d}=-\mathbf{A}^{-1}\nabla f\left(\tilde{\mathbf{x}}\right)$
\end_inset

, which implies that 
\begin_inset Formula $\mathbf{d}$
\end_inset

 will be a descent direction if
\begin_inset Formula 
\begin{flalign*}
0 & >\left(-\mathbf{A}^{-1}\nabla f\left(\tilde{\mathbf{x}}\right)\right)\cdot\nabla f\left(\tilde{\mathbf{x}}\right)\\
\implies0 & <\mathbf{A}^{-1}\nabla f\left(\tilde{\mathbf{x}}\right)\cdot\nabla f\left(\tilde{\mathbf{x}}\right)\\
 & =\left[\mathbf{A}^{-1}\nabla f\left(\tilde{\mathbf{x}}\right)\right]^{\mathsf{T}}\nabla f\left(\tilde{\mathbf{x}}\right)\\
 & =\left[\nabla f\left(\tilde{\mathbf{x}}\right)\right]^{\mathsf{T}}\left(\mathbf{A}^{-1}\right)^{\mathsf{T}}\nabla f\left(\tilde{\mathbf{x}}\right)\\
 & =\left[\nabla f\left(\tilde{\mathbf{x}}\right)\right]^{\mathsf{T}}\left(\mathbf{A}^{\mathsf{T}}\right)^{\mathsf{T}}\nabla f\left(\tilde{\mathbf{x}}\right)\tag{\ensuremath{\mathbf{A}} is symmetric}\\
 & =\left[\nabla f\left(\tilde{\mathbf{x}}\right)\right]^{\mathsf{T}}\mathbf{A}\nabla f\left(\tilde{\mathbf{x}}\right).
\end{flalign*}

\end_inset

The quantity 
\begin_inset Formula $\left[\nabla f\left(\tilde{\mathbf{x}}\right)\right]^{\mathsf{T}}\mathbf{A}\nabla f\left(\tilde{\mathbf{x}}\right)$
\end_inset

 will be positive for all 
\begin_inset Formula $\nabla f\left(\tilde{\mathbf{x}}\right)\neq\mathbf{0}$
\end_inset

 in the case that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is positive definite, and in this case 
\begin_inset Formula $\mathbf{d}$
\end_inset

 will be a descent direction.
\end_layout

\begin_layout Proposition
Let 
\begin_inset Formula $f$
\end_inset

 be a twice-differentiable function of 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 with Hessian 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

, and let 
\begin_inset Formula $\mathbf{d}$
\end_inset

 be the Damped Newton's Method direction at a point 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

.
 Then,
\end_layout

\begin_deeper
\begin_layout Enumerate
if 
\begin_inset Formula $\left[\mathbf{H}_{f}\left(\tilde{\mathbf{x}}\right)\right]^{-1}$
\end_inset

 is positive definite, then 
\begin_inset Formula $\mathbf{d}$
\end_inset

 is a descent direction.
\end_layout

\begin_layout Enumerate
if 
\begin_inset Formula $\mathbf{H}_{f}\left(\tilde{\mathbf{x}}\right)$
\end_inset

 is positive definite, then 
\begin_inset Formula $\mathbf{d}$
\end_inset

 is a descent direction.
\end_layout

\end_deeper
\begin_layout Standard
We are now ready to state a result that extends Newton's Method in the case
 that the Hessian of the function 
\begin_inset Formula $f$
\end_inset

 is positive definite.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f$
\end_inset

 be a twice-differentiable function of 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 with Hessian 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

 is positive definite for all 
\begin_inset Formula $\mathbf{x}\neq\mathbf{0}$
\end_inset

, then Newton's Method always gives descent directions.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
Thus, if the Hessian of 
\begin_inset Formula $f$
\end_inset

 is positive definite, then Damped Newton's Method is a descent algorithm.
\end_layout

\begin_layout Subsection
Convexity
\end_layout

\begin_layout Definition
Suppose that 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

, and let 
\begin_inset Formula $\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\lambda\in\left[0,1\right]$
\end_inset

.
 Then,
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $f$
\end_inset

 is 
\emph on
convex
\emph default
 if 
\begin_inset Formula $f\left(\lambda\mathbf{x}+\left(1-\lambda\right)\mathbf{y}\right)\leq\lambda f\left(\mathbf{x}\right)+\left(1-\lambda\right)f\left(\mathbf{y}\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
\begin_inset Formula $f$
\end_inset

 is 
\emph on
concave
\emph default
 if 
\begin_inset Formula $f\left(\lambda\mathbf{x}+\left(1-\lambda\right)\mathbf{y}\right)\geq\lambda f\left(\mathbf{x}\right)+\left(1-\lambda\right)f\left(\mathbf{y}\right)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
Geometrically, if the function 
\begin_inset Formula $f$
\end_inset

 is plotted, and a line is drawn between the points 
\begin_inset Formula $\left(\mathbf{x},f\left(\mathbf{x}\right)\right)$
\end_inset

 and 
\begin_inset Formula $\left(\mathbf{y},f\left(\mathbf{y}\right)\right)$
\end_inset

, 
\begin_inset Formula $f$
\end_inset

 will be convex if the line is always 
\begin_inset Quotes eld
\end_inset

above
\begin_inset Quotes erd
\end_inset

 the graph of 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Example
Concretely, suppose that 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, and let 
\begin_inset Formula $f\left(x\right)=x^{2}$
\end_inset

.
 Suppose that 
\begin_inset Formula $x_{1}=-0.5$
\end_inset

 and 
\begin_inset Formula $x_{2}=1$
\end_inset

, so that 
\begin_inset Formula $f\left(x_{1}\right)=0.25$
\end_inset

 and 
\begin_inset Formula $f\left(x_{2}\right)=1$
\end_inset

.
 Figure 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:ex-convex}
\end_layout

\end_inset

 shows 
\begin_inset Formula $f$
\end_inset

 and the points 
\begin_inset Formula $\left(x_{1},f\left(x_{1}\right)\right)$
\end_inset

 and 
\begin_inset Formula $\left(x_{2},f\left(x_{2}\right)\right)$
\end_inset

.
 Observe that the (dashed) line connecting these points lies above the graph
 of 
\begin_inset Formula $f$
\end_inset

.
 It is easy to see that this will be true for any choices of 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

.
 
\end_layout

\begin_layout Example
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ex-convex, fig.height = 3, fig.width = 5, fig.align = 'center', fig.pos =
 'h', fig.cap = '$f
\backslash

\backslash
left(x
\backslash

\backslash
right)$ is convex'>>=
\end_layout

\begin_layout Plain Layout

par(mgp = c(1.5,0.5,0), mar = c(2.5,2.5,0.5,1))
\end_layout

\begin_layout Plain Layout

curve(x^2, xlim = c(-2, 2), ylab = "f(x)", cex.lab = 0.75, 
\end_layout

\begin_layout Plain Layout

      xaxt = "n", yaxt = "n")
\end_layout

\begin_layout Plain Layout

x1 <- c(-0.5, 0.25)
\end_layout

\begin_layout Plain Layout

x2 <- c(1, 1)
\end_layout

\begin_layout Plain Layout

axis(side = 1, at = c(x1[1], x2[1]), cex.axis = 0.75, 
\end_layout

\begin_layout Plain Layout

     labels = c(expression(x[1]), expression(x[2])))
\end_layout

\begin_layout Plain Layout

points(x = c(-0.5, 1), y = c(0.25, 1))
\end_layout

\begin_layout Plain Layout

segments(x0 = -0.5, y0 = 0.25, x1 = 1, y1 = 1, lty = 2)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Example
We also have 
\begin_inset Formula 
\begin{flalign*}
f\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right) & =\left(\lambda x_{1}+\left(1-\lambda\right)x_{2}\right)^{2}\\
 & =\lambda^{2}x_{1}^{2}+2\left(\lambda x_{1}\right)\left(1-\lambda\right)x_{2}+\left(1-\lambda\right)^{2}x_{2}^{2}\\
 & =\lambda^{2}x_{1}^{2}+2\lambda x_{1}\left(x_{2}-\lambda x_{2}\right)+\left(1-2\lambda+\lambda^{2}\right)x_{2}^{2}\\
 & =\lambda^{2}x_{1}^{2}+2\lambda x_{1}x_{2}-2\lambda^{2}x_{1}x_{2}+x_{2}^{2}-2\lambda x_{2}^{2}+\lambda^{2}x_{2}^{2}
\end{flalign*}

\end_inset

and 
\begin_inset Formula 
\[
\lambda f\left(x_{1}\right)+\left(1-\lambda\right)f\left(x_{2}\right)=\lambda x_{1}^{2}+\left(1-\lambda\right)x_{2}^{2}=\lambda x_{1}^{2}+x_{2}^{2}-\lambda x_{2}^{2}.
\]

\end_inset

Then, 
\begin_inset Formula $f$
\end_inset

 will be convex if 
\begin_inset Formula 
\[
\lambda^{2}x_{1}^{2}+2\lambda x_{1}x_{2}-2\lambda^{2}x_{1}x_{2}+x_{2}^{2}-2\lambda x_{2}^{2}+\lambda^{2}x_{2}^{2}\leq\lambda x_{1}^{2}+x_{2}^{2}-\lambda x_{2}^{2}
\]

\end_inset

for all 
\begin_inset Formula $x_{1},x_{2}\in\mathbb{R}$
\end_inset

 and all 
\begin_inset Formula $\lambda\in\left[0,1\right]$
\end_inset

.
 Suppose that the inequality holds, so that 
\begin_inset Formula 
\begin{flalign*}
0 & \leq\lambda x_{1}^{2}+x_{2}^{2}-\lambda x_{2}^{2}-\lambda^{2}x_{1}^{2}-2\lambda x_{1}x_{2}+2\lambda^{2}x_{1}x_{2}-x_{2}^{2}+2\lambda x_{2}^{2}-\lambda^{2}x_{2}^{2}\\
 & =\left(1-\lambda\right)\lambda x_{1}^{2}+\lambda x_{2}^{2}-2\left(1-\lambda\right)\lambda x_{1}x_{2}-\lambda^{2}x_{2}^{2}\\
 & =\left(1-\lambda\right)\lambda x_{1}^{2}+\left(1-\lambda\right)\lambda x_{2}^{2}-2\left(1-\lambda\right)\lambda x_{1}x_{2}\\
 & =\lambda\left(1-\lambda\right)\left(x_{1}^{2}+x_{2}^{2}-2x_{1}x_{2}\right)\\
 & =\lambda\left(1-\lambda\right)\left(x_{1}-x_{2}\right)^{2}.
\end{flalign*}

\end_inset

We have 
\begin_inset Formula $\lambda\in\left[0,1\right]$
\end_inset

, so that both 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $1-\lambda$
\end_inset

 will be nonnegative.
 Then, for any real numbers 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

, the quantity 
\begin_inset Formula $\left(x_{1}-x_{2}\right)^{2}$
\end_inset

 will be nonnegative.
 It follows that the product 
\begin_inset Formula $\lambda\left(1-\lambda\right)\left(x_{1}-x_{2}\right)^{2}$
\end_inset

 will be nonnegative for all 
\begin_inset Formula $x_{1},x_{2}\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $\lambda\in\left[0,1\right]$
\end_inset

, so that the inequality holds, and it follows that 
\begin_inset Formula $f$
\end_inset

 is convex.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:convex-positive-definite"

\end_inset

Let 
\begin_inset Formula $f$
\end_inset

 be a twice-differentiable function of 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 with Hessian 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

.
 Then, 
\begin_inset Formula $f$
\end_inset

 is convex if and only if 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

 is positive definite.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
Observe that if 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, the equivalent condition for convexity is that 
\begin_inset Formula $f''\left(x\right)>0$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset space ~
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Damped Newton's Method applied to a convex function is a descent algorithm.
\end_layout

\begin_layout Enumerate
If a function 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 has a minimum, then Damped Newton's Method with converge to the minimum.
\end_layout

\end_deeper
\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
We now state some important facts.
\end_layout

\begin_layout Proposition
Suppose that 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $f$
\end_inset

 is convex, then 
\begin_inset Formula $-f$
\end_inset

 is concave.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

 are convex, then 
\begin_inset Formula $c_{1}f\left(\mathbf{x}\right)+c_{2}g\left(\mathbf{x}\right)$
\end_inset

 is convex if 
\begin_inset Formula $c_{1},c_{2}>0$
\end_inset

.
\end_layout

\begin_layout Enumerate
A linear function is both concave and convex.
\end_layout

\end_deeper
\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
In the absence of constraints, convex optimization is a 
\begin_inset Quotes eld
\end_inset

solved
\begin_inset Quotes erd
\end_inset

 problem, whereas non-convex optimization remains a 
\begin_inset Quotes eld
\end_inset

hard
\begin_inset Quotes erd
\end_inset

 problem.
\end_layout

\begin_layout Section
Logistic regression
\end_layout

\begin_layout Standard
We will see that 
\emph on
logistic regression
\emph default
 is not quadratic, but is nevertheless a convex problem.
 As in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "exa:logistic-regression"

\end_inset

, suppose that we collect data 
\begin_inset Formula $\left(Y_{i},x_{i}\right)$
\end_inset

 for 
\begin_inset Formula $i\in\left\{ 1,2,\ldots,n\right\} $
\end_inset

, where 
\begin_inset Formula $x_{i}\in\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $Y_{i}\in\left\{ 0,1\right\} $
\end_inset

.
 We would like to fit a 
\emph on
sigmoid curve
\emph default
 to the data.
 Logistic regression assumes the model
\begin_inset Formula 
\[
P\left(Y=1|x,\alpha_{0},\alpha_{1}\right)=\frac{1}{1+\exp\left(\alpha_{0}+\alpha_{1}x\right)}.
\]

\end_inset

We now show that fitting the logistic regression corresponds to a convex
 optimization problem.
 Either 
\begin_inset Formula $Y_{i}=0$
\end_inset

 or 
\begin_inset Formula $Y_{i}=1$
\end_inset

, so that the pmf of 
\begin_inset Formula $Y_{i}$
\end_inset

 can be written as 
\begin_inset Formula 
\[
P\left(Y_{i}=y_{i}|x_{i},\boldsymbol{\alpha}\right)=\left[P\left(Y_{i}=1|x_{i},\boldsymbol{\alpha}\right)\right]^{y_{i}}\left[P\left(Y_{i}=0|x_{i},\boldsymbol{\alpha}\right)\right]^{1-y_{i}}.
\]

\end_inset

We assume the 
\begin_inset Formula $Y_{i}\text{'s}$
\end_inset

 are a random sample, so that they are iid.
 Then, the log-likelihood is given by
\begin_inset Formula 
\begin{flalign*}
\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\log\prod_{i=1}^{N}P\left(Y_{i}=y_{i}|x_{i},\boldsymbol{\alpha}\right)\\
 & =\sum_{i=1}^{N}\log\left[P\left(Y_{i}=1|x_{i},\boldsymbol{\alpha}\right)\right]^{y_{i}}\left[P\left(Y_{i}=0|x_{i},\boldsymbol{\alpha}\right)\right]^{1-y_{i}}\\
 & =\sum_{i=1}^{N}\left[\log\left[P\left(Y_{i}=1|x_{i},\boldsymbol{\alpha}\right)\right]^{y_{i}}+\log\left[P\left(Y_{i}=0|x_{i},\boldsymbol{\alpha}\right)\right]^{1-y_{i}}\right]\\
 & =\sum_{i=1}^{N}\left[y_{i}\log P\left(Y_{i}=1|x_{i},\boldsymbol{\alpha}\right)+\left(1-y_{i}\right)\log P\left(Y_{i}=0|x_{i},\boldsymbol{\alpha}\right)\right]\\
 & =\sum_{i=1}^{N}\left[y_{i}\log\frac{1}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}+\left(1-y_{i}\right)\log\left[1-P\left(Y_{i}=1|x_{i}\boldsymbol{\alpha}\right)\right]\right]\\
 & =\sum_{i=1}^{N}\left[y_{i}\left(\log1-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right)+\left(1-y_{i}\right)\log\left(1-\frac{1}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\right]\\
 & =\sum_{i=1}^{N}\left[y_{i}\left(0-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right)+\left(1-y_{i}\right)\log\left(\frac{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}-1}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\right]\\
 & =\sum_{i=1}^{N}\left[-y_{i}\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)+\left(1-y_{i}\right)\left[\log\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\right]\\
 & =\sum_{i=1}^{N}\left[-y_{i}\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)+\left(1-y_{i}\right)\left[\alpha_{0}+\alpha_{1}x_{i}-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\right]\\
 & =\sum_{i=1}^{N}\left[-y_{i}\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)+\alpha_{0}+\alpha_{1}x_{i}-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right.\\
 & \qquad\quad\left.-y_{i}\left(\alpha_{0}+\alpha_{1}x_{i}\right)+y_{i}\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\\
 & =\sum_{i=1}^{N}\left[\left(1-y_{i}\right)\left(\alpha_{0}+\alpha_{1}x_{i}\right)-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right].
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\alpha_{0}$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\alpha_{0}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{0}}\sum_{i=1}^{N}\left[\left(1-y_{i}\right)\left(\alpha_{0}+\alpha_{1}x_{i}\right)-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{0}}\left[\left(1-y_{i}\right)\left(\alpha_{0}+\alpha_{1}x_{i}\right)-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\\
 & =\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right).
\end{flalign*}

\end_inset

Taking the derivative with respect to 
\begin_inset Formula $\alpha_{1}$
\end_inset

 gives
\begin_inset Formula 
\begin{flalign*}
\frac{\partial}{\partial\alpha_{1}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{1}}\sum_{i=1}^{N}\left[\left(1-y_{i}\right)\left(\alpha_{0}+\alpha_{1}x_{i}\right)-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{1}}\left[\left(1-y_{i}\right)\left(\alpha_{0}+\alpha_{1}x_{i}\right)-\log\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\right]\\
 & =\sum_{i=1}^{N}\left[\left(1-y_{i}\right)x_{i}-\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right]\\
 & =\sum_{i=1}^{N}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right).
\end{flalign*}

\end_inset

Then,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\begin_inset Formula 
\begin{flalign*}
\nabla\ell\left(\boldsymbol{\alpha}\right) & =\begin{bmatrix}\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
\sum_{i=1}^{N}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)
\end{bmatrix}\\
 & =\sum_{i=1}^{N}\begin{bmatrix}1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\\
x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)
\end{bmatrix}\\
 & =\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\begin{bmatrix}1\\
x_{i}
\end{bmatrix}.
\end{flalign*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

We now evaluate the second partial derivatives of the log-likelihood.
\begin_inset Formula 
\begin{flalign*}
\frac{\partial^{2}}{\partial\alpha_{0}^{2}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{0}}\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{0}}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\left[0-0-\frac{\partial}{\partial\alpha_{0}}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right]\\
 & =\sum_{i=1}^{N}-\frac{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\cdot1-\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\left(\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\cdot1}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
 & =\sum_{i=1}^{N}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}+\left(\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}-\left(\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
 & =\sum_{i=1}^{N}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
\frac{\partial^{2}}{\partial\alpha_{1}^{2}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{1}}\sum_{i=1}^{N}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{1}}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}x_{i}\frac{\partial}{\partial\alpha_{1}}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}x_{i}\left[0-0-\frac{\partial}{\partial\alpha_{1}}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right]\\
 & =\sum_{i=1}^{N}-x_{i}\frac{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\cdot x_{i}-\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\left(\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)\cdot x_{i}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
 & =\sum_{i=1}^{N}-x_{i}\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}-\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
 & =\sum_{i=1}^{N}-\frac{x_{i}^{2}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
\frac{\partial^{2}}{\partial\alpha_{1}\partial\alpha_{0}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{1}}\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{1}}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\left[0-0-\frac{\partial}{\partial\alpha_{1}}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right]\\
 & =\sum_{i=1}^{N}-\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
\frac{\partial^{2}}{\partial\alpha_{0}\partial\alpha_{1}}\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\frac{\partial}{\partial\alpha_{0}}\sum_{i=1}^{N}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}\frac{\partial}{\partial\alpha_{0}}x_{i}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}x_{i}\frac{\partial}{\partial\alpha_{0}}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\\
 & =\sum_{i=1}^{N}-\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}
\end{flalign*}

\end_inset

Then, the Hessian is
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.5}
\end_layout

\end_inset


\begin_inset Formula 
\begin{flalign*}
\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right) & =\begin{bmatrix}-\sum_{i=1}^{N}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}} & -\sum_{i=1}^{N}\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
-\sum_{i=1}^{N}\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}} & -\sum_{i=1}^{N}\frac{x_{i}^{2}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}
\end{bmatrix}\\
 & =-\sum_{i=1}^{N}\begin{bmatrix}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}} & \frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\\
\frac{x_{i}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}} & \frac{x_{i}^{2}\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}
\end{bmatrix}\\
 & =-\sum_{i=1}^{N}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\begin{bmatrix}1 & x_{i}\\
x_{i} & x_{i}^{2}
\end{bmatrix}\\
 & =-\sum_{i=1}^{N}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\begin{bmatrix}1 & x_{i}\end{bmatrix}.
\end{flalign*}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.0}
\end_layout

\end_inset

We have 
\begin_inset Formula $\mathrm{e}^{z}>0$
\end_inset

 for 
\begin_inset Formula $z\in\mathbb{R}$
\end_inset

, so it follows that the quantity 
\begin_inset Formula 
\[
k_{i}=\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}},\quad i=1,\ldots,N
\]

\end_inset

is positive.
 Now, suppose that 
\begin_inset Formula $\mathbf{z}\in\mathbb{R}^{2}$
\end_inset

 and 
\begin_inset Formula $\mathbf{z}\neq\mathbf{0}$
\end_inset

.
 Noting that we can write the Hessian as 
\begin_inset Formula 
\[
\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)=-\sum_{i=1}^{N}k_{i}\begin{bmatrix}1 & x_{i}\\
x_{i} & x_{i}^{2}
\end{bmatrix}=\begin{bmatrix}-k_{1} & -k_{1}x_{1}\\
-k_{1}x_{1} & -k_{1}x_{1}^{2}
\end{bmatrix}+\cdots+\begin{bmatrix}-k_{N} & -k_{N}x_{N}\\
-k_{N}x_{N} & -k_{N}x_{N}^{2}
\end{bmatrix}=\mathbf{A}_{1}+\cdots+\mathbf{A}_{N},
\]

\end_inset

we have 
\begin_inset Formula 
\begin{flalign*}
\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z} & =\mathbf{z}^{\mathsf{T}}\left(\mathbf{A}_{1}+\cdots+\mathbf{A}_{N}\right)\mathbf{z}\\
 & =\mathbf{z}^{\mathsf{T}}\left(\mathbf{A}_{1}\mathbf{z}+\cdots+\mathbf{A}_{N}\mathbf{z}\right)\tag{distributive property}\\
 & =\mathbf{z}^{\mathsf{T}}\mathbf{A}_{1}\mathbf{z}+\cdots+\mathbf{z}^{\mathsf{T}}\mathbf{A}_{N}\mathbf{z}.\tag{distributive property}
\end{flalign*}

\end_inset

For any 
\begin_inset Formula $\mathbf{A}_{i}$
\end_inset

, we have
\begin_inset Formula 
\begin{flalign*}
\mathbf{z}^{\mathsf{T}}\mathbf{A}_{i}\mathbf{z} & =\mathbf{z}^{\mathsf{T}}\begin{bmatrix}-k_{i} & -k_{i}x_{i}\\
-k_{i}x_{i} & -k_{i}x_{i}^{2}
\end{bmatrix}\mathbf{z}\\
 & =\mathbf{z}^{\mathsf{T}}-k_{i}\begin{bmatrix}1 & x_{i}\\
x_{i} & x_{i}^{2}
\end{bmatrix}\mathbf{z}\tag{\ensuremath{k_{i}} is a scalar}\\
 & =-k_{i}\mathbf{z}^{\mathsf{T}}\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\begin{bmatrix}1 & x_{i}\end{bmatrix}\mathbf{z}\\
 & =-k_{i}\left(\mathbf{z}\cdot\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\right)\left(\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\cdot\mathbf{z}\right)\tag{\ensuremath{\mathbf{u}^{\mathsf{T}}\mathbf{v}=\mathbf{u}\cdot\mathbf{v}}}\\
 & =-k_{i}\left(\mathbf{z}\cdot\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\right)^{2}.\tag{\ensuremath{\mathbf{u}\cdot\mathbf{v}=\mathbf{v}\cdot\mathbf{u}}}
\end{flalign*}

\end_inset


\begin_inset Formula $k_{i}$
\end_inset

 is positive, and the square of the dot product of 
\begin_inset Formula $\mathbf{z}$
\end_inset

 and 
\begin_inset Formula $\begin{bmatrix}1\\
x_{i}
\end{bmatrix}$
\end_inset

 will be nonnegative (we may have 
\begin_inset Formula $z_{1}+z_{2}x_{i}=0$
\end_inset

), so that 
\begin_inset Formula 
\[
\mathbf{z}^{\mathsf{T}}\mathbf{A}_{i}\mathbf{z}=-k_{i}\left(\mathbf{z}\cdot\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\right)^{2}\leq0,
\]

\end_inset

i.e., 
\begin_inset Formula $\mathbf{A}_{i}$
\end_inset

 is negative semidefinite.
 Now, 
\begin_inset Formula $\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}$
\end_inset

 is equal to the sum of the 
\begin_inset Formula $\mathbf{z}^{\mathsf{T}}\mathbf{A}_{i}\mathbf{z}$
\end_inset

 terms, all of which are less than or equal to zero, i.e., each 
\begin_inset Formula $\mathbf{A}_{i}$
\end_inset

 is negative semidefinite, so it follows that 
\begin_inset Formula $\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}$
\end_inset

 will be be less than or equal to zero.
 (The inequality will be strict in the case that at least two of the 
\begin_inset Formula $x_{i}\text{'s}$
\end_inset

 are non-zero and unequal, which is likely for a reasonable data set.) The
 log-likelihood is thus concave, and the maximum likelihood estimate 
\begin_inset Formula $\hat{\boldsymbol{\alpha}}$
\end_inset

 corresponds to a global maximum.
 Now consider the negative log-likelihood, i.e., 
\begin_inset Formula $-\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)$
\end_inset

.
 We may equivalently minimize this function to find the maximum likelihood
 estimate 
\begin_inset Formula $\hat{\boldsymbol{\alpha}}$
\end_inset

.
 In this case, we will have 
\begin_inset Formula 
\[
\nabla\left[-\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\right]=-\sum_{i=1}^{N}\left(1-y_{i}-\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}\right)\begin{bmatrix}1\\
x_{i}
\end{bmatrix}
\]

\end_inset

and thus
\begin_inset Formula 
\[
\mathbf{H}_{-\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)=-\left[-\sum_{i=1}^{N}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\begin{bmatrix}1 & x_{i}\end{bmatrix}\right]=\sum_{i=1}^{N}\frac{\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}}{\left(1+\mathrm{e}^{\alpha_{0}+\alpha_{1}x_{i}}\right)^{2}}\begin{bmatrix}1\\
x_{i}
\end{bmatrix}\begin{bmatrix}1 & x_{i}\end{bmatrix}.
\]

\end_inset

We have shown that 
\begin_inset Formula $\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}\leq0$
\end_inset

 for all 
\begin_inset Formula $\mathbf{z}\neq\mathbf{0}$
\end_inset

, so it follows that
\begin_inset Formula 
\[
\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}\leq0\implies-\mathbf{z}^{\mathsf{T}}\mathbf{H}_{\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}\geq0\implies\mathbf{z}^{\mathsf{T}}\mathbf{H}_{-\ell}\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)\mathbf{z}\geq0\quad\forall\ \mathbf{z}\neq\mathbf{0},
\]

\end_inset

i.e., the Hessian of the negative log-likelihood is positive semidefinite.
 In the case that the inequality is strict (which will be true for a reasonable
 data set), then 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:convex-positive-definite"

\end_inset

 implies that 
\begin_inset Formula $-\ell\left(\boldsymbol{\alpha}|\mathbf{x},\mathbf{y}\right)$
\end_inset

 is convex, so that finding the maximum likelihood estimate, i.e., minimizing
 the negative log-likelihood, is a convex optimization problem.
\end_layout

\begin_layout Standard
We now consider the problem in multiple dimensions, i.e., when 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 for 
\begin_inset Formula $n>1$
\end_inset

.
 In this case, logistic regression assumes the model 
\begin_inset Formula 
\[
P\left(Y=1|x,\boldsymbol{\alpha}\right)=\frac{1}{1+\exp\left(\alpha_{0}+\sum_{i=1}^{n}\alpha_{i}x_{i}\right)}.
\]

\end_inset

In the multidimensional case, we can apply similar ideas as in the 1-dimensional
 case to conclude that logistic regression corresponds to a convex optimization.
\end_layout

\begin_layout Chapter
Numerical linear algebra
\end_layout

\begin_layout Section
Machine representation
\end_layout

\begin_layout Standard
Suppose that we encode an integer using 32 
\emph on
bits
\emph default
 (equivalently, 4 
\emph on
bytes
\emph default
), each of which may be 0 or 1.
 We allocate a single bit 
\begin_inset Formula $s$
\end_inset

 to hold the sign, and we denote the 
\begin_inset Formula $k\text{th}$
\end_inset

 bit by 
\begin_inset Formula $i_{k}$
\end_inset

.
 Then, an integer 
\begin_inset Formula $x$
\end_inset

 can be written as
\begin_inset Formula 
\[
x=\left(-1\right)^{s}\cdot\left(i_{1}2^{0}+i_{2}2^{1}+i_{3}2^{2}+\cdots+i_{31}2^{30}\right)=\left(-1\right)^{s}\sum_{k=1}^{31}i_{k}2^{k-1},
\]

\end_inset

and we can store this representation as
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $s$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i_{3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i_{30}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $i_{31}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

,
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $i_{k}=1$
\end_inset

 if the 
\begin_inset Formula $k\text{th}$
\end_inset

 term is included in the sum and 0 if it is not.
 Now, this encoding is inefficient because it represents zero in two ways
 (zero is unsigned, so we may have 
\begin_inset Formula $s=0$
\end_inset

 or 
\begin_inset Formula $s=1$
\end_inset

), hence we are losing a bit.
 Suppose instead that we allocate all 32 bits to terms in the above summation,
 and by convention subtract 
\begin_inset Formula $2^{31}$
\end_inset

 from the sum, so that negative integers can be represented.
 Then, under this scheme, 
\begin_inset Formula 
\[
x=\left(i_{1}2^{0}+i_{2}2^{1}+i_{3}2^{2}+\cdots+i_{31}2^{30}+i_{32}2^{31}\right)-2^{31}=\sum_{k=1}^{32}i_{k}2^{k-1}-2^{31}.
\]

\end_inset

To represent 1, we will have 
\begin_inset Formula $i_{1}=1$
\end_inset

, 
\begin_inset Formula $i_{32}=1$
\end_inset

, and 
\begin_inset Formula $i_{k}=0$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 2,3,\ldots,31\right\} $
\end_inset

.
 The next consecutive integer is 2, which we represent with 
\begin_inset Formula $i_{2}=1$
\end_inset

, 
\begin_inset Formula $i_{32}=1$
\end_inset

, and 
\begin_inset Formula $i_{k}=0$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 1\right\} \cup\left\{ 3,4,\ldots,31\right\} $
\end_inset

.
 We represent 3 as 
\begin_inset Formula $i_{1}=1$
\end_inset

, 
\begin_inset Formula $i_{2}=1$
\end_inset

, 
\begin_inset Formula $i_{32}=1$
\end_inset

, and 
\begin_inset Formula $i_{k}=0$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 3,4,\ldots,31\right\} $
\end_inset

.
 We proceed in this fashion until 
\begin_inset Formula $i_{k}=1$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 1,2,\ldots,32\right\} $
\end_inset

, i.e., 
\begin_inset Formula 
\[
x_{\text{max}}=\left[2^{0}+2^{1}+2^{2}+\cdots+2^{30}+2^{31}\right]-2^{31}=2^{0}+2^{1}+2^{2}+\cdots+2^{30}=2147483647=2^{31}-1,
\]

\end_inset

which is the largest integer that can be stored under this encoding scheme.
 The largest negative integer that can be stored occurs when each 
\begin_inset Formula $i_{k}$
\end_inset

 is 0, and this is 
\begin_inset Formula $-2^{31}$
\end_inset

.
 Zero is represented by setting 
\begin_inset Formula $i_{32}=1$
\end_inset

 and 
\begin_inset Formula $i_{k}=0$
\end_inset

 for 
\begin_inset Formula $k\in\left\{ 1,2,\ldots,31\right\} $
\end_inset

.
\end_layout

\begin_layout Subsection
Floating-point numbers
\end_layout

\begin_layout Standard
Under the IEEE standard, numbers are encoded as 64-bit words of the form
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="9">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $s$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $e_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $e_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $e_{11}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $b_{1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $b_{2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $b_{52}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset

,
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s$
\end_inset

 is the sign bit and the 52 
\begin_inset Formula $b_{j}$
\end_inset

 bits represent the mantissa.
 The 11 
\begin_inset Formula $e_{k}$
\end_inset

 bits represent the positive binary integer that is the sum of the exponent
 and the 
\emph on
bias
\emph default
 
\begin_inset Formula $2^{10}-1=1023$
\end_inset

 (for exponents between 
\begin_inset Formula $-1022$
\end_inset

 and 1023).
 For example, an exponent of 0 is represented as 
\begin_inset Formula $0+1023=\left(11\,1111\,1111\right)_{2}$
\end_inset

, so that 
\begin_inset Formula $\boxed{e_{1}e_{2}\ldots e_{11}}=\mathtt{011\,1111\,1111}$
\end_inset

.
 An exponent of 1 is represented as 
\begin_inset Formula $1+1023=1024=\left(100\,0000\,0000\right)_{2}$
\end_inset

, so that 
\begin_inset Formula $\boxed{e_{1}e_{2}\ldots e_{11}}=\mathtt{100\,0000\,0000}$
\end_inset

.
 To find the actual exponent, we must subtract the bias.

\emph on
 
\emph default
Then, under this scheme, the representation of 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

 is 
\begin_inset Formula 
\[
x=\left(-1\right)^{s}\cdot1.\boxed{b_{1}b_{2}\ldots b_{52}}\cdot2^{\boxed{e_{1}e_{2}\ldots e_{11}}-1023},
\]

\end_inset

where 
\begin_inset Formula $\boxed{b_{1}b_{2}\dots b_{52}}$
\end_inset

 is the concatenation of the 
\begin_inset Formula $b_{j}$
\end_inset

 bits, so that the term 
\begin_inset Formula $2^{-j}$
\end_inset

 should be included in the decimal representation of 
\begin_inset Formula $x$
\end_inset

 if the 
\begin_inset Formula $j\text{th}$
\end_inset

 mantissa bit is 1, and 
\begin_inset Formula $\boxed{e_{1}e_{2}\ldots e_{11}}$
\end_inset

 is the concatenation of the 
\begin_inset Formula $e_{k}$
\end_inset

 bits.
 
\end_layout

\begin_layout Standard
Now, the exponent value 
\begin_inset Formula $2047_{10}=\left(111\,1111\,1111\right)_{2}$
\end_inset

 is reserved to represent infinity if every 
\begin_inset Formula $b_{j}$
\end_inset

 is zero, i.e., if the mantissa bits are all zero, and to represent NaN (Not
 a Number) otherwise.
 The exponent value 0 is used to represent 
\emph on
subnormal
\emph default
 floating point numbers, or those numbers where the left-most bit is not
 assumed to be 1.
 Thus, the smallest non-reserved value the exponent bits can take is 
\begin_inset Formula $\mathtt{000\,0000\,001}$
\end_inset

, which corresponds to an exponent of 
\begin_inset Formula $2^{0}-1023=-1022$
\end_inset

.
 The largest non-reserved value the exponent bits can take is 
\begin_inset Formula $\mathtt{111\,1111\,1110}$
\end_inset

, which corresponds to an exponent of 
\begin_inset Formula 
\[
\sum_{k=1}^{10}2^{k}-1023=1023.
\]

\end_inset

Thus, the range of the exponent is 
\begin_inset Formula $\left(-2^{10}+2,2^{10}-1\right)$
\end_inset

, so that the largest number that can be represented using the 
\emph on
double precision
\emph default
 floating-point encoding is 
\begin_inset Formula $2^{2^{10}-1}$
\end_inset

.
 In this encoding, the 
\emph on
radix point
\emph default
 (generalization of a decimal point) 
\begin_inset Quotes eld
\end_inset

floats.
\begin_inset Quotes erd
\end_inset

 We also observe that we cannot represent every real number exactly.
 For example, 
\begin_inset Formula $\sqrt{2}$
\end_inset

 does not have a finite decimal expansion, and extremely large or small
 numbers cannot be represented exactly due to the defined (and finite) number
 of bits available for representation.
\end_layout

\begin_layout Definition
For some 
\begin_inset Formula $x\in\mathbb{R}$
\end_inset

, denote by 
\begin_inset Formula $\text{fl}\left(x\right)$
\end_inset

 the closest double precision floating-point number to 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
We now consider the accuracy of floating-point representation.
 We have 52 stored mantissa bits plus 1 implicit leading bit to store the
 precision of 
\begin_inset Formula $x$
\end_inset

, which is equivalent to 15-17 decimal bits.
 The 
\emph on
relative roundoff error
\emph default
 in representing 
\begin_inset Formula $x$
\end_inset

 is 
\begin_inset Formula $\left|\text{fl}\left(x\right)-x\right|/\left|x\right|$
\end_inset

.
\end_layout

\begin_layout Example
Suppose that 
\begin_inset Formula $x=12345678901234567890$
\end_inset

.
 Assuming that 16 decimal bits of precision are available to represent 
\begin_inset Formula $x$
\end_inset

, we have 
\begin_inset Formula $\text{fl}\left(x\right)=12345678901234560000$
\end_inset

, so that the relative roundoff error is
\begin_inset Formula 
\[
\frac{\left|\text{fl}\left(x\right)-x\right|}{\left|x\right|}=\frac{\left|7890\right|}{\left|x\right|}\approx\frac{10^{4}}{10^{20}}=10^{-16}.
\]

\end_inset


\end_layout

\begin_layout Definition
The number 
\emph on
machine epsilon
\emph default
, denoted 
\begin_inset Formula $\epsilon_{\text{mach}}$
\end_inset

, is the distance between 1 and the smallest floating point number greater
 than 1.
\end_layout

\begin_layout Standard
Alternatively, 
\begin_inset Formula $\epsilon_{\text{mach}}$
\end_inset

 is the smallest positive number for which 
\begin_inset Formula $\text{fl}\left(1+\epsilon_{\text{mach}}\right)\neq1$
\end_inset

.
 Under the IEEE double precision floating-point standard, machine epsilon
 is 
\begin_inset Formula $2^{-52}\approx10^{-16}$
\end_inset

.
 The relative roundoff error in representing some 
\begin_inset Formula $x\neq0$
\end_inset

 will be at most 
\begin_inset Formula $\epsilon_{\text{mach}}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\frac{\left|\text{fl}\left(x\right)-x\right|}{\left|x\right|}\leq\epsilon_{\text{mach}}.
\]

\end_inset

In the worst case, i.e., equality, we will have 
\begin_inset Formula $\left|\text{fl}\left(x\right)-x\right|=\left|x\right|\epsilon_{\text{mach}}$
\end_inset

.
 If 
\begin_inset Formula $\text{fl}\left(x\right)\geq x$
\end_inset

, then 
\begin_inset Formula $\left|\text{fl}\left(x\right)-x\right|\geq0$
\end_inset

, so that this expression becomes
\begin_inset Formula 
\[
\left|\text{fl}\left(x\right)-x\right|=\left|x\right|\epsilon_{\text{mach}}\implies\text{fl}\left(x\right)-x=\left|x\right|\epsilon_{\text{mach}}\implies\text{fl}\left(x\right)=x+\left|x\right|\epsilon_{\text{mach}}.
\]

\end_inset

If 
\begin_inset Formula $\text{fl}\left(x\right)<x$
\end_inset

, then 
\begin_inset Formula $\left|\text{fl}\left(x\right)-x\right|<0$
\end_inset

, so that the expression becomes
\begin_inset Formula 
\[
\left|\text{fl}\left(x\right)-x\right|=\left|x\right|\epsilon_{\text{mach}}\implies-\left(\text{fl}\left(x\right)-x\right)=\left|x\right|\epsilon_{\text{mach}}\implies\text{fl}\left(x\right)=x-\left|x\right|\epsilon_{\text{mach}},
\]

\end_inset

which we can express compactly as 
\begin_inset Formula $\text{fl}\left(x\right)=x\pm\left|x\right|\epsilon_{\text{mach}}$
\end_inset

.
 If 
\begin_inset Formula $x\geq0$
\end_inset

, then the right side of this expression becomes 
\begin_inset Formula $x\pm x\epsilon_{\text{mach}}=x\left(1\pm\epsilon_{\text{mach}}\right)$
\end_inset

.
 If 
\begin_inset Formula $x<0$
\end_inset

, the right side of the expression becomes 
\begin_inset Formula 
\[
x\pm\left(-x\right)\epsilon_{\text{mach}}=x\pm x\epsilon_{\text{mach}}=x\left(1\pm\epsilon_{\text{mach}}\right),
\]

\end_inset

hence 
\begin_inset Formula $\text{fl}\left(x\right)=x\left(1\pm\epsilon_{\text{mach}}\right)$
\end_inset

 for all real 
\begin_inset Formula $x$
\end_inset

.
 Thus, the relative error of floating-point representation is bounded by
 
\begin_inset Formula $x\left(1\pm\epsilon_{\text{mach}}\right)$
\end_inset

.
 
\end_layout

\begin_layout Example
Suppose 
\begin_inset Formula $x,y\in\mathbb{R}$
\end_inset

, and consider the floating-point representation of 
\begin_inset Formula $\left(x+y\right)^{2}$
\end_inset

.
 We have 
\begin_inset Formula 
\begin{flalign*}
\text{fl}\left(\left(x+y\right)^{2}\right) & =\text{fl}\left(\left(\text{fl}\left(x\right)+\text{fl}\left(y\right)\right)^{2}\right)\\
 & =\text{fl}\left(\left(\text{fl}\left(x\right)+\text{fl}\left(y\right)\right)\cdot\left(\text{fl}\left(x\right)+\text{fl}\left(y\right)\right)\right)\\
 & =\text{fl}\left(\left(x\left(1+\epsilon_{1}\right)+y\left(1+\epsilon_{2}\right)\right)\cdot\left(x\left(1+\epsilon_{1}\right)+y\left(1+\epsilon_{2}\right)\right)\right)\\
 & =\text{fl}\left(x^{2}\left(1+\epsilon_{1}\right)^{2}\left(1+\epsilon_{3}\right)+y^{2}\left(1+\epsilon_{2}\right)^{2}\left(1+\epsilon_{4}\right)+2xy\left(1+\epsilon_{1}\right)\left(1+\epsilon_{2}\right)\left(1+\epsilon_{5}\right)\right)\\
 & =\left[x^{2}\left(1+\epsilon_{1}\right)^{2}\left(1+\epsilon_{3}\right)+y^{2}\left(1+\epsilon_{2}\right)^{2}\left(1+\epsilon_{4}\right)+2xy\left(1+\epsilon_{1}\right)\left(1+\epsilon_{2}\right)\left(1+\epsilon_{5}\right)\right]\left(1+\epsilon_{6}\right),
\end{flalign*}

\end_inset

where the 
\begin_inset Formula $\epsilon_{i}$
\end_inset

 terms are specific to the respective floating-point representations of
 each quantity and are on the order of 
\begin_inset Formula $\epsilon_{\text{mach}}$
\end_inset

.
 This expression can be simplified as
\begin_inset Formula 
\[
\text{fl}\left(\left(x+y\right)^{2}\right)=\left(x+y\right)^{2}\left(1+c\epsilon_{\text{mach}}\right),
\]

\end_inset

where 
\begin_inset Formula $c\approx4$
\end_inset

 (and in particular, 
\begin_inset Formula $c>1$
\end_inset

).
\end_layout

\begin_layout Standard
We see from this example that floating-point errors can add up.
 Let 
\begin_inset Formula 
\[
\mathbf{A}=\begin{bmatrix}10^{7} & 0 & 0\\
0 & 10^{20} & 0\\
0 & 0 & 1
\end{bmatrix},\quad\mathbf{b}=\begin{bmatrix}5\\
3\\
2
\end{bmatrix},
\]

\end_inset

and consider solving 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

.
 When we invert 
\begin_inset Formula $\mathbf{A}$
\end_inset

 to find the solution 
\begin_inset Formula $\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}$
\end_inset

, we will mix quantities, e.g., 1 and 
\begin_inset Formula $10^{20}$
\end_inset

, on very different scales, and the 1 will be wiped out by error.
\end_layout

\begin_layout Section
Gaussian elimination
\end_layout

\begin_layout Standard
Consider the problem of finding 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 that satisfies 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

 for some 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and 
\begin_inset Formula $n$
\end_inset

-dimensional vector 
\begin_inset Formula $\mathbf{b}$
\end_inset

.
 This problem arises in varied contexts, including solving the normal equations
 
\begin_inset Formula $\left(\mathbf{B}^{\mathsf{T}}\mathbf{B}\right)\boldsymbol{\alpha}=\mathbf{B}^{\mathsf{T}}\mathbf{y}$
\end_inset

 and using Newton's method for optimization, where we must solve 
\begin_inset Formula $\left[\mathbf{H}_{f}\left(\mathbf{x}\right)\right]\mathbf{z}=\nabla f\left(\mathbf{x}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
We can numerically solve the system 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

 by using 
\emph on
Gaussian elimination
\emph default
, which reduces the augmented matrix 
\begin_inset Formula $\left[\begin{array}{c|c}
\mathbf{A} & \mathbf{b}\end{array}\right]$
\end_inset

 to upper triangular form, then back-substitutes to solve for 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Recall that there are two permitted operations: multiplying an equation
 by a constant and subtracting one equation from another.
\end_layout

\begin_layout Example
Solve the system of equations
\begin_inset Formula 
\begin{flalign*}
3x_{1}+5x_{2}+x_{3} & =1\\
2x_{1}+2x_{2} & =2\\
3x_{1}+6x_{2}+10x_{3} & =3.
\end{flalign*}

\end_inset


\end_layout

\begin_layout Example
We can write this system as 
\begin_inset Formula 
\[
\begin{bmatrix}3 & 5 & 1\\
2 & 2 & 0\\
3 & 6 & 10
\end{bmatrix}\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}
\end{bmatrix}=\begin{bmatrix}1\\
2\\
3
\end{bmatrix},
\]

\end_inset

which can be expressed the system in row-echelon form as
\begin_inset Formula 
\[
\left[\begin{array}{ccc|c}
3 & 5 & 1 & 1\\
2 & 2 & 0 & 2\\
3 & 6 & 10 & 3
\end{array}\right].
\]

\end_inset

We wish to make this matrix upper triangular, i.e., 
\begin_inset Formula 
\[
\left[\begin{array}{ccc|c}
3 & 5 & 1 & 1\\
2 & 2 & 0 & 2\\
3 & 6 & 10 & 3
\end{array}\right]\begin{array}{c}
\\
-\frac{2}{3}R_{1}\\
\\
\end{array}\sim\left[\begin{array}{ccc|c}
3 & 5 & 1 & 1\\
0 & -\frac{4}{3} & -\frac{2}{3} & \frac{4}{3}\\
3 & 6 & 10 & 3
\end{array}\right]\begin{array}{c}
\\
\\
-R_{1}
\end{array}\sim\left[\begin{array}{ccc|c}
3 & 5 & 1 & 1\\
0 & -\frac{4}{3} & -\frac{2}{3} & \frac{4}{3}\\
0 & 1 & 9 & 2
\end{array}\right]\begin{array}{c}
\\
\\
+\frac{3}{4}R_{2}
\end{array}\sim\left[\begin{array}{ccc|c}
3 & 5 & 1 & 1\\
0 & -\frac{4}{3} & -\frac{2}{3} & \frac{4}{3}\\
0 & 0 & \frac{17}{2} & 3
\end{array}\right].
\]

\end_inset

We can now back-solve, so that 
\begin_inset Formula 
\begin{flalign*}
\frac{17}{2}x_{3}=3 & \implies x_{3}=\frac{6}{17},\\
-\frac{4}{3}x_{2}-\frac{2}{3}\left(\frac{6}{17}\right)=\frac{4}{3}\implies-\frac{4}{3}x_{2}=\frac{4}{3}+\frac{4}{17}\implies-\frac{4}{3}x_{2}=\frac{80}{51} & \implies x_{2}=-\frac{20}{17}\\
3x_{1}-5\left(\frac{20}{17}\right)+\frac{6}{17}=1\implies3x_{1}=\frac{111}{17} & \implies x_{1}=\frac{111}{51}.
\end{flalign*}

\end_inset

Thus, 
\begin_inset Formula 
\[
\mathbf{x}=\begin{bmatrix}x_{1}\\
x_{2}\\
x_{3}
\end{bmatrix}=\begin{bmatrix}\frac{111}{51}\\
-\frac{20}{17}\\
\frac{6}{17}
\end{bmatrix}
\]

\end_inset

is the solution to the system.
\end_layout

\begin_layout Standard
We now consider the computational complexity of Gaussian elimination for
 the general augmented matrix 
\begin_inset Formula 
\[
\left[\begin{array}{c|c}
\mathbf{A} & \mathbf{b}\end{array}\right]=\left[\begin{array}{cccc|c}
A_{11} & A_{12} & \cdots & A_{1n} & b_{1}\\
A_{21} & A_{22} & \cdots & A_{2n} & b_{2}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
A_{n1} & A_{n2} & \cdots & A_{nn} & b_{n}
\end{array}\right].
\]

\end_inset

The first step is to produce zeros in the first column of rows 2 through
 
\begin_inset Formula $n$
\end_inset

.
 Not including the first column, which will become zero, this requires one
 division (to find the factor 
\begin_inset Formula $c$
\end_inset

 that satisfies 
\begin_inset Formula $A_{i1}-cA_{11}=0$
\end_inset

), 
\begin_inset Formula $n-1+1=n$
\end_inset

 multiplications, and 
\begin_inset Formula $n$
\end_inset

 additions per row.
 Similarly, to produce zeros in the second column of rows 3 through 
\begin_inset Formula $n$
\end_inset

 requires one division, 
\begin_inset Formula $n-2+1=n-1$
\end_inset

 multiplications, and 
\begin_inset Formula $n-1$
\end_inset

 additions.
 Producing a zero in the 
\begin_inset Formula $\left(n-1\right)\text{th}$
\end_inset

 column of row 
\begin_inset Formula $n$
\end_inset

 requires one division, 
\begin_inset Formula $n-\left(n-1\right)+1=2$
\end_inset

 multiplications, and 2 additions.
 The elimination step thus requires 
\begin_inset Formula 
\[
n\left(n-1\right)+\left(n-1\right)\left(n-2\right)+\left(n-2\right)\left(n-3\right)\cdots+\left(n-\left(n-1\right)\right)\left(n-\left(n-2\right)\right)=\sum_{i=1}^{n-1}i\left(i+1\right)
\]

\end_inset

multiplications (and the same number of additions).
 Observe that 
\begin_inset Formula 
\[
\sum_{i=1}^{n-1}i\left(i+1\right)=\sum_{i=1}^{n-1}\left(i^{2}+i\right)=\sum_{i=1}^{n-1}i^{2}+\sum_{i=1}^{n-1}i=\sum_{i=1}^{n-1}i^{2}+\frac{\left(n-1\right)\left(n-1+1\right)}{2},
\]

\end_inset

where the final equality follows from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sum-of-first-n-numbers"

\end_inset

.
 Now, for any positive integer 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula 
\[
1^{2}+2^{2}+\cdots+n^{2}=\sum_{i=1}^{n}i^{2}=\frac{n\left(n+1\right)\left(2n+1\right)}{6},
\]

\end_inset

so it follows that
\begin_inset Formula 
\begin{flalign*}
\sum_{i=1}^{n-1}i\left(i+1\right) & =\frac{\left(n-1\right)\left(n-1+1\right)\left[2\left(n-1\right)+1\right]}{6}+\frac{\left(n-1\right)\left(n-1+1\right)}{2}\\
 & =\frac{n\left(n-1\right)}{2}\left(\frac{2n-2+1}{3}+1\right)\\
 & =\frac{n\left(n-1\right)}{2}\left(\frac{2n+2}{3}\right)\\
 & =n\left(n-1\right)\frac{n+1}{3}\\
 & =\frac{1}{3}n\left(n^{2}-1\right)\\
 & =\frac{1}{3}n^{3}-\frac{1}{3}n
\end{flalign*}

\end_inset

total multiplications are required.
 Now, reducing the first column requires 
\begin_inset Formula $1\left(n-1\right)=n-1$
\end_inset

 divisions, reducing the second column requires 
\begin_inset Formula $n-2$
\end_inset

 divisions, and in general reducing the 
\begin_inset Formula $j\text{th}$
\end_inset

 column requires 
\begin_inset Formula $n-j$
\end_inset

 divisions.
 It follows that 
\begin_inset Formula 
\begin{flalign*}
\left(n-1\right)+\left(n-2\right)+\cdots+\left(n-\left(n-1\right)\right) & =\sum_{j=1}^{n-1}\left(n-j\right)\\
 & =\sum_{j=1}^{n-1}n-\sum_{j=1}^{n-1}j\\
 & =n\left(n-1\right)-\frac{\left(n-1\right)\left(n-1+1\right)}{2}\\
 & =n\left(n-1\right)\left(1-\frac{1}{2}\right)\\
 & =\frac{1}{2}n^{2}-\frac{1}{2}n
\end{flalign*}

\end_inset

total divisions are required, so that the total operation count for the
 elimination step is
\begin_inset Formula 
\[
2\left(\frac{1}{3}n^{3}-\frac{1}{3}n\right)+\left(\frac{1}{2}n^{2}-\frac{1}{2}n\right)=\frac{2}{3}n^{3}-\frac{2}{3}n+\frac{1}{2}n^{2}-\frac{1}{2}n=\frac{2}{3}n^{3}+\frac{1}{2}n^{2}-\frac{4}{6}n-\frac{3}{6}n=\frac{2}{3}n^{3}+\frac{1}{2}n^{2}-\frac{7}{6}n.
\]

\end_inset

The resulting matrix is lower triangular, so we can solve the system by
 back-substituting, working from the bottom up.
 The 
\begin_inset Formula $n\text{th}$
\end_inset

 row requires one division to solve for 
\begin_inset Formula $x_{n}$
\end_inset

.
 Let 
\begin_inset Formula $a_{ij}$
\end_inset

 be the 
\begin_inset Formula $ij\text{th}$
\end_inset

 entry of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 after the elimination step, so that the 
\begin_inset Formula $\left(n-1\right)\text{th}$
\end_inset

 row is solved by
\begin_inset Formula 
\[
a_{n-1,n-1}x_{n-1}+a_{n-1,n}x_{n}=b_{n-1}\implies x_{n-1}=\frac{b_{n-1}-a_{n-1,n}x_{n}}{a_{n-1,n-1}},
\]

\end_inset

which requires one multiplication, one addition, and one division.
 Similarly, the 
\begin_inset Formula $\left(n-2\right)\text{th}$
\end_inset

 row is solved by 
\begin_inset Formula 
\[
x_{n-2}=\frac{b_{n-2}-a_{n-2,n-1}x_{n-1}-a_{n-2,n}x_{n}}{a_{n-2,n-2}},
\]

\end_inset

which requires two multiplications, two additions, and one division, i.e.,
 5 total operations.
 Finally, the first row is solved by
\begin_inset Formula 
\[
x_{11}=\frac{b_{1}-a_{12}x_{2}-a_{13}x_{3}+\cdots+a_{1n}x_{n}}{a_{11}},
\]

\end_inset

which requires 
\begin_inset Formula $n-1$
\end_inset

 multiplications, 
\begin_inset Formula $n-1$
\end_inset

 additions, and one division, i.e., 
\begin_inset Formula $\left(n-1\right)+\left(n-1\right)+1=2n-1$
\end_inset

 total operations.
 Then, the total operation count for the back-substitution step is
\begin_inset Formula 
\[
1+3+5+\cdots+\left(2n-1\right)=\sum_{i=1}^{n}\left(2i-1\right)=2\sum_{i=1}^{n}i-\sum_{i=1}^{n}1=2\left(\frac{n\left(n+1\right)}{2}\right)-n=n^{2}+n-n=n^{2}.
\]

\end_inset

Noting that this total operation count consists of
\begin_inset Formula 
\[
1+2+3+\cdots+\left(n-1\right)=\sum_{i=1}^{n-1}i=\frac{\left(n-1\right)\left(n-1+1\right)}{2}=\frac{1}{2}n\left(n-1\right)=\frac{1}{2}n^{2}-\frac{1}{2}n
\]

\end_inset

total multiplications, the same number of additions, and 
\begin_inset Formula $n$
\end_inset

 divisions (one for each of the 
\begin_inset Formula $n$
\end_inset

 rows), we see that
\begin_inset Formula 
\[
\left(\frac{1}{2}n^{2}-\frac{1}{2}n\right)+\left(\frac{1}{2}n^{2}-\frac{1}{2}n\right)+n=n^{2}-n+n=n^{2}.
\]

\end_inset

Thus, solving 
\begin_inset Formula $\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}$
\end_inset

 by Gaussian elimination requires 
\begin_inset Formula 
\[
\left(\frac{1}{3}n^{3}-\frac{1}{3}n\right)+\left(\frac{1}{2}n^{2}-\frac{1}{2}n\right)=\frac{1}{3}n^{3}-\frac{2}{6}n+\frac{1}{2}n^{2}-\frac{3}{6}n=\frac{1}{3}n^{3}+\frac{1}{2}n^{2}-\frac{5}{6}n
\]

\end_inset

total multiplications, the same number of additions, and 
\begin_inset Formula 
\[
\left(\frac{1}{2}n^{2}-\frac{1}{2}n\right)+n=\frac{1}{2}n^{2}+\frac{1}{2}n
\]

\end_inset

total divisions, so that the total operation count for the entire procedure
 is
\begin_inset Formula 
\[
2\left(\frac{1}{3}n^{3}+\frac{1}{2}n^{2}-\frac{5}{6}n\right)+\left(\frac{1}{2}n^{2}+\frac{1}{2}n\right)=\frac{2}{3}n^{3}+\frac{3}{2}n^{2}-\frac{7}{6}n,
\]

\end_inset

which is of 
\begin_inset Formula $\mathcal{O}\left(n^{3}\right)$
\end_inset

 complexity.
 Increasing 
\begin_inset Formula $n$
\end_inset

 by 10, for example, thus increases the number of operations required to
 perform Gaussian elimination by roughly 
\begin_inset Formula $10^{3}=1000$
\end_inset

.
\end_layout

\begin_layout Section
Condition number
\end_layout

\begin_layout Standard
Consider again the problem of finding 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

 that satisfies 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

 for some 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and 
\begin_inset Formula $n$
\end_inset

-dimensional vector 
\begin_inset Formula $\mathbf{b}$
\end_inset

, and suppose that 
\begin_inset Formula $\mathbf{A}\mathbf{x}^{\star}=\mathbf{b}+\Delta\mathbf{b}$
\end_inset

.
 Let 
\begin_inset Formula $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$
\end_inset

 be the function defined by 
\begin_inset Formula $f\left(\mathbf{b}\right)=\mathbf{A}^{-1}\mathbf{b}$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{x}=f\left(\mathbf{b}\right)$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}^{\star}=f\left(\mathbf{b}+\Delta\mathbf{b}\right)$
\end_inset

, where 
\begin_inset Formula $\Delta\mathbf{b}\in\mathbb{R}^{n}$
\end_inset

.
 We wish to calculate the 
\begin_inset Quotes eld
\end_inset

derivative
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $f$
\end_inset

,
\begin_inset Formula 
\[
\frac{\left\Vert f\left(\mathbf{b}+\Delta\mathbf{b}\right)-f\left(\mathbf{b}\right)\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }=\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }=\frac{\left\Vert \mathbf{A}^{-1}\left(\mathbf{b}+\Delta\mathbf{b}\right)-\mathbf{A}^{-1}\mathbf{b}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }=\frac{\left\Vert \mathbf{A}^{-1}\Delta\mathbf{b}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }.
\]

\end_inset

Now, 
\begin_inset Formula $\left\Vert \Delta\mathbf{b}\right\Vert $
\end_inset

 is a constant, so that for some 
\begin_inset Formula $\Delta\mathbf{b}\neq\mathbf{0}$
\end_inset

, we have
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{A}^{-1}\Delta\mathbf{b}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }=\left\Vert \frac{\mathbf{A}^{-1}\Delta\mathbf{b}}{\left\Vert \Delta\mathbf{b}\right\Vert }\right\Vert =\left\Vert \mathbf{A}^{-1}\left(\frac{\Delta\mathbf{b}}{\left\Vert \Delta\mathbf{b}\right\Vert }\right)\right\Vert \leq\max_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \mathbf{A}^{-1}\mathbf{z}\right\Vert ,
\]

\end_inset

where we have replaced the unit vector 
\begin_inset Formula $\Delta\mathbf{b}/\left\Vert \Delta\mathbf{b}\right\Vert $
\end_inset

 by 
\begin_inset Formula $\mathbf{z}$
\end_inset

.
 Thus, the 
\begin_inset Quotes eld
\end_inset

derivative
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $f$
\end_inset

 is the maximum value of the norm of the vector 
\begin_inset Formula $\mathbf{A}^{-1}\mathbf{z}$
\end_inset

 over all 
\begin_inset Formula $n$
\end_inset

-dimensional unit vectors 
\begin_inset Formula $\mathbf{z}$
\end_inset

, or informally, how far 
\begin_inset Formula $\mathbf{A}^{-1}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

stretches
\begin_inset Quotes erd
\end_inset

 the unit ball defined by 
\begin_inset Formula $\mathbf{z}\in\mathbb{R}^{n}$
\end_inset

.
\end_layout

\begin_layout Definition
Suppose that 
\begin_inset Formula $\mathbf{A}\in\mathbf{M}_{m,n}\left(\mathbb{R}\right)$
\end_inset

.
 Then, the 
\emph on
norm
\emph default
 of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, written 
\begin_inset Formula $\left\Vert \mathbf{A}\right\Vert $
\end_inset

, is given by 
\begin_inset Formula 
\[
\left\Vert \mathbf{A}\right\Vert =\max_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \mathbf{A}\mathbf{z}\right\Vert .
\]

\end_inset


\end_layout

\begin_layout Standard
We can thus express the 
\begin_inset Quotes eld
\end_inset

derivative
\begin_inset Quotes erd
\end_inset

 of 
\begin_inset Formula $f$
\end_inset

 as
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }\leq\left\Vert \mathbf{A}^{-1}\right\Vert .
\]

\end_inset

Now suppose that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is symmetric, and consider calculating 
\begin_inset Formula $\left\Vert \mathbf{A}\right\Vert $
\end_inset

.
 Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:spectral"

\end_inset

 implies that 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has an orthonormal basis of eigenvectors, which we denote by 
\begin_inset Formula $\left\{ \mathbf{q}^{\left(i\right)}\right\} _{i=1}^{n}$
\end_inset

, with corresponding eigenvalues 
\begin_inset Formula $\left|\lambda_{1}\right|>\left|\lambda_{2}\right|>\cdots>\left|\lambda_{n}\right|$
\end_inset

.
 Noting that we can express any 
\begin_inset Formula $\left\{ \mathbf{z}:\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1\right\} $
\end_inset

 in the 
\begin_inset Formula $\mathbf{q}$
\end_inset

-basis, i.e., 
\begin_inset Formula 
\[
\mathbf{z}=c_{1}\mathbf{q}^{\left(1\right)}+c_{2}\mathbf{q}^{\left(2\right)}+\cdots+c_{n}\mathbf{q}^{\left(n\right)}=\sum_{i=1}^{n}c_{i}\mathbf{q}^{\left(i\right)},
\]

\end_inset

it follows that 
\begin_inset Formula 
\[
\left\Vert \mathbf{z}\right\Vert ^{2}=\mathbf{z}\cdot\mathbf{z}=\left(\sum_{i=1}^{n}c_{i}\mathbf{q}^{\left(i\right)}\right)\cdot\left(\sum_{j=1}^{n}c_{j}\mathbf{q}^{\left(j\right)}\right)=\left(\sum_{i=1}^{n}c_{i}\mathbf{q}^{\left(i\right)}\right)\cdot c_{1}\mathbf{q}^{\left(1\right)}+\cdots+\left(\sum_{i=1}^{n}c_{i}\mathbf{q}^{\left(i\right)}\right)\cdot c_{n}\mathbf{q}^{\left(n\right)}.
\]

\end_inset

The expansion of the 
\begin_inset Formula $k\text{th}$
\end_inset

 term in this sum is another sum whose summands are of the form 
\begin_inset Formula $c_{i}c_{k}\left(\mathbf{q}^{\left(i\right)}\cdot\mathbf{q}^{\left(k\right)}\right)$
\end_inset

.
 Because the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 are orthonormal, these summands will be equal to one in the case that 
\begin_inset Formula $i=k$
\end_inset

 and zero otherwise, i.e., 
\begin_inset Formula 
\[
c_{i}c_{k}\left(\mathbf{q}^{\left(i\right)}\cdot\mathbf{q}^{\left(k\right)}\right)=\begin{cases}
1, & \text{if }i=k\\
0, & \text{otherwise}
\end{cases},
\]

\end_inset

and it follows that 
\begin_inset Formula 
\[
\left\Vert \mathbf{z}\right\Vert ^{2}=c_{1}\cdot c_{1}\cdot1+c_{2}\cdot c_{2}\cdot1+\cdots+c_{n}\cdot c_{n}\cdot1=\sum_{i=1}^{n}c_{i}^{2}.
\]

\end_inset

We have 
\begin_inset Formula $\left\Vert \mathbf{z}\right\Vert =1$
\end_inset

, which implies that 
\begin_inset Formula $\left\Vert \mathbf{z}\right\Vert ^{2}=1$
\end_inset

, hence that 
\begin_inset Formula $\sum_{i=1}^{n}c_{i}^{2}=1$
\end_inset

.
 Then,
\begin_inset Formula 
\[
\mathbf{A}\mathbf{z}=\mathbf{A}\left(\sum_{i=1}^{n}c_{i}\mathbf{q}^{\left(i\right)}\right)=\sum_{i=1}^{n}c_{i}\mathbf{A}\mathbf{q}^{\left(i\right)}=\sum_{i=1}^{n}c_{i}\lambda_{i}\mathbf{q}^{\left(i\right)}.
\]

\end_inset

Now, if a unit-norm vector 
\begin_inset Formula $\mathbf{z}^{\star}\in\mathbb{R}^{n}$
\end_inset

 maximizes 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert $
\end_inset

, i.e., if 
\begin_inset Formula 
\[
\mathbf{z}^{\star}=\argmax_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \mathbf{A}\mathbf{z}\right\Vert ,
\]

\end_inset

then 
\begin_inset Formula $\mathbf{z}^{\star}$
\end_inset

 will also maximize 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}$
\end_inset

.
 We can then consider maximizing 
\begin_inset Formula 
\[
\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}=\left(\mathbf{A}\mathbf{z}\right)\cdot\left(\mathbf{A}\mathbf{z}\right)=\left(\sum_{i=1}^{n}c_{i}\lambda_{i}\mathbf{q}^{\left(i\right)}\right)\cdot\left(\sum_{j=1}^{n}c_{j}\lambda_{j}\mathbf{q}^{\left(j\right)}\right).
\]

\end_inset

Reasoning as above, i.e., exploiting the orthonormality of the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

, this expression becomes
\begin_inset Formula 
\[
\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}=\sum_{i=1}^{n}\left(c_{i}\lambda_{i}\right)^{2}=\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{2}.
\]

\end_inset

We can thus recast calculating the norm of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 as a constrained optimization, i.e., as finding the 
\begin_inset Formula $\mathbf{z}$
\end_inset

 that maximizes 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}$
\end_inset

 subject to 
\begin_inset Formula $\sum_{i=1}^{n}c_{i}^{2}=1$
\end_inset

.
 Recalling that 
\begin_inset Formula $\lambda_{k}$
\end_inset

 is eigenvalue of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 with the 
\begin_inset Formula $k\text{th}$
\end_inset

 largest absolute value, it follows that to maximize 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}$
\end_inset

, we should put all the 
\begin_inset Quotes eld
\end_inset

weight
\begin_inset Quotes erd
\end_inset

 into 
\begin_inset Formula $c_{1}$
\end_inset

, i.e., set 
\begin_inset Formula $c_{1}=1$
\end_inset

.
 Thus, 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}$
\end_inset

 is maximized when 
\begin_inset Formula $c_{1}=1$
\end_inset

, and in this case 
\begin_inset Formula $\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}=\lambda_{1}^{2}$
\end_inset

.
 When we set 
\begin_inset Formula $c_{1}=1$
\end_inset

, we set all the remaining weights 
\begin_inset Formula $c_{i}$
\end_inset

 to zero, so that 
\begin_inset Formula 
\[
\mathbf{A}\mathbf{z}=\sum_{i=1}^{n}c_{i}\lambda_{i}\mathbf{q}^{\left(i\right)}=1\cdot\lambda_{1}\mathbf{q}^{\left(1\right)}+0+\cdots+0=\lambda_{1}\mathbf{q}^{\left(1\right)}\implies\left\Vert \mathbf{A}\mathbf{z}\right\Vert ^{2}=\left\Vert \lambda_{1}\mathbf{q}^{\left(1\right)}\right\Vert ^{2}=\left\Vert \mathbf{A}\mathbf{q}^{\left(1\right)}\right\Vert ^{2}\implies\mathbf{z}=\mathbf{q}^{\left(1\right)},
\]

\end_inset

where we have used the fact that 
\begin_inset Formula $\lambda_{1}$
\end_inset

 is an eigenvalue of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 with corresponding eigenvector 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}$
\end_inset

.
 Thus, the norm of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is
\begin_inset Formula 
\[
\left\Vert \mathbf{A}\right\Vert =\max_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \mathbf{A}\mathbf{z}\right\Vert =\max_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \lambda_{1}\mathbf{q}^{\left(1\right)}\right\Vert =\left\Vert \lambda_{1}\mathbf{q}^{\left(1\right)}\right\Vert =\lambda_{1}\left\Vert \mathbf{q}^{\left(1\right)}\right\Vert =\lambda_{1}\cdot1=\lambda_{1}.
\]

\end_inset


\end_layout

\begin_layout Proposition
The norm of a real 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 with eigenvalues 
\begin_inset Formula $\left|\lambda_{1}\right|>\left|\lambda_{2}\right|>\cdots\left|\lambda_{n}\right|$
\end_inset

 is 
\begin_inset Formula $\left\Vert \mathbf{A}\right\Vert =\lambda_{1}$
\end_inset

.
\end_layout

\begin_layout Proof
[proof goes here]
\end_layout

\begin_layout Standard
In practice, our interest will typically be in the 
\emph on
relative
\emph default
 error.
 We will examine the ratio of the 
\emph on
relative forward error
\emph default
 
\begin_inset Formula $\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \mathbf{x}\right\Vert $
\end_inset

 to the 
\emph on
relative backward error
\emph default
 
\begin_inset Formula $\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert $
\end_inset

, where 
\begin_inset Quotes eld
\end_inset

forward
\begin_inset Quotes erd
\end_inset

 refers to solving for 
\begin_inset Formula $\mathbf{x}$
\end_inset

, i.e., the error in the solution, and 
\begin_inset Quotes eld
\end_inset

backward
\begin_inset Quotes erd
\end_inset

 refers to the fact that 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is an input.
 We have
\begin_inset Formula 
\begin{flalign*}
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert } & =\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \Delta\mathbf{b}\right\Vert }{\left\Vert \mathbf{x}\right\Vert /\left\Vert \mathbf{b}\right\Vert }\\
 & =\frac{\left\Vert \mathbf{A}^{-1}\Delta\mathbf{b}\right\Vert /\left\Vert \Delta\mathbf{b}\right\Vert }{\left\Vert \mathbf{x}\right\Vert /\left\Vert \mathbf{A}\mathbf{x}\right\Vert }\\
 & =\frac{\left\Vert \mathbf{A}^{-1}\Delta\mathbf{b}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert }\cdot\frac{\left\Vert \mathbf{A}\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }\\
 & =\left\Vert \mathbf{A}^{-1}\frac{\Delta\mathbf{b}}{\left\Vert \Delta\mathbf{b}\right\Vert }\right\Vert \cdot\left\Vert \mathbf{A}\frac{\mathbf{x}}{\left\Vert \mathbf{x}\right\Vert }\right\Vert \\
 & \leq\left(\max_{\mathbf{z}\in\mathbb{R}^{n},\left\Vert \mathbf{z}\right\Vert =1}\left\Vert \mathbf{A}^{-1}\mathbf{z}\right\Vert \right)\left(\max_{\mathbf{w}\in\mathbb{R}^{n},\left\Vert \mathbf{w}\right\Vert =1}\left\Vert \mathbf{A}\mathbf{w}\right\Vert \right)\\
 & =\left\Vert \mathbf{A}^{-1}\right\Vert \left\Vert \mathbf{A}\right\Vert ,
\end{flalign*}

\end_inset

where the penultimate inequality follows from two applications of our result
 above.
 Thus, the ratio of the relative forward error to the relative backward
 error is bounded by the product of the norms of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and 
\begin_inset Formula $\mathbf{A}^{-1}$
\end_inset

.
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:inverse-eigenvalues"

\end_inset

Let 
\begin_inset Formula $\mathbf{M}$
\end_inset

 be a real, invertible 
\begin_inset Formula $n\times n$
\end_inset

 matrix, and let 
\begin_inset Formula $\lambda$
\end_inset

 be an eigenvalue of 
\begin_inset Formula $\mathbf{M}$
\end_inset

 with corresponding eigenvector 
\begin_inset Formula $\mathbf{v}$
\end_inset

.
 Then, 
\begin_inset Formula $1/\lambda$
\end_inset

 is an eigenvalue of 
\begin_inset Formula $\mathbf{M}^{-1}$
\end_inset

 with corresponding eigenvector 
\begin_inset Formula $\mathbf{v}$
\end_inset

.
\end_layout

\begin_layout Proof
We have
\begin_inset Formula 
\[
\mathbf{Mv}=\lambda\mathbf{v}\implies\mathbf{M}^{-1}\mathbf{M}\mathbf{v}=\mathbf{M}^{-1}\lambda\mathbf{v}\implies\mathbf{I}_{n}\mathbf{v}=\lambda\mathbf{M}^{-1}\mathbf{v}\implies\mathbf{M}^{-1}\mathbf{v}=\frac{1}{\lambda}\mathbf{v}.
\]

\end_inset


\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is symmetric with eigenvalues 
\begin_inset Formula $\left|\lambda_{1}\right|>\left|\lambda_{2}\right|>\cdots>\left|\lambda_{n}\right|$
\end_inset

.
 Then, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:inverse-eigenvalues"

\end_inset

 implies that 
\begin_inset Formula $\mathbf{A}^{-1}$
\end_inset

 has eigenvalues 
\begin_inset Formula $\left|1/\lambda_{n}\right|>\left|1/\lambda_{n-1}\right|>\cdots>\left|1/\lambda_{1}\right|$
\end_inset

, so that 
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert }\leq\left\Vert \mathbf{A}^{-1}\right\Vert \left\Vert \mathbf{A}\right\Vert =\frac{1}{\lambda_{n}}\cdot\lambda_{1}=\frac{\lambda_{1}}{\lambda_{n}},
\]

\end_inset

i.e., the ratio of the relative errors is bounded by the ratio of the largest
 eigenvalue of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 to the smallest.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mathbf{A}$
\end_inset

 be a real, invertible 
\begin_inset Formula $n\times n$
\end_inset

 matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 with eigenvalues 
\begin_inset Formula $\left|\lambda_{1}\right|>\left|\lambda_{2}\right|>\cdots>\left|\lambda_{n}\right|$
\end_inset

.
 The 
\emph on
condition number
\emph default
 of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, written 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)$
\end_inset

, is given by 
\begin_inset Formula 
\[
\kappa\left(\mathbf{A}\right)=\left\Vert \mathbf{A}^{-1}\right\Vert \left\Vert \mathbf{A}\right\Vert =\frac{\lambda_{1}}{\lambda_{n}}.
\]

\end_inset


\end_layout

\begin_layout Standard
We can now express our previous result for the ratio of the relative errors
 in terms of the condition number of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert }\leq\left\Vert \mathbf{A}^{-1}\right\Vert \left\Vert \mathbf{A}\right\Vert =\kappa\left(\mathbf{A}\right)\implies\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }\leq\kappa\left(\mathbf{A}\right)\cdot\frac{\left\Vert \Delta\mathbf{b}\right\Vert }{\left\Vert \mathbf{b}\right\Vert }.
\]

\end_inset

The condition number quantifies the sensitivity of the solution 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to changes in 
\begin_inset Formula $\mathbf{b}$
\end_inset

, or how much 
\begin_inset Quotes eld
\end_inset

damage
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathbf{A}$
\end_inset

 can do given a small change in 
\begin_inset Formula $\mathbf{b}$
\end_inset

.
 The condition number is thus, in a sense, a derivative.
\end_layout

\begin_layout Standard
Intuitively, 
\begin_inset Formula $\Delta\mathbf{b}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

smears
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathbf{b}$
\end_inset

, and the question arises of the optimal direction in which to smear 
\begin_inset Formula $\mathbf{b}$
\end_inset

 so that the solutions 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}^{\star}$
\end_inset

 of 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

 and 
\begin_inset Formula $\mathbf{A}\mathbf{x}^{\star}=\mathbf{b}+\Delta\mathbf{b}$
\end_inset

, respectively, are maximally different.
 Take 
\begin_inset Formula $\mathbf{b}=\mathbf{q}^{\left(1\right)}$
\end_inset

, so that 
\begin_inset Formula 
\[
\mathbf{A}\mathbf{x}=\mathbf{q}^{\left(1\right)}\implies\mathbf{x}=\mathbf{A}^{-1}\mathbf{q}^{\left(1\right)}=\frac{1}{\lambda_{1}}\mathbf{q}^{\left(1\right)}.
\]

\end_inset

We see that, having taken 
\begin_inset Formula $\mathbf{b}=\mathbf{q}^{\left(1\right)}$
\end_inset

, which has unit length, we divide by the largest eigenvalue 
\begin_inset Formula $\lambda_{1}$
\end_inset

, so we have 
\begin_inset Quotes eld
\end_inset

shrunk
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as much as possible.
 Now let 
\begin_inset Formula $\Delta\mathbf{b}=\varepsilon\mathbf{q}^{\left(n\right)}$
\end_inset

, so that 
\begin_inset Formula 
\[
\mathbf{A}\mathbf{x}^{\star}=\mathbf{q}^{\left(1\right)}+\varepsilon\mathbf{q}^{\left(n\right)}\implies\mathbf{x}^{\star}=\mathbf{A}^{-1}\left(\mathbf{q}^{\left(1\right)}+\varepsilon\mathbf{q}^{\left(n\right)}\right)=\mathbf{A}^{-1}\mathbf{q}^{\left(1\right)}+\varepsilon\mathbf{A}^{-1}\mathbf{q}^{\left(n\right)}=\frac{1}{\lambda_{1}}\mathbf{q}^{\left(1\right)}+\frac{\varepsilon}{\lambda_{n}}\mathbf{q}^{\left(n\right)}.
\]

\end_inset

Thus, we have taken 
\begin_inset Formula $\mathbf{b}$
\end_inset

, which 
\begin_inset Quotes eld
\end_inset

lived
\begin_inset Quotes erd
\end_inset

 along 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}$
\end_inset

, and smeared it such that it now has a component along 
\begin_inset Formula $\mathbf{q}^{\left(n\right)}$
\end_inset

.
 We now examine the ratio of the relative errors
\begin_inset Formula 
\begin{flalign*}
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert /\left\Vert \mathbf{x}\right\Vert }{\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert } & =\frac{\left\Vert \left(\mathbf{q}^{\left(1\right)}/\lambda_{1}+\varepsilon\mathbf{q}^{\left(n\right)}/\lambda_{n}\right)-\mathbf{q}^{\left(1\right)}/\lambda_{1}\right\Vert /\left\Vert \mathbf{q}^{\left(1\right)}/\lambda_{1}\right\Vert }{\left\Vert \varepsilon\mathbf{q}^{\left(n\right)}\right\Vert /\left\Vert \mathbf{q}^{\left(1\right)}\right\Vert }\\
 & =\frac{\left(\varepsilon/\lambda_{n}\right)\left\Vert \mathbf{q}^{\left(n\right)}\right\Vert /\left(1/\lambda_{1}\right)\left\Vert \mathbf{q}^{\left(1\right)}\right\Vert }{\varepsilon\left\Vert \mathbf{q}^{\left(n\right)}\right\Vert /\left\Vert \mathbf{q}^{\left(1\right)}\right\Vert }\\
 & =\frac{\left(\varepsilon/\lambda_{n}\right)\cdot1/\left(1/\lambda_{1}\right)\cdot1}{\varepsilon\cdot1/1}\\
 & =\frac{\lambda_{1}}{\lambda_{n}}.
\end{flalign*}

\end_inset

We see that our result is precisely the condition number of 
\begin_inset Formula $\mathbf{A}$
\end_inset

, i.e., the maximum 
\begin_inset Quotes eld
\end_inset

damage
\begin_inset Quotes erd
\end_inset

 is indeed inflicted by taking 
\begin_inset Formula $\Delta\mathbf{b}=\varepsilon\mathbf{q}^{\left(n\right)}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{A}$
\end_inset

 has eigenvectors on very different scales, then 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)$
\end_inset

 will be very large.
 We now consider the implications of roundoff error.
 On a computer, we are solving not 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

, but 
\begin_inset Formula $\mathbf{A}\mathbf{x}^{\star}=\text{fl}\left(\mathbf{b}\right)$
\end_inset

 (and leaving aside for the moment the fact that we have not 
\begin_inset Formula $\mathbf{A}$
\end_inset

, but its floating-point representation as well).
 Now, 
\begin_inset Formula $\text{fl}\left(\mathbf{b}\right)$
\end_inset

 will be equal to 
\begin_inset Formula $\mathbf{b}$
\end_inset

 plus a term on the order of 
\begin_inset Formula $\epsilon_{\text{mach}}\approx10^{-16}$
\end_inset

 times 
\begin_inset Formula $\Delta\mathbf{b}$
\end_inset

, i.e., 
\begin_inset Formula $\text{fl}\left(\mathbf{b}\right)=\mathbf{b}+\epsilon_{\text{mach}}\Delta\mathbf{b}$
\end_inset

.
 Accouting for floating-point error, our result above becomes 
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }\leq\kappa\left(\mathbf{A}\right)\cdot\frac{\left\Vert \epsilon_{\text{mach}}\Delta\mathbf{b}\right\Vert }{\left\Vert \mathbf{b}\right\Vert }.
\]

\end_inset

Now, the quantity 
\begin_inset Formula $\left\Vert \Delta\mathbf{b}\right\Vert /\left\Vert \mathbf{b}\right\Vert $
\end_inset

 will be some constant, which for the moment we ignore, leaving only machine
 epsilon, so that the relative error due to roundoff is 
\begin_inset Formula 
\[
\frac{\left\Vert \mathbf{x}^{\star}-\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }\leq\kappa\left(\mathbf{A}\right)\cdot\epsilon_{\text{mach}}.
\]

\end_inset

It is clear that if 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)=10^{20}$
\end_inset

, for example, then 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)\cdot\epsilon_{\text{mach}}=10^{20}\cdot10^{-16}=10^{4}$
\end_inset

, so that the relative error will be large, and the results of such a computatio
n compromised by roundoff error.
\end_layout

\begin_layout Standard
Now consider a one-dimensional example, i.e., 
\begin_inset Formula $x^{\star},x\in\mathbb{R}$
\end_inset

.
 Then, in a 
\begin_inset Quotes eld
\end_inset

worst-case
\begin_inset Quotes erd
\end_inset

 scenario, the relative error will be
\begin_inset Formula 
\[
\left|x^{\star}-x\right|=\kappa\left(\mathbf{A}\right)\cdot\epsilon_{\text{mach}}\left|x\right|\implies x^{\star}=x\left(1\pm\kappa\left(\mathbf{A}\right)\cdot\epsilon_{\text{mach}}\right).
\]

\end_inset

If 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)=10^{d}$
\end_inset

 and taking 
\begin_inset Formula $\epsilon_{\text{mach}}=10^{-16}$
\end_inset

, we have 
\begin_inset Formula $x^{\star}=x\left(1\pm10^{d}\cdot10^{-16}\right)=x\left(1\pm10^{d-16}\right)$
\end_inset

, so that 
\begin_inset Formula $x$
\end_inset

 will be 
\begin_inset Quotes eld
\end_inset

shifted
\begin_inset Quotes erd
\end_inset

 by 
\begin_inset Formula $d-16$
\end_inset

 digits.
 For example, if 
\begin_inset Formula $x=1234567$
\end_inset

 and 
\begin_inset Formula $\kappa\left(\mathbf{A}\right)=10^{14}$
\end_inset

, then we will have 
\begin_inset Formula 
\[
x^{\star}=x\left(1+10^{-2}\right)=1234567+12345.67=1246912.67,
\]

\end_inset

i.e., we retain only 2 significant digits of 
\begin_inset Formula $x$
\end_inset

 (the remaining digits are incorrect).
 (We obtain the same result when we consider 
\begin_inset Formula $x^{\star}=x\left(1-10^{-2}\right)$
\end_inset

, i.e., we retain only two correct digits.) In general, we will retain 
\begin_inset Formula $\min\left(\left|d-16\right|,0\right)$
\end_inset

 significant digits, so that in the case that 
\begin_inset Formula $d\geq16$
\end_inset

, we will not retain 
\emph on
any
\emph default
 correct digits.
 Consider the following implications:
\end_layout

\begin_layout Enumerate

\emph on
Solving systems of the form 
\begin_inset Formula $\mathbf{A}\mathbf{x}=\mathbf{b}$
\end_inset

.

\emph default
 For example, when we compute a direction in Newton's Method, we are solving
 
\begin_inset Formula $\left[\mathbf{H}_{f}\left(\mathbf{x}\right)\right]\mathbf{d}=-\nabla f\left(\mathbf{x}\right)$
\end_inset

.
 If the Hessian 
\begin_inset Formula $\mathbf{H}_{f}\left(\mathbf{x}\right)$
\end_inset

 has high condition number, this computation may fail.
\end_layout

\begin_layout Enumerate

\emph on
Fitting a linear regression by solving the normal equations.

\emph default
 We fit a linear regression by solving the normal equations 
\begin_inset Formula $\mathbf{B}^{\mathsf{T}}\mathbf{B}\boldsymbol{\alpha}=\mathbf{B}^{\mathsf{T}}\mathbf{y}$
\end_inset

 for 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

.
 The matrix 
\begin_inset Formula $\mathbf{B}^{\mathsf{T}}\mathbf{B}$
\end_inset

 tends to have the square of the condition number of 
\begin_inset Formula $\mathbf{B}$
\end_inset

 , which may be high.
\end_layout

\begin_layout Enumerate

\emph on
Fitting a linear regression given observations 
\begin_inset Formula $\mathbf{y}$
\end_inset

.

\emph default
 When we solve the normal equations, our observations 
\begin_inset Formula $\mathbf{y}$
\end_inset

 are actually 
\begin_inset Formula $\mathbf{y}$
\end_inset

 plus some noise term 
\begin_inset Formula $\Delta\mathbf{y}$
\end_inset

, which is on the order of 
\begin_inset Formula $10^{-2}$
\end_inset

 for the NBA data set.
\end_layout

\begin_layout Standard
The normal equations tend to be badly conditioned because 
\begin_inset Formula $\kappa\left(\mathbf{B}^{\mathsf{T}}\mathbf{B}\right)$
\end_inset

 is often high.
\end_layout

\begin_layout Section
Projections
\end_layout

\begin_layout Standard
We now consider fitting a linear regression in the case that solution via
 the normal equations is impractical, e.g., due to the high condition number
 of 
\begin_inset Formula $\mathbf{B}^{\mathsf{T}}\mathbf{B}$
\end_inset

.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\Omega$
\end_inset

 be a set in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

, and suppose that 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

.
 Then, the 
\emph on
projection
\emph default
 of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 onto 
\begin_inset Formula $\Omega$
\end_inset

, written 
\begin_inset Formula $\text{proj}_{\Omega}\left(\mathbf{x}\right)$
\end_inset

, is defined by 
\begin_inset Formula 
\[
\text{proj}_{\Omega}\left(\mathbf{x}\right)=\argmin_{\mathbf{z}\in\Omega}\left\Vert \mathbf{z}-\mathbf{x}\right\Vert .
\]

\end_inset


\end_layout

\begin_layout Standard
Informally, the projection of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 onto 
\begin_inset Formula $\Omega$
\end_inset

 is the vector 
\begin_inset Formula $\mathbf{z}\in\Omega$
\end_inset

 closest to 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Let 
\begin_inset Formula $\left\{ \mathbf{v}^{\left(i\right)}\right\} _{i=1}^{k}\in\mathbb{R}^{n}$
\end_inset

, and let 
\begin_inset Formula $\Omega$
\end_inset

 be the span of the 
\begin_inset Formula $\mathbf{v}^{\left(i\right)}$
\end_inset

.
 We wish to find 
\begin_inset Formula $\text{proj}_{\Omega}\left(\mathbf{x}\right)$
\end_inset

.
 Because the 
\begin_inset Formula $\mathbf{v}^{\left(i\right)}$
\end_inset

 span 
\begin_inset Formula $\Omega$
\end_inset

, we can write any 
\begin_inset Formula $\mathbf{z}\in\Omega$
\end_inset

 as a linear combination of the 
\begin_inset Formula $\mathbf{v}^{\left(i\right)}$
\end_inset

, i.e., 
\begin_inset Formula 
\[
\mathbf{z}=\alpha_{1}\mathbf{v}^{\left(1\right)}+\alpha_{2}\mathbf{v}^{\left(2\right)}+\cdots+\alpha_{k}\mathbf{v}^{\left(k\right)}=\sum_{i=1}^{k}\alpha_{i}\mathbf{v}^{\left(i\right)},
\]

\end_inset

so that we can rewrite the projection as the equivalent problem of finding
 the 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

 that solves
\begin_inset Formula 
\[
\hat{\boldsymbol{\alpha}}=\argmin_{\boldsymbol{\alpha}\in\mathbb{R}^{k}}\left\Vert \mathbf{x}-\sum_{i=1}^{k}\alpha_{i}\mathbf{v}^{\left(i\right)}\right\Vert .
\]

\end_inset

Now, any 
\begin_inset Formula $\boldsymbol{\alpha}$
\end_inset

 that minimizes this quantity will also minimize its square, so that 
\begin_inset Formula 
\[
\hat{\boldsymbol{\alpha}}=\argmin_{\boldsymbol{\alpha}\in\mathbb{R}^{k}}\left\Vert \mathbf{x}-\sum_{i=1}^{k}\alpha_{i}\mathbf{v}^{\left(i\right)}\right\Vert ^{2}.
\]

\end_inset

Let 
\begin_inset Formula $\mathbf{V}$
\end_inset

 be the matrix whose 
\begin_inset Formula $j\text{th}$
\end_inset

 column is given by 
\begin_inset Formula $\mathbf{v}^{\left(j\right)}$
\end_inset

, so that 
\begin_inset Formula 
\[
\sum_{i=1}^{k}\alpha_{i}\mathbf{v}^{\left(i\right)}=\begin{bmatrix}\mathbf{v}^{\left(1\right)} & \mathbf{v}^{\left(2\right)} & \cdots & \mathbf{v}^{\left(k\right)}\end{bmatrix}\begin{bmatrix}\alpha_{1}\\
\alpha_{2}\\
\vdots\\
\alpha_{k}
\end{bmatrix}=\mathbf{V}\boldsymbol{\alpha}\implies\hat{\boldsymbol{\alpha}}=\argmin_{\boldsymbol{\alpha}\in\mathbb{R}^{k}}\left\Vert \mathbf{x}-\mathbf{V}\boldsymbol{\alpha}\right\Vert ^{2}.
\]

\end_inset

We recognize this expression as having the same form as the least-squares
 loss function, and it follows that this quantity is minimized by 
\begin_inset Formula $\boldsymbol{\alpha}=\left(\mathbf{V}^{\mathsf{T}}\mathbf{V}\right)^{-1}\mathbf{V}^{\mathsf{T}}\mathbf{x}$
\end_inset

.
 Thus, linear regression is simply a projection of the observations 
\begin_inset Formula $\mathbf{y}$
\end_inset

 onto the basis (the span of the columns) of the model matrix 
\begin_inset Formula $\mathbf{B}$
\end_inset

 (and observe that we can replace the particular vectors that form 
\begin_inset Formula $\mathbf{B}$
\end_inset

 with a collection of vectors having the same span without affecting the
 projection).
 The normal equations are then seen to be badly conditioned because 
\begin_inset Formula $\mathbf{B}$
\end_inset

 is in general a 
\begin_inset Quotes eld
\end_inset

bad
\begin_inset Quotes erd
\end_inset

 basis, i.e., its columns are 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 to being linearly dependent.
 Suppose that we have just two columns, i.e., 
\begin_inset Formula $\left\{ \mathbf{v}^{\left(i\right)}\right\} _{i=1}^{2}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{v}^{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{v}^{\left(2\right)}$
\end_inset

 are close, then 
\begin_inset Formula $\kappa\left(\mathbf{V}\right)$
\end_inset

 will be high.
 If 
\begin_inset Formula $\mathbf{v}^{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{v}^{\left(2\right)}$
\end_inset

 are orthogonal, then 
\begin_inset Formula $\kappa\left(\mathbf{V}\right)$
\end_inset

 will be low, so that the best possible basis consists of mutually orthogonal
 vectors.
 In particular, suppose that the model matrix admits the decomposition 
\begin_inset Formula $\mathbf{B}=\mathbf{Q}\mathbf{R}$
\end_inset

.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mathbf{A}\in\mathbf{M}_{m,n}\left(\mathbb{R}\right)$
\end_inset

.
 Then, there exists an orthonormal matrix 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 and an invertible, upper triangular matrix 
\begin_inset Formula $\mathbf{R}$
\end_inset

 such that 
\begin_inset Formula $\mathbf{A}=\mathbf{Q}\mathbf{R}$
\end_inset

, and the span of the columns of 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is equal to the span of the columns of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

.
 This decomposition is known as the 
\emph on

\begin_inset Formula $\mathbf{Q}\mathbf{R}$
\end_inset

 decomposition.
\end_layout

\begin_layout Standard
If we can compute the 
\begin_inset Formula $\mathbf{Q}\mathbf{R}$
\end_inset

 decomposition of the model matrix 
\begin_inset Formula $\mathbf{B}$
\end_inset

, then we can replace a possibly badly conditioned matrix with a matrix
 having much better condition number (because the 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 matrix consists of orthogonal columns), i.e., 
\begin_inset Formula 
\begin{flalign*}
\boldsymbol{\alpha} & =\left(\mathbf{B}^{\mathsf{T}}\mathbf{B}\right)^{-1}\mathbf{B}^{\mathsf{T}}\mathbf{y}\\
 & =\left(\left(\mathbf{Q}\mathbf{R}\right)^{\mathsf{T}}\mathbf{Q}\mathbf{R}\right)^{-1}\left(\mathbf{Q}\mathbf{R}\right)^{\mathsf{T}}\mathbf{y}\\
 & =\left(\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{\mathsf{T}}\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{\mathsf{T}}\mathbf{y}\\
 & =\left(\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{-1}\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{\mathsf{T}}\mathbf{y}\tag{\ensuremath{\mathbf{Q}} is orthogonal}\\
 & =\left(\mathbf{R}^{\mathsf{T}}\mathbf{R}\right)^{-1}\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{\mathsf{T}}\mathbf{y}\\
 & =\mathbf{R}^{-1}\left(\mathbf{R}^{\mathsf{T}}\right)^{-1}\mathbf{R}^{\mathsf{T}}\mathbf{Q}^{\mathsf{T}}\mathbf{y}\\
 & =\mathbf{R}^{-1}\mathbf{Q}^{\mathsf{T}}\mathbf{y}.
\end{flalign*}

\end_inset

We can interpret the product 
\begin_inset Formula $\mathbf{Q}^{\mathsf{T}}\mathbf{y}$
\end_inset

 as projecting 
\begin_inset Formula $\mathbf{y}$
\end_inset

 onto the span of the columns of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

, and the action of 
\begin_inset Formula $\mathbf{R}^{-1}$
\end_inset

 as returning this product to the 
\begin_inset Formula $\mathbf{B}$
\end_inset

-basis.
 In general, 
\begin_inset Formula $\mathbf{R}^{-1}$
\end_inset

 is usually much better conditioned than 
\begin_inset Formula $\mathbf{B}^{\mathsf{T}}\mathbf{B}$
\end_inset

.
\end_layout

\begin_layout Standard
We continue to consider the problem of fitting a linear regression, i.e.,
 solving the optimization 
\begin_inset Formula 
\[
\hat{\boldsymbol{\alpha}}=\argmin_{\boldsymbol{\alpha}}\left\Vert \mathbf{y}-\mathbf{B}\boldsymbol{\alpha}\right\Vert ,
\]

\end_inset

where 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is a vector of observations and 
\begin_inset Formula $\mathbf{B}$
\end_inset

 is the model matrix.
 We can view the product 
\begin_inset Formula $\mathbf{B}\boldsymbol{\alpha}$
\end_inset

 as linearly combining a basis, i.e., the columns of 
\begin_inset Formula $\mathbf{B}$
\end_inset

.
 Recall that 
\begin_inset Formula $\mathbf{B}$
\end_inset

 is often a 
\begin_inset Quotes eld
\end_inset

bad
\begin_inset Quotes erd
\end_inset

 basis in the sense that it may have columns that are close to being (or
 actually are) linearly dependent, which occurs often when collecting data.
 The 
\begin_inset Formula $\mathbf{Q}\mathbf{R}$
\end_inset

 decomposition may allow us to fit the regression by finding a matrix 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 such that the span of the columns of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 is equal to the span of the columns of 
\begin_inset Formula $\mathbf{B}$
\end_inset

.
 We then project 
\begin_inset Formula $\mathbf{y}$
\end_inset

 onto 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 by solving
\begin_inset Formula 
\[
\hat{\boldsymbol{\beta}}=\argmin_{\boldsymbol{\beta}}\left\Vert \mathbf{y}-\mathbf{Q}\boldsymbol{\beta}\right\Vert ^{2},
\]

\end_inset

which is solved by 
\begin_inset Formula $\boldsymbol{\beta}=\left(\mathbf{Q}^{\mathsf{T}}\mathbf{Q}\right)^{-1}\mathbf{Q}^{\mathsf{T}}\mathbf{y}=\left(\mathbf{Q}^{-1}\mathbf{Q}\right)^{-1}\mathbf{Q}^{\mathsf{T}}\mathbf{y}=\mathbf{Q}^{\mathsf{T}}\mathbf{y}$
\end_inset

.
 Observe that the product 
\begin_inset Formula $\mathbf{Q}\boldsymbol{\beta}$
\end_inset

 can be viewed as linearly combining the columns of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

, which have the same span as the columns of 
\begin_inset Formula $\mathbf{B}$
\end_inset

, so that we can write any 
\begin_inset Formula $\mathbf{z}\in\text{span}\left\{ \text{col}\left(\mathbf{B}\right)\right\} $
\end_inset

 as a linear combination of the columns of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

, i.e., 
\begin_inset Formula $\mathbf{z}=\mathbf{Q}\boldsymbol{\beta}$
\end_inset

.
 We also have 
\begin_inset Formula $\mathbf{z}=\mathbf{B}\boldsymbol{\alpha}$
\end_inset

, so it follows that 
\begin_inset Formula $\mathbf{Q}\boldsymbol{\beta}=\mathbf{B}\boldsymbol{\alpha}$
\end_inset

.
 If we fit the linear regression in the 
\begin_inset Formula $\mathbf{Q}$
\end_inset

-basis, we must then convert the coefficients 
\begin_inset Formula $\boldsymbol{\beta}$
\end_inset

 back to the 
\begin_inset Formula $\mathbf{B}$
\end_inset

-basis, which is accomplished as
\begin_inset Formula 
\[
\mathbf{B}\boldsymbol{\alpha}=\mathbf{Q}\boldsymbol{\beta}\implies\boldsymbol{\alpha}=\mathbf{B}^{-1}\mathbf{Q}\boldsymbol{\beta}=\left(\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{Q}\boldsymbol{\beta}=\mathbf{R}^{-1}\mathbf{Q}^{-1}\mathbf{Q}\boldsymbol{\beta}=\mathbf{R}^{-1}\boldsymbol{\beta},
\]

\end_inset

where we have replaced 
\begin_inset Formula $\mathbf{B}$
\end_inset

 by its 
\begin_inset Formula $\mathbf{Q}\mathbf{R}$
\end_inset

 decomposition.
 Now, 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 is orthonormal and 
\begin_inset Formula $\mathbf{R}$
\end_inset

 is upper triangular and invertible, so that the decomposition provides
 both computational stability (through a possibly lower condition number)
 and performance (due to the upper triangular shape of 
\begin_inset Formula $\mathbf{R}$
\end_inset

).
 We now consider the problem of finding the decomposition.
 Letting 
\begin_inset Formula $\left\{ \mathbf{v}^{\left(i\right)}\right\} _{i=1}^{k}\in\mathbb{R}^{n}$
\end_inset

, we wish to find 
\begin_inset Formula $\left\{ \mathbf{q}^{\left(i\right)}\right\} _{i=1}^{k}\in\mathbb{R}^{n}$
\end_inset

 such that the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 are orthonormal and
\begin_inset Formula 
\[
\text{span}\left\{ \mathbf{v}^{\left(1\right)},\mathbf{v}^{\left(2\right)},\ldots,\mathbf{v}^{\left(\ell\right)}\right\} =\text{span}\left\{ \mathbf{q}^{\left(1\right)},\mathbf{q}^{\left(2\right)},\ldots,\mathbf{q}^{\left(\ell\right)}\right\} ,\quad\ell\in\left\{ 1,2,\ldots,k\right\} .
\]

\end_inset

We can find the 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}$
\end_inset

 through Gram-Schmidt orthogonalization.
\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}=\mathbf{v}^{\left(1\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Normalize 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}$
\end_inset

, i.e., set 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}=\mathbf{q}^{\left(1\right)}/\left\Vert \mathbf{q}^{\left(1\right)}\right\Vert $
\end_inset

.
\end_layout

\begin_layout Enumerate
To produce 
\begin_inset Formula $\mathbf{q}^{\left(2\right)}$
\end_inset

, we will subtract from 
\begin_inset Formula $\mathbf{v}^{\left(2\right)}$
\end_inset

 its projection onto 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}$
\end_inset

, i.e., we will set 
\begin_inset Formula $\mathbf{q}^{\left(2\right)}=\mathbf{v}^{\left(2\right)}-\left(\mathbf{v}^{\left(2\right)}\cdot\mathbf{q}^{\left(1\right)}\right)\mathbf{q}^{\left(1\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Normalize 
\begin_inset Formula $\mathbf{q}^{\left(2\right)}$
\end_inset

, i.e., set 
\begin_inset Formula $\mathbf{q}^{\left(2\right)}=\mathbf{q}^{\left(2\right)}/\left\Vert \mathbf{q}^{\left(2\right)}\right\Vert $
\end_inset

.
\end_layout

\begin_layout Enumerate
To produce 
\begin_inset Formula $\mathbf{q}^{\left(3\right)}$
\end_inset

, we will subtract from 
\begin_inset Formula $\mathbf{v}^{\left(3\right)}$
\end_inset

 its projections onto 
\begin_inset Formula $\mathbf{q}^{\left(1\right)}$
\end_inset

 and 
\begin_inset Formula $\mathbf{q}^{\left(2\right)}$
\end_inset

, i.e., we will set 
\begin_inset Formula $\mathbf{q}^{\left(3\right)}=\mathbf{v}^{\left(3\right)}-\left(\mathbf{v}^{\left(3\right)}\cdot\mathbf{q}^{\left(1\right)}\right)\mathbf{q}^{\left(1\right)}-\left(\mathbf{v}^{\left(3\right)}\cdot\mathbf{q}^{\left(2\right)}\right)\mathbf{q}^{\left(2\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Normalize 
\begin_inset Formula $\mathbf{q}^{\left(3\right)}$
\end_inset

, i.e., set 
\begin_inset Formula $\mathbf{q}^{\left(3\right)}=\mathbf{q}^{\left(3\right)}/\left\Vert \mathbf{q}^{\left(3\right)}\right\Vert $
\end_inset

.
\end_layout

\begin_layout Enumerate
Continue in this fashion until we construct 
\begin_inset Formula $\mathbf{q}^{\left(k\right)}$
\end_inset


\end_layout

\begin_layout Standard
Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:gram-schmidt-orth"

\end_inset

 provides a formal statement of the Gram-Schmidt iteration.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithmic}[1]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Require
\end_layout

\end_inset

 
\begin_inset Formula $\left\{ \mathbf{v}^{\left(i\right)}\right\} _{i=1}^{k}\in\mathbb{R}^{n}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
For{$i=1,2,
\backslash
ldots,k$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{w}^{\left(i\right)}\gets\mathbf{v}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
For{$j=1,2,
\backslash
ldots,i-1$}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{w}^{\left(i\right)}\gets\mathbf{w}^{\left(i\right)}-\left(\mathbf{v}^{\left(i\right)}\cdot\mathbf{q}^{\left(j\right)}\right)\mathbf{q}^{\left(j\right)}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
State
\end_layout

\end_inset

 
\begin_inset Formula $\mathbf{q}^{\left(i\right)}\gets\mathbf{w}^{\left(i\right)}/\left\Vert \mathbf{w}^{\left(i\right)}\right\Vert $
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
EndFor
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Return
\end_layout

\end_inset

 
\begin_inset Formula $\left\{ \mathbf{q}^{\left(i\right)}\right\} _{i=1}^{k}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:gram-schmidt-orth"

\end_inset

Gram-Schmidt Orthogonalization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
